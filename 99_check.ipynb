{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check workload zero rows return (valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check original workload valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files with zero rows found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import csv\n",
    "\n",
    "def check_zero_row_results(base_path):\n",
    "    zero_rows = []\n",
    "    \n",
    "    query_dirs = glob(os.path.join(base_path, \"0_original_testing_time_check\", \"*\"))\n",
    "    \n",
    "    for query_dir in sorted(query_dirs):\n",
    "        query_id = os.path.basename(query_dir)\n",
    "        \n",
    "        methods = ['cardinality', 'csv', 'kepler']\n",
    "        phases = ['train', 'test']\n",
    "        \n",
    "        for method in methods:\n",
    "            for phase in phases:\n",
    "                file_path = os.path.join(\n",
    "                    base_path,\n",
    "                    \"0_original_testing_time_check\",\n",
    "                    query_id,\n",
    "                    f\"{method}_{phase}_result.csv\"\n",
    "                )\n",
    "                        \n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        columns_to_check = ['Row Count']\n",
    "                        zero_counts = {}\n",
    "                        for col in columns_to_check:\n",
    "                            if col in df.columns:\n",
    "                                zero_counts[col] = (df[col] == 0).sum()\n",
    "                        \n",
    "                        if any(count > 0 for count in zero_counts.values()):\n",
    "                            zero_rows.append({\n",
    "                                'query_id': query_id,\n",
    "                                'method': method,\n",
    "                                'phase': phase,\n",
    "                                'file_path': file_path,\n",
    "                                'Row_Count_Zeros': zero_counts.get('Row Count', 0)\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "    \n",
    "    if zero_rows:\n",
    "        output_file = os.path.join(base_path, \"0_original_testing_time_check\", \"zero_return.csv\")\n",
    "        with open(output_file, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                'query_id', 'method', \n",
    "                'phase', 'file_path','Row_Count_Zeros'\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(zero_rows)\n",
    "        print(f\"Found {len(zero_rows)} files with zero rows. Results saved to {output_file}\")\n",
    "        \n",
    "        unique_query_ids = sorted(set(row['query_id'] for row in zero_rows))\n",
    "        print(\"ERROR query IDs:\", unique_query_ids)\n",
    "    else:\n",
    "        print(\"No files with zero rows found.\")\n",
    "\n",
    "def main():\n",
    "    base_path = \".\"\n",
    "    check_zero_row_results(base_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check sample workload valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files with zero rows found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import csv\n",
    "\n",
    "def check_zero_row_results(base_path):\n",
    "    zero_rows = []\n",
    "    \n",
    "    query_dirs = glob(os.path.join(base_path, \"0_sample_testing_time_check\", \"*\"))\n",
    "    \n",
    "    for query_dir in sorted(query_dirs):\n",
    "        query_id = os.path.basename(query_dir)\n",
    "        \n",
    "        robustness_methods = ['category', 'random', 'sliding']\n",
    "        methods = ['cardinality', 'csv', 'kepler']\n",
    "        instances = [1, 4]\n",
    "        phases = ['train', 'test']\n",
    "        \n",
    "        for rob in robustness_methods:\n",
    "            for method in methods:\n",
    "                for i in instances:\n",
    "                    for phase in phases:\n",
    "                        file_path = os.path.join(\n",
    "                            base_path,\n",
    "                            \"0_sample_testing_time_check\",\n",
    "                            query_id,\n",
    "                            f\"{rob}_{method}_db{i}_{phase}_result.csv\"\n",
    "                        )\n",
    "                        \n",
    "                        if os.path.exists(file_path):\n",
    "                            try:\n",
    "                                df = pd.read_csv(file_path)\n",
    "                                columns_to_check = ['Row Count']\n",
    "                                zero_counts = {}\n",
    "                                for col in columns_to_check:\n",
    "                                    if col in df.columns:\n",
    "                                        zero_counts[col] = (df[col] == 0).sum()\n",
    "                                \n",
    "                                if any(count > 0 for count in zero_counts.values()):\n",
    "                                    zero_rows.append({\n",
    "                                        'query_id': query_id,\n",
    "                                        'robustness': rob,\n",
    "                                        'method': method,\n",
    "                                        'db_instance': i,\n",
    "                                        'phase': phase,\n",
    "                                        'file_path': file_path,\n",
    "                                        'Row_Count_Zeros': zero_counts.get('Row Count', 0)\n",
    "                                    })\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "    \n",
    "    if zero_rows:\n",
    "        output_file = os.path.join(base_path, \"0_sample_testing_time_check\", \"zero_return.csv\")\n",
    "        with open(output_file, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                'query_id', 'robustness', 'method', 'db_instance', \n",
    "                'phase', 'file_path','Row_Count_Zeros'\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(zero_rows)\n",
    "        print(f\"Found {len(zero_rows)} files with zero rows. Results saved to {output_file}\")\n",
    "        \n",
    "        unique_query_ids = sorted(set(row['query_id'] for row in zero_rows))\n",
    "        print(\"ERROR query IDs:\", unique_query_ids)\n",
    "    else:\n",
    "        print(\"No files with zero rows found.\")\n",
    "\n",
    "def main():\n",
    "    base_path = \".\"\n",
    "    check_zero_row_results(base_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample workload generate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 query IDs: ['1-0', '10-0', '11-0', '12-0', '13-0', '14-0', '15-0', '16-0', '17-0', '18-0', '19-0', '2-0', '20-0', '21-0', '22-0', '23-0', '24-0', '25-0', '26-0', '27-0', '28-0', '3-0', '30-0', '31-0', '32-0', '33-0', '4-0', '5-0', '6-0', '7-0', '8-0', '9-0']\n",
      "Results saved to 0_sample_repo/all_params_results.csv\n",
      "Missing files information saved to 0_sample_repo/missing_files.csv\n",
      "Found 11 rows with missing 50 training parameters, saved to 0_sample_repo/missing_50_params.csv\n",
      "31 Non-missing query_ids: ['1-0', '10-0', '11-0', '12-0', '13-0', '14-0', '15-0', '17-0', '18-0', '19-0', '2-0', '20-0', '21-0', '22-0', '23-0', '24-0', '25-0', '26-0', '27-0', '28-0', '3-0', '30-0', '31-0', '32-0', '33-0', '4-0', '5-0', '6-0', '7-0', '8-0', '9-0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_all_query_ids(base_path):\n",
    "    \"\"\"\n",
    "    Automatically detect all query IDs from the directory structure\n",
    "    \n",
    "    Args:\n",
    "        base_path: Base directory path where all sample directories are located\n",
    "    \n",
    "    Returns:\n",
    "        set: Set of unique query IDs found in the directory structure\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"0_sample_repo/imdb_*_sample\")\n",
    "    matching_dirs = glob(pattern)\n",
    "    \n",
    "    query_ids = set()\n",
    "    for dir_path in matching_dirs:\n",
    "        dir_name = os.path.basename(dir_path)\n",
    "        try:\n",
    "            query_id = dir_name.split('_')[1]\n",
    "            query_ids.add(query_id)\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Could not extract query ID from directory: {dir_path}\")\n",
    "    \n",
    "    return query_ids\n",
    "\n",
    "def extract_params_from_csv(file_path, param_keys):\n",
    "    \"\"\"\n",
    "    Extract specified parameters from a CSV file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        param_keys: List of parameter names to extract\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing parameter names and their corresponding values\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        results = {}\n",
    "        for param in param_keys:\n",
    "            value = df.loc[df['Key'] == param, 'Value'].iloc[0] if not df.loc[df['Key'] == param, 'Value'].empty else None\n",
    "            results[param] = value\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_all_files(base_path, query_id):\n",
    "    \"\"\"\n",
    "    Process specific CSV files and collect both found and missing files\n",
    "    \n",
    "    Args:\n",
    "        base_path: Base directory path\n",
    "        query_id: Query ID to match in file paths\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (list of found file results, list of missing file records)\n",
    "    \"\"\"\n",
    "    params_to_extract = [\n",
    "        'original_testing_params',\n",
    "        'original_50_training_params',\n",
    "        'distinct_testing_params',\n",
    "        'distinct_50_training_params'\n",
    "    ]\n",
    "    \n",
    "    robustness_methods = ['category', 'random', 'sliding']\n",
    "    instance_numbers = [1, 4]\n",
    "    # methods = ['cardinality', 'csv', 'kepler', 'cardinality_full']\n",
    "    methods = ['cardinality', 'csv', 'kepler']\n",
    "    \n",
    "    results = []\n",
    "    missing_files = []\n",
    "    \n",
    "    for rob_method in robustness_methods:\n",
    "        for i in instance_numbers:\n",
    "            for method in methods:\n",
    "                file_path = os.path.join(\n",
    "                    base_path,\n",
    "                    \"0_sample_repo\",\n",
    "                    f\"imdb_{query_id}_sample\",\n",
    "                    rob_method,\n",
    "                    f\"db_instance_{i}\",\n",
    "                    method,\n",
    "                    \"inputs\",\n",
    "                    \"metadata\",\n",
    "                    f\"{query_id}.csv\"\n",
    "                )\n",
    "                \n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        params = extract_params_from_csv(file_path, params_to_extract)\n",
    "                        if params:\n",
    "                            record = {\n",
    "                                'query_id': query_id,\n",
    "                                'robustness_method': rob_method,\n",
    "                                'instance': i,\n",
    "                                'method': method,\n",
    "                                **params\n",
    "                            }\n",
    "                            results.append(record)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "                else:\n",
    "                    missing_files.append({\n",
    "                        'Query ID': query_id,\n",
    "                        'Robustness Method': rob_method,\n",
    "                        'Instance': i,\n",
    "                        'Method': method\n",
    "                    })\n",
    "    \n",
    "    return results, missing_files\n",
    "\n",
    "def save_results_to_csv(all_results, output_file):\n",
    "    \"\"\"\n",
    "    Save all results to a single CSV file\n",
    "    \"\"\"\n",
    "    header = ['Query ID', 'Robustness Method', 'Instance', 'Method',\n",
    "              'original_testing_params',\n",
    "              'original_50_training_params',\n",
    "              'distinct_testing_params',\n",
    "              'distinct_50_training_params']\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for record in all_results:\n",
    "            row = {\n",
    "                'Query ID': record['query_id'],\n",
    "                'Robustness Method': record['robustness_method'],\n",
    "                'Instance': record['instance'],\n",
    "                'Method': record['method'],\n",
    "                'original_testing_params': record.get('original_testing_params', ''),\n",
    "                'original_50_training_params': record.get('original_50_training_params', ''),\n",
    "                'distinct_testing_params': record.get('distinct_testing_params', ''),\n",
    "                'distinct_50_training_params': record.get('distinct_50_training_params', '')\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "def save_missing_files_to_csv(missing_files, output_file):\n",
    "    \"\"\"\n",
    "    Save missing file information to a CSV file\n",
    "    \n",
    "    Args:\n",
    "        missing_files: List of dictionaries containing missing file information\n",
    "        output_file: Path to the output CSV file\n",
    "    \"\"\"\n",
    "    header = ['Query ID', 'Robustness Method', 'Instance', 'Method']\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(missing_files)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that processes all query IDs and saves results to CSV files\n",
    "    \"\"\"\n",
    "    base_path = \".\"\n",
    "    results_output_file = \"0_sample_repo/all_params_results.csv\"\n",
    "    missing_files_output = \"0_sample_repo/missing_files.csv\"\n",
    "    \n",
    "    all_results = []\n",
    "    all_missing_files = []\n",
    "    \n",
    "    query_ids = find_all_query_ids(base_path)\n",
    "    print(f\"Found {len(query_ids)} query IDs: {sorted(query_ids)}\")\n",
    "    \n",
    "    for query_id in sorted(query_ids):\n",
    "        results, missing_files = process_all_files(base_path, query_id)\n",
    "        all_results.extend(results)\n",
    "        all_missing_files.extend(missing_files)\n",
    "    \n",
    "    # Save results\n",
    "    save_results_to_csv(all_results, results_output_file)\n",
    "    save_missing_files_to_csv(all_missing_files, missing_files_output)\n",
    "    print(f\"Results saved to {results_output_file}\")\n",
    "    print(f\"Missing files information saved to {missing_files_output}\")\n",
    "    \n",
    "    try:\n",
    "        # Check for missing 50 training parameters\n",
    "        df = pd.read_csv(results_output_file)\n",
    "        missing_50 = df[df['original_50_training_params'] < 50]\n",
    "        \n",
    "        if len(missing_50) > 0:\n",
    "            missing_50_output = \"0_sample_repo/missing_50_params.csv\"\n",
    "            missing_50.to_csv(missing_50_output, index=False)\n",
    "            print(f\"Found {len(missing_50)} rows with missing 50 training parameters, saved to {missing_50_output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while analyzing CSV: {str(e)}\")\n",
    "    \n",
    "    missing_query_ids = set(record['Query ID'] for record in all_missing_files)\n",
    "    non_missing_query_ids = sorted([query_id for query_id in query_ids if query_id not in missing_query_ids])\n",
    "    print(f\"{len(non_missing_query_ids)} Non-missing query_ids: {non_missing_query_ids}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check sample training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing\n",
      "28-0 sliding db1 cardinality\n",
      "28-0 sliding db1 csv\n",
      "28-0 sliding db1 kepler\n",
      "28-0 sliding db4 cardinality\n",
      "28-0 sliding db4 csv\n",
      "28-0 sliding db4 kepler\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "query_ids = [f\"{i}-0\" for i in range(1, 34) if i not in {16, 29}]\n",
    "# robustness_types = ['category', 'sliding', 'random', 'cardinality_full']\n",
    "robustness_types = ['category', 'sliding', 'random']\n",
    "instance_ids = [1, 4]\n",
    "methods = ['cardinality', 'csv', 'kepler']\n",
    "\n",
    "print(\"Missing\")\n",
    "for query_id in query_ids:\n",
    "    for robustness in robustness_types:\n",
    "        for i in instance_ids:\n",
    "            for method in methods:\n",
    "                file_path = f\"0_sample_repo/imdb_{query_id}_sample/{robustness}/db_instance_{i}/{method}/outputs/hints/{query_id}/training_50/candidate_metadata.json\"\n",
    "                exists = os.path.exists(file_path)\n",
    "                if not exists:\n",
    "                    print(f\"{query_id} {robustness} db{i} {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check sample robust plan\n",
    "\n",
    "Empty results:\n",
    "1. 13-0,category,4,csv\n",
    "2. 14-0,category,4,csv\n",
    "3. 15-0,sliding,1,csv\n",
    "4. 16-0,category,1,csv\n",
    "5. 16-0,category,4,csv\n",
    "6. 19-0,sliding,1,csv\n",
    "7. 20-0,category,4,csv\n",
    "8. 23-0,sliding,1,csv\n",
    "9. 24-0,sliding,1,csv\n",
    "10. 26-0,category,4,csv\n",
    "11. 33-0,category,4,csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing\n",
      "14-0 category db4 csv\n",
      "19-0 category db1 cardinality\n",
      "19-0 category db1 csv\n",
      "19-0 category db1 kepler\n",
      "19-0 category db4 cardinality\n",
      "19-0 category db4 csv\n",
      "19-0 category db4 kepler\n",
      "19-0 random db1 cardinality\n",
      "19-0 random db1 csv\n",
      "19-0 random db1 kepler\n",
      "19-0 random db4 cardinality\n",
      "19-0 random db4 csv\n",
      "19-0 random db4 kepler\n",
      "19-0 sliding db1 cardinality\n",
      "19-0 sliding db1 csv\n",
      "19-0 sliding db1 kepler\n",
      "19-0 sliding db4 cardinality\n",
      "19-0 sliding db4 csv\n",
      "19-0 sliding db4 kepler\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "query_ids = ['17-0', '14-0', '3-0', '4-0', '30-0', '19-0']\n",
    "robustness_types = ['category', 'random', 'sliding']\n",
    "instance_ids = [1, 4]\n",
    "methods = ['cardinality', 'csv', 'kepler']\n",
    "\n",
    "print(\"Missing\")\n",
    "for query_id in query_ids:\n",
    "    for robustness in robustness_types:\n",
    "        for i in instance_ids:\n",
    "            for method in methods:\n",
    "                file_path = f\"0_sample_repo/imdb_{query_id}_sample/{robustness}/db_instance_{i}/{method}/outputs/results/{query_id}/training_50/metadata.json\"\n",
    "                exists = os.path.exists(file_path)\n",
    "                if not exists:\n",
    "                    print(f\"{query_id} {robustness} db{i} {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### robustness plan statistics (robust plan length & test set plan length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file 0_sample_repo/imdb_14-0_sample/category/db_instance_4/csv/outputs/results/14-0/training_50/execution_output/imdbloadbase_14-0_metadata.json: 'plan_cover'\n",
      "Results saved to 0_sample_analysis/plan_cover_lengths.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def process_json_files(query_ids, methods, training_sizes, robustness_types, instance_ids):\n",
    "    # Initialize list to store results\n",
    "    results = []\n",
    "    \n",
    "    for robustness in robustness_types:\n",
    "        for db_i in instance_ids:\n",
    "            for query_id in query_ids:\n",
    "                for method in methods:\n",
    "                    for training_size in training_sizes:\n",
    "                        # Construct file path\n",
    "                        file_path = f\"0_sample_repo/imdb_{query_id}_sample/{robustness}/db_instance_{db_i}/{method}/outputs/results/{query_id}/training_{training_size}/execution_output/imdbloadbase_{query_id}_metadata.json\"\n",
    "                        \n",
    "                        # Check if file exists\n",
    "                        if os.path.exists(file_path):\n",
    "                            try:\n",
    "                                # Read and parse JSON file\n",
    "                                with open(file_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "                                    \n",
    "                                # Extract plan_cover length\n",
    "                                plan_cover_length = len(data[str(query_id)][\"plan_cover\"])\n",
    "                                \n",
    "                                # Add result to list\n",
    "                                results.append({\n",
    "                                    'query-id': query_id,\n",
    "                                    \"robustness\": robustness,\n",
    "                                    \"db_instance\": db_i,\n",
    "                                    'method': method,\n",
    "                                    'training_size': training_size,\n",
    "                                    'plan_cover_length': plan_cover_length\n",
    "                                })\n",
    "                            except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "                                print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "                        else:\n",
    "                            print(f\"File not found: {file_path}\")\n",
    "            \n",
    "    # Create DataFrame and save to CSV\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        output_dir = \"0_sample_analysis\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        output_file = f\"{output_dir}/plan_cover_lengths.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid files were found to process\")\n",
    "\n",
    "# Configuration\n",
    "query_ids = ['17-0', '14-0', '3-0', '4-0']\n",
    "robustness_types = ['category', 'random', 'sliding']\n",
    "instance_ids = [1, 4]\n",
    "methods = ['cardinality', 'kepler', 'csv']\n",
    "training_sizes = [50]\n",
    "\n",
    "process_json_files(query_ids, methods, training_sizes, robustness_types, instance_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Directory not found: 0_sample_repo/imdb_14-0_sample/category/db_instance_4/csv/outputs/evaluation/14-0/training_50/confidence_0/predictions/\n",
      "Results saved to 0_sample_analysis/confidence_range.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_confidence_ranges(query_id):\n",
    "    robustness_types = ['category', 'random', 'sliding']\n",
    "    instance_ids = [1, 4]\n",
    "    methods = ['cardinality', 'kepler', 'csv']\n",
    "    training_sizes = [50]\n",
    "    results = []\n",
    "    \n",
    "    for robustness in robustness_types:\n",
    "        for db_i in instance_ids:\n",
    "            for method in methods:\n",
    "                for training_size in training_sizes:\n",
    "                    base_path = f'0_sample_repo/imdb_{query_id}_sample/{robustness}/db_instance_{db_i}/{method}/outputs/evaluation/{query_id}/training_{training_size}/confidence_0/predictions/'\n",
    "                    \n",
    "                    if not os.path.exists(base_path):\n",
    "                        print(f\"Warning: Directory not found: {base_path}\")\n",
    "                        continue\n",
    "                        \n",
    "                    pattern = re.compile(f\"{query_id}_.*_batch_0\\.csv$\")\n",
    "                    matching_files = []\n",
    "                    \n",
    "                    for file in os.listdir(base_path):\n",
    "                        if pattern.match(file):\n",
    "                            matching_files.append(os.path.join(base_path, file))\n",
    "                    \n",
    "                    if not matching_files:\n",
    "                        print(f\"Warning: No matching files found for {method} - training_{training_size}\")\n",
    "                        continue\n",
    "                    \n",
    "                    all_confidences = []\n",
    "                    for file in matching_files:\n",
    "                        try:\n",
    "                            df = pd.read_csv(file)\n",
    "                            if 'confidence' in df.columns:\n",
    "                                all_confidences.extend(df['confidence'].tolist())\n",
    "                            else:\n",
    "                                print(f\"Warning: No confidence column in {file}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reading file {file}: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    if all_confidences:\n",
    "                        min_conf = min(all_confidences)\n",
    "                        max_conf = max(all_confidences)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'query_id': query_id,\n",
    "                            \"robustness\": robustness,\n",
    "                            \"db_instance\": db_i,\n",
    "                            'method': method,\n",
    "                            'training_size': training_size,\n",
    "                            'min_confidence': min_conf,\n",
    "                            'max_confidence': max_conf\n",
    "                        })\n",
    "                    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_ids = ['17-0', '14-0', '3-0', '4-0']\n",
    "    all_results = []\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        all_results.extend(analyze_confidence_ranges(query_id))\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    output_dir = '0_sample_analysis'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'confidence_range.csv')\n",
    "    results_df.to_csv(output_file, index=False, mode='w')\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def analyze_workload_files(directory):\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('training_size_50.csv'):\n",
    "            query_id = filename.split('_')[0]\n",
    "            robustness = filename.split('_')[1]\n",
    "            db_i = filename.split('_')[2]\n",
    "            method = filename.split('_')[3]\n",
    "            \n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            plan_counter = Counter(df['plan_id'])\n",
    "            results[query_id][f\"{robustness}_{db_i}_{method}\"] = dict(plan_counter)\n",
    "    \n",
    "    return dict(results)\n",
    "\n",
    "def main():\n",
    "    # sample plans\n",
    "    input_directory = '0_sample_plans_0'\n",
    "    output_file = '0_sample_analysis/sample_plan_statistics.json'\n",
    "    result_dict = analyze_workload_files(input_directory)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(result_dict, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get training metadata (length of robust plan set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: 0_finished_repo/imdb_3-0_original/kepler/outputs/results/3-0/training_400/execution_output/imdbloadbase_3-0_metadata.json\n",
      "Error processing file 0_finished_repo/imdb_4-0_original/cardinality/outputs/results/4-0/training_400/execution_output/imdbloadbase_4-0_metadata.json: 'plan_cover'\n",
      "File not found: 0_finished_repo/imdb_4-0_original/kepler/outputs/results/4-0/training_400/execution_output/imdbloadbase_4-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_4-0_original/csv/outputs/results/4-0/training_400/execution_output/imdbloadbase_4-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_6-0_original/cardinality/outputs/results/6-0/training_400/execution_output/imdbloadbase_6-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_6-0_original/kepler/outputs/results/6-0/training_400/execution_output/imdbloadbase_6-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_6-0_original/csv/outputs/results/6-0/training_400/execution_output/imdbloadbase_6-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_8-0_original/cardinality/outputs/results/8-0/training_400/execution_output/imdbloadbase_8-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_8-0_original/kepler/outputs/results/8-0/training_400/execution_output/imdbloadbase_8-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_8-0_original/csv/outputs/results/8-0/training_400/execution_output/imdbloadbase_8-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_9-0_original/cardinality/outputs/results/9-0/training_400/execution_output/imdbloadbase_9-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_9-0_original/kepler/outputs/results/9-0/training_400/execution_output/imdbloadbase_9-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_9-0_original/csv/outputs/results/9-0/training_400/execution_output/imdbloadbase_9-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_10-0_original/cardinality/outputs/results/10-0/training_400/execution_output/imdbloadbase_10-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_10-0_original/kepler/outputs/results/10-0/training_400/execution_output/imdbloadbase_10-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_10-0_original/csv/outputs/results/10-0/training_400/execution_output/imdbloadbase_10-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_11-0_original/cardinality/outputs/results/11-0/training_400/execution_output/imdbloadbase_11-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_11-0_original/kepler/outputs/results/11-0/training_400/execution_output/imdbloadbase_11-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_11-0_original/csv/outputs/results/11-0/training_400/execution_output/imdbloadbase_11-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_12-0_original/cardinality/outputs/results/12-0/training_400/execution_output/imdbloadbase_12-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_12-0_original/kepler/outputs/results/12-0/training_400/execution_output/imdbloadbase_12-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_12-0_original/csv/outputs/results/12-0/training_400/execution_output/imdbloadbase_12-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_13-0_original/cardinality/outputs/results/13-0/training_400/execution_output/imdbloadbase_13-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_13-0_original/kepler/outputs/results/13-0/training_400/execution_output/imdbloadbase_13-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_13-0_original/csv/outputs/results/13-0/training_400/execution_output/imdbloadbase_13-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_14-0_original/cardinality/outputs/results/14-0/training_400/execution_output/imdbloadbase_14-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_14-0_original/kepler/outputs/results/14-0/training_400/execution_output/imdbloadbase_14-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_14-0_original/csv/outputs/results/14-0/training_400/execution_output/imdbloadbase_14-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_15-0_original/cardinality/outputs/results/15-0/training_400/execution_output/imdbloadbase_15-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_15-0_original/kepler/outputs/results/15-0/training_400/execution_output/imdbloadbase_15-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_15-0_original/csv/outputs/results/15-0/training_400/execution_output/imdbloadbase_15-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_16-0_original/cardinality/outputs/results/16-0/training_400/execution_output/imdbloadbase_16-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_16-0_original/kepler/outputs/results/16-0/training_400/execution_output/imdbloadbase_16-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_16-0_original/csv/outputs/results/16-0/training_400/execution_output/imdbloadbase_16-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_17-0_original/cardinality/outputs/results/17-0/training_400/execution_output/imdbloadbase_17-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_17-0_original/kepler/outputs/results/17-0/training_400/execution_output/imdbloadbase_17-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_17-0_original/csv/outputs/results/17-0/training_400/execution_output/imdbloadbase_17-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_18-0_original/cardinality/outputs/results/18-0/training_400/execution_output/imdbloadbase_18-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_18-0_original/kepler/outputs/results/18-0/training_400/execution_output/imdbloadbase_18-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_18-0_original/csv/outputs/results/18-0/training_400/execution_output/imdbloadbase_18-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_19-0_original/cardinality/outputs/results/19-0/training_400/execution_output/imdbloadbase_19-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_19-0_original/kepler/outputs/results/19-0/training_400/execution_output/imdbloadbase_19-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_19-0_original/csv/outputs/results/19-0/training_400/execution_output/imdbloadbase_19-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_20-0_original/cardinality/outputs/results/20-0/training_400/execution_output/imdbloadbase_20-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_20-0_original/kepler/outputs/results/20-0/training_400/execution_output/imdbloadbase_20-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_20-0_original/csv/outputs/results/20-0/training_400/execution_output/imdbloadbase_20-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_21-0_original/cardinality/outputs/results/21-0/training_400/execution_output/imdbloadbase_21-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_21-0_original/kepler/outputs/results/21-0/training_400/execution_output/imdbloadbase_21-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_21-0_original/csv/outputs/results/21-0/training_400/execution_output/imdbloadbase_21-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_22-0_original/cardinality/outputs/results/22-0/training_400/execution_output/imdbloadbase_22-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_22-0_original/kepler/outputs/results/22-0/training_400/execution_output/imdbloadbase_22-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_22-0_original/csv/outputs/results/22-0/training_400/execution_output/imdbloadbase_22-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_23-0_original/cardinality/outputs/results/23-0/training_400/execution_output/imdbloadbase_23-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_23-0_original/kepler/outputs/results/23-0/training_400/execution_output/imdbloadbase_23-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_23-0_original/csv/outputs/results/23-0/training_400/execution_output/imdbloadbase_23-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_24-0_original/cardinality/outputs/results/24-0/training_400/execution_output/imdbloadbase_24-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_24-0_original/kepler/outputs/results/24-0/training_400/execution_output/imdbloadbase_24-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_24-0_original/csv/outputs/results/24-0/training_400/execution_output/imdbloadbase_24-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_25-0_original/cardinality/outputs/results/25-0/training_400/execution_output/imdbloadbase_25-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_25-0_original/kepler/outputs/results/25-0/training_400/execution_output/imdbloadbase_25-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_25-0_original/csv/outputs/results/25-0/training_400/execution_output/imdbloadbase_25-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_26-0_original/cardinality/outputs/results/26-0/training_400/execution_output/imdbloadbase_26-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_26-0_original/kepler/outputs/results/26-0/training_400/execution_output/imdbloadbase_26-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_26-0_original/csv/outputs/results/26-0/training_400/execution_output/imdbloadbase_26-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_27-0_original/cardinality/outputs/results/27-0/training_400/execution_output/imdbloadbase_27-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_27-0_original/kepler/outputs/results/27-0/training_400/execution_output/imdbloadbase_27-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_27-0_original/csv/outputs/results/27-0/training_400/execution_output/imdbloadbase_27-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_28-0_original/cardinality/outputs/results/28-0/training_400/execution_output/imdbloadbase_28-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_28-0_original/kepler/outputs/results/28-0/training_400/execution_output/imdbloadbase_28-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_28-0_original/csv/outputs/results/28-0/training_400/execution_output/imdbloadbase_28-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_30-0_original/cardinality/outputs/results/30-0/training_400/execution_output/imdbloadbase_30-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_30-0_original/kepler/outputs/results/30-0/training_400/execution_output/imdbloadbase_30-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_30-0_original/csv/outputs/results/30-0/training_400/execution_output/imdbloadbase_30-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_31-0_original/cardinality/outputs/results/31-0/training_400/execution_output/imdbloadbase_31-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_31-0_original/kepler/outputs/results/31-0/training_400/execution_output/imdbloadbase_31-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_31-0_original/csv/outputs/results/31-0/training_400/execution_output/imdbloadbase_31-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_32-0_original/cardinality/outputs/results/32-0/training_400/execution_output/imdbloadbase_32-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_32-0_original/kepler/outputs/results/32-0/training_400/execution_output/imdbloadbase_32-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_32-0_original/csv/outputs/results/32-0/training_400/execution_output/imdbloadbase_32-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_33-0_original/cardinality/outputs/results/33-0/training_400/execution_output/imdbloadbase_33-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_33-0_original/kepler/outputs/results/33-0/training_400/execution_output/imdbloadbase_33-0_metadata.json\n",
      "File not found: 0_finished_repo/imdb_33-0_original/csv/outputs/results/33-0/training_400/execution_output/imdbloadbase_33-0_metadata.json\n",
      "Results saved to 0_original_analysis/plan_cover_lengths.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def process_json_files(query_ids, methods, training_sizes):\n",
    "    # Initialize list to store results\n",
    "    results = []\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        for method in methods:\n",
    "            for training_size in training_sizes:\n",
    "                # Construct file path\n",
    "                file_path = f\"0_finished_repo/imdb_{query_id}_original/{method}/outputs/results/{query_id}/training_{training_size}/execution_output/imdbloadbase_{query_id}_metadata.json\"\n",
    "                \n",
    "                # Check if file exists\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        # Read and parse JSON file\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            data = json.load(f)\n",
    "                            \n",
    "                        # Extract plan_cover length\n",
    "                        plan_cover_length = len(data[str(query_id)][\"plan_cover\"])\n",
    "                        \n",
    "                        # Add result to list\n",
    "                        results.append({\n",
    "                            'query-id': query_id,\n",
    "                            'method': method,\n",
    "                            'training_size': training_size,\n",
    "                            'plan_cover_length': plan_cover_length\n",
    "                        })\n",
    "                    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "                        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        output_dir = \"0_original_analysis\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        output_file = f\"{output_dir}/plan_cover_lengths.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid files were found to process\")\n",
    "\n",
    "# Configuration\n",
    "methods = ['cardinality', 'kepler', 'csv']\n",
    "training_sizes = [50, 400]\n",
    "\n",
    "query_ids = [f'{i}-0' for i in range(1, 34) if i != 29]\n",
    "process_json_files(query_ids, methods, training_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check training directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All required training directories exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def check_training_directories(base_dir=\"0_finished_repo\"):\n",
    "    \"\"\"\n",
    "    Check if training_50 and training_400 directories exist in hints folder\n",
    "    for each query and method\n",
    "    \n",
    "    Parameters:\n",
    "    base_dir (str): Base directory to start search from\n",
    "    \n",
    "    Returns:\n",
    "    dict: Results showing missing directories for each query and method\n",
    "    \"\"\"\n",
    "    # Store results\n",
    "    missing_dirs = {}\n",
    "    \n",
    "    # Find all imdb_*_original directories\n",
    "    query_dirs = glob.glob(os.path.join(base_dir, \"imdb_*_original\"))\n",
    "    # print(query_dirs)\n",
    "    \n",
    "    for query_dir in query_dirs:\n",
    "        # Extract query_id from directory name\n",
    "        query_id = query_dir.split(\"_\")[3]\n",
    "        \n",
    "        # Check each method directory\n",
    "        method_dirs = os.listdir(query_dir)\n",
    "        for method in method_dirs:\n",
    "            # Construct paths to check\n",
    "            hints_path = os.path.join(query_dir, method, \"outputs\", \"results\", query_id)\n",
    "            training_50_path = os.path.join(hints_path, \"training_50\", \"metadata.json\")\n",
    "            # training_400_path = os.path.join(hints_path, \"training_400\", \"metadata.json\")\n",
    "            \n",
    "            # Check if paths exist\n",
    "            paths_status = {\n",
    "                \"training_50\": not os.path.exists(training_50_path)\n",
    "                # \"training_400\": not os.path.exists(training_400_path)\n",
    "            }\n",
    "            \n",
    "            # If either directory is missing, add to results\n",
    "            if any(paths_status.values()):\n",
    "                if query_id not in missing_dirs:\n",
    "                    missing_dirs[query_id] = {}\n",
    "                \n",
    "                missing_dirs[query_id][method] = [\n",
    "                    name for name, is_missing in paths_status.items() if is_missing\n",
    "                ]\n",
    "\n",
    "    return missing_dirs\n",
    "\n",
    "def print_missing_dirs(results):\n",
    "    \"\"\"\n",
    "    Print only cases where training directories are missing\n",
    "    \n",
    "    Parameters:\n",
    "    results (dict): Results from check_training_directories\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"\\nAll required training directories exist.\")\n",
    "        return\n",
    "    \n",
    "    # finished = ['1-0', '2-0', '5-0', '7-0', '16-0', '18-0']\n",
    "    # missing = sorted(list(set(results.keys()) - set(finished)))\n",
    "    \n",
    "    print(\"\\nMissing training directories:\", results)\n",
    "    # print(\"=\"*50)\n",
    "    \n",
    "    for query_id, methods in sorted(results.items()):\n",
    "        for method, missing in methods.items():\n",
    "            missing_dirs_str = \", \".join(missing)\n",
    "            if method in ['cardinality', 'csv', 'kepler']:\n",
    "                print(f\"Query {query_id} - Method {method}: Missing {missing_dirs_str}\")\n",
    "\n",
    "def main():\n",
    "    # Check current directory\n",
    "    results = check_training_directories()\n",
    "    print_missing_dirs(results)\n",
    "    \n",
    "    # Print total count of issues\n",
    "    total_issues = sum(\n",
    "        len(methods) for methods in results.values()\n",
    "    )\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original: check the min-max confidence for each query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 0_original_analysis/confidence_range.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_confidence_ranges(query_id):\n",
    "    methods = ['cardinality', 'csv', 'kepler']\n",
    "    training_sizes = ['50']\n",
    "    results = []\n",
    "    \n",
    "    for method in methods:\n",
    "        for training_size in training_sizes:\n",
    "            base_path = f'imdb_{query_id}_original/{method}/outputs/evaluation/{query_id}/training_{training_size}/confidence_0/predictions/'\n",
    "            \n",
    "            if not os.path.exists(base_path):\n",
    "                print(f\"Warning: Directory not found: {base_path}\")\n",
    "                continue\n",
    "                \n",
    "            pattern = re.compile(f\"{query_id}_.*_batch_0\\.csv$\")\n",
    "            matching_files = []\n",
    "            \n",
    "            for file in os.listdir(base_path):\n",
    "                if pattern.match(file):\n",
    "                    matching_files.append(os.path.join(base_path, file))\n",
    "            \n",
    "            if not matching_files:\n",
    "                print(f\"Warning: No matching files found for {method} - training_{training_size}\")\n",
    "                continue\n",
    "            \n",
    "            all_confidences = []\n",
    "            for file in matching_files:\n",
    "                try:\n",
    "                    df = pd.read_csv(file)\n",
    "                    if 'confidence' in df.columns:\n",
    "                        all_confidences.extend(df['confidence'].tolist())\n",
    "                    else:\n",
    "                        print(f\"Warning: No confidence column in {file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if all_confidences:\n",
    "                min_conf = min(all_confidences)\n",
    "                max_conf = max(all_confidences)\n",
    "                \n",
    "                results.append({\n",
    "                    'query_id': query_id,\n",
    "                    'method': method,\n",
    "                    'training_size': training_size,\n",
    "                    'min_confidence': min_conf,\n",
    "                    'max_confidence': max_conf\n",
    "                })\n",
    "            \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_ids = ['1-0', '2-0', '3-0', '4-0', '5-0', '6-0', '7-0', '8-0', '9-0', '10-0',\n",
    "             '11-0', '12-0', '13-0', '14-0', '15-0', '16-0', '17-0', '18-0', '19-0', '20-0',\n",
    "             '21-0', '22-0', '23-0', '24-0', '25-0', '26-0', '27-0', '28-0', '30-0',\n",
    "             '31-0', '32-0', '33-0']\n",
    "    all_results = []\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        all_results.extend(analyze_confidence_ranges(query_id))\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    output_dir = '0_original_analysis'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'confidence_range.csv')\n",
    "    results_df.to_csv(output_file, index=False, mode='w')\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original & mixture unique plan count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def analyze_workload_files(directory):\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('training_size_50.csv') and 'workload' in filename:\n",
    "            query_id = filename.split('_')[0]\n",
    "            method = filename.split('_')[2]\n",
    "            \n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            plan_counter = Counter(df['plan_id'])\n",
    "            results[query_id][method] = dict(plan_counter)\n",
    "    \n",
    "    return dict(results)\n",
    "\n",
    "def main():\n",
    "    # mixture plans\n",
    "    input_directory = '0_mixture_plans'\n",
    "    output_file = '0_original_analysis/mixture_plan_statistics.json'\n",
    "    result_dict = analyze_workload_files(input_directory)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(result_dict, f, indent=2)\n",
    "    \n",
    "    # original plans\n",
    "    input_directory = '0_original_plans_0'\n",
    "    output_file = '0_original_analysis/original_plan_statistics.json'\n",
    "    result_dict = analyze_workload_files(input_directory)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(result_dict, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original training time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def process_training_times():\n",
    "    base_dir = \"0_finished_repo\"\n",
    "    output_dir = \"0_original_analysis\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = []\n",
    "    \n",
    "    for dir_name in os.listdir(base_dir):\n",
    "        if dir_name.startswith(\"imdb_\"):\n",
    "            query_id = dir_name.split(\"_\")[1]\n",
    "            base_path = Path(base_dir) / dir_name\n",
    "            \n",
    "            for method in ['cardinality', 'kepler', 'csv']:\n",
    "                method_path = base_path / method\n",
    "                \n",
    "                try:\n",
    "                    # get candidate plan generation time\n",
    "                    candidate_metadata_path = method_path / \"outputs\" / \"hints\" / query_id / \"training_50\" / \"candidate_metadata.json\"\n",
    "                    with open(candidate_metadata_path) as f:\n",
    "                        candidate_data = json.load(f)\n",
    "                        candidate_time = candidate_data[\"candidate_plan_generation_time_seconds\"] / 60\n",
    "                    \n",
    "                    # get training data generation time\n",
    "                    training_metadata_path = method_path / \"outputs\" / \"results\" / query_id / \"training_50\" / \"metadata.json\"\n",
    "                    with open(training_metadata_path) as f:\n",
    "                        training_data = json.load(f)\n",
    "                        training_time = training_data[\"training_data_all_time_seconds\"] / 60\n",
    "                    \n",
    "                    # get model prediction time\n",
    "                    eval_metadata_path = method_path / \"outputs\" / \"evaluation\" / query_id / \"training_50\" / \"confidence_0\" / \"metadata.json\"\n",
    "                    with open(eval_metadata_path) as f:\n",
    "                        eval_data = json.load(f)\n",
    "                        prediction_time = eval_data[\"model_prediction_time\"]\n",
    "                    \n",
    "                    # get training & testing time\n",
    "                    model_metadata_path = method_path / \"outputs\" / \"evaluation_single\" / query_id / \"training_50\" / \"confidence_0\" / \"metadata.json\"\n",
    "                    with open(model_metadata_path) as f:\n",
    "                        model_data = json.load(f)\n",
    "                        model_train_time = model_data[\"model_train_time\"]\n",
    "                        model_test_time = model_data[\"model_predict_time (already * 200)\"]\n",
    "                    \n",
    "                    # add result\n",
    "                    results.append({\n",
    "                        \"query_id\": query_id,\n",
    "                        \"method\": method,\n",
    "                        \"candidate_time\": round(candidate_time, 2),\n",
    "                        \"training_time\": round(training_time, 2),\n",
    "                        \"prediction_time\": round(prediction_time, 2),\n",
    "                        \"model_train_time\": round(model_train_time, 2),\n",
    "                        \"model_predict_time\": round(model_test_time, 2)\n",
    "                    })\n",
    "                    \n",
    "                except FileNotFoundError as e:\n",
    "                    print(f\"ERROR: {e}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"ERROR: {e}\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"ERROR: {e}\")\n",
    "    \n",
    "    # training_time.csv\n",
    "    output_file = Path(output_dir) / \"training_time.csv\"\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            \"query_id\",\n",
    "            \"method\",\n",
    "            \"candidate plan generation time (mins)\",\n",
    "            \"robust plan set generation (mins)\",\n",
    "            \"model prediction time (ms)\",\n",
    "            \"model train time (ms)\",\n",
    "            \"model predict time (ms)\"\n",
    "        ])\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for row in sorted(results, key=lambda x: (x[\"query_id\"])):\n",
    "            writer.writerow({\n",
    "                \"query_id\": row[\"query_id\"],\n",
    "                \"method\": row[\"method\"],\n",
    "                \"candidate plan generation time (mins)\": row[\"candidate_time\"],\n",
    "                \"robust plan set generation (mins)\": row[\"training_time\"],\n",
    "                \"model prediction time (ms)\": row[\"prediction_time\"],\n",
    "                \"model train time (ms)\": row[\"model_train_time\"],\n",
    "                \"model predict time (ms)\": row[\"model_predict_time\"]\n",
    "            })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_training_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: [Errno 2] No such file or directory: '0_finished_repo/imdb_3-0_original/kepler/outputs/results/3-0/training_400/metadata.json'\n",
      "ERROR: [Errno 2] No such file or directory: '0_finished_repo/imdb_3-0_original/csv/outputs/evaluation/3-0/training_400/confidence_0/metadata.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def process_training_times():\n",
    "    base_dir = \"0_finished_repo\"\n",
    "    output_dir = \"0_original_analysis\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = []\n",
    "    \n",
    "    for dir_name in os.listdir(base_dir):\n",
    "        if dir_name.startswith(\"imdb_\"):\n",
    "            query_id = dir_name.split(\"_\")[1]\n",
    "            if query_id not in ['1-0', '2-0', '3-0', '5-0', '7-0']:\n",
    "                continue\n",
    "            \n",
    "            base_path = Path(base_dir) / dir_name\n",
    "            \n",
    "            for method in ['cardinality', 'kepler', 'csv']:\n",
    "                method_path = base_path / method\n",
    "                \n",
    "                try:\n",
    "                    # get candidate plan generation time\n",
    "                    candidate_metadata_path = method_path / \"outputs\" / \"hints\" / query_id / \"training_400\" / \"candidate_metadata.json\"\n",
    "                    with open(candidate_metadata_path) as f:\n",
    "                        candidate_data = json.load(f)\n",
    "                        candidate_time = candidate_data[\"candidate_plan_generation_time_seconds\"] / 60\n",
    "                    \n",
    "                    # get training data generation time\n",
    "                    training_metadata_path = method_path / \"outputs\" / \"results\" / query_id / \"training_400\" / \"metadata.json\"\n",
    "                    with open(training_metadata_path) as f:\n",
    "                        training_data = json.load(f)\n",
    "                        training_time = training_data[\"training_data_all_time_seconds\"] / 60\n",
    "                    \n",
    "                    # get model prediction time\n",
    "                    eval_metadata_path = method_path / \"outputs\" / \"evaluation\" / query_id / \"training_400\" / \"confidence_0\" / \"metadata.json\"\n",
    "                    with open(eval_metadata_path) as f:\n",
    "                        eval_data = json.load(f)\n",
    "                        prediction_time = eval_data[\"model_prediction_time\"]\n",
    "                    \n",
    "                    # add result\n",
    "                    results.append({\n",
    "                        \"query_id\": query_id,\n",
    "                        \"method\": method,\n",
    "                        \"candidate_time\": round(candidate_time, 2),\n",
    "                        \"training_time\": round(training_time, 2),\n",
    "                        \"prediction_time\": round(prediction_time, 2)\n",
    "                    })\n",
    "                    \n",
    "                except FileNotFoundError as e:\n",
    "                    print(f\"ERROR: {e}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"ERROR: {e}\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"ERROR: {e}\")\n",
    "    \n",
    "    # training_time.csv\n",
    "    output_file = Path(output_dir) / \"training_time_400.csv\"\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            \"query_id\",\n",
    "            \"method\",\n",
    "            \"candidate plan generation time (mins)\",\n",
    "            \"robust plan set generation (mins)\",\n",
    "            \"model prediction time (ms)\"\n",
    "        ])\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for row in sorted(results, key=lambda x: (x[\"query_id\"])):\n",
    "            writer.writerow({\n",
    "                \"query_id\": row[\"query_id\"],\n",
    "                \"method\": row[\"method\"],\n",
    "                \"candidate plan generation time (mins)\": row[\"candidate_time\"],\n",
    "                \"robust plan set generation (mins)\": row[\"training_time\"],\n",
    "                \"model prediction time (ms)\": row[\"prediction_time\"]\n",
    "            })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_training_times()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample training time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: [Errno 2] No such file or directory: '0_sample_repo/imdb_14-0_sample/category/db_instance_4/csv/outputs/results/14-0/training_50/metadata.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def process_training_times():\n",
    "    base_dir = \"0_sample_repo\"\n",
    "    output_dir = \"0_sample_analysis\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = []\n",
    "    \n",
    "    for dir_name in os.listdir(base_dir):\n",
    "        if dir_name.startswith(\"imdb_\"):\n",
    "            query_id = dir_name.split(\"_\")[1]\n",
    "            if query_id not in ['3-0', '4-0', '14-0', '17-0']:\n",
    "                continue\n",
    "            base_path = Path(base_dir) / dir_name\n",
    "            \n",
    "            for robustness in ['category', 'random', 'sliding']:\n",
    "                for db_i in [1, 4]:\n",
    "                    for method in ['cardinality', 'kepler', 'csv']:\n",
    "                        method_path = base_path / robustness / f\"db_instance_{db_i}\" / method\n",
    "                        \n",
    "                        try:\n",
    "                            # get candidate plan generation time\n",
    "                            candidate_metadata_path = method_path / \"outputs\" / \"hints\" / query_id / \"training_50\" / \"candidate_metadata.json\"\n",
    "                            with open(candidate_metadata_path) as f:\n",
    "                                candidate_data = json.load(f)\n",
    "                                candidate_time = candidate_data[\"candidate_plan_generation_time_seconds\"] / 60\n",
    "                            \n",
    "                            # get training data generation time\n",
    "                            training_metadata_path = method_path / \"outputs\" / \"results\" / query_id / \"training_50\" / \"metadata.json\"\n",
    "                            with open(training_metadata_path) as f:\n",
    "                                training_data = json.load(f)\n",
    "                                training_time = training_data[\"training_data_all_time_seconds\"] / 60\n",
    "                            \n",
    "                            # get model prediction time\n",
    "                            eval_metadata_path = method_path / \"outputs\" / \"evaluation\" / query_id / \"training_50\" / \"confidence_0\" / \"metadata.json\"\n",
    "                            with open(eval_metadata_path) as f:\n",
    "                                eval_data = json.load(f)\n",
    "                                prediction_time = eval_data[\"model_prediction_time\"]\n",
    "                            \n",
    "                            # add result\n",
    "                            results.append({\n",
    "                                \"query_id\": query_id,\n",
    "                                \"robustness\": robustness,\n",
    "                                \"db_instance\": db_i,\n",
    "                                \"method\": method,\n",
    "                                \"candidate_time\": round(candidate_time, 2),\n",
    "                                \"training_time\": round(training_time, 2),\n",
    "                                \"prediction_time\": round(prediction_time, 2)\n",
    "                            })\n",
    "                            \n",
    "                        except FileNotFoundError as e:\n",
    "                            print(f\"ERROR: {e}\")\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"ERROR: {e}\")\n",
    "                        except KeyError as e:\n",
    "                            print(f\"ERROR: {e}\")\n",
    "            \n",
    "    # training_time.csv\n",
    "    output_file = Path(output_dir) / \"training_time.csv\"\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            \"query_id\",\n",
    "            \"robustness\",\n",
    "            \"db_instance\",\n",
    "            \"method\",\n",
    "            \"candidate plan generation time (mins)\",\n",
    "            \"robust plan set generation (mins)\",\n",
    "            \"model prediction time (ms)\"\n",
    "        ])\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for row in sorted(results, key=lambda x: (x[\"query_id\"])):\n",
    "            writer.writerow({\n",
    "                \"query_id\": row[\"query_id\"],\n",
    "                \"robustness\": row[\"robustness\"],\n",
    "                \"db_instance\": row[\"db_instance\"],\n",
    "                \"method\": row[\"method\"],\n",
    "                \"candidate plan generation time (mins)\": row[\"candidate_time\"],\n",
    "                \"robust plan set generation (mins)\": row[\"training_time\"],\n",
    "                \"model prediction time (ms)\": row[\"prediction_time\"]\n",
    "            })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_training_times()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample plans conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 combinations...\n",
      "\n",
      "Processing query_id=30-0, training_size=50, confidence_threshold=0\n",
      "Generated: 0_sample_plans_0/30-0_category_db1_cardinality_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_category_db1_csv_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_category_db1_kepler_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_category_db4_cardinality_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_category_db4_csv_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_category_db4_kepler_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_random_db1_cardinality_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_random_db1_csv_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_random_db1_kepler_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_random_db4_cardinality_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_random_db4_csv_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_random_db4_kepler_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_sliding_db1_cardinality_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_sliding_db1_csv_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_sliding_db1_kepler_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_sliding_db4_cardinality_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_sliding_db4_csv_training_size_50.csv\n",
      "Generated: 0_sample_plans_0/30-0_sliding_db4_kepler_training_size_50.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "def process_data(query_id, training_size, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Process data for a given query_id and training_size combination.\n",
    "    \n",
    "    Args:\n",
    "        query_id (str): The ID of the query to process\n",
    "        training_size (int): The training size to process\n",
    "    \"\"\"\n",
    "    # Define the methods to process - now using the new set of methods\n",
    "    methods = ['cardinality', 'csv', 'kepler']\n",
    "    robustness_types = ['category', 'random', 'sliding']\n",
    "    instance_ids = [1, 4]\n",
    "    \n",
    "    for robustness in robustness_types:\n",
    "        for db_i in instance_ids:\n",
    "            for method in methods:\n",
    "                # Form the predictions file path using the new structure\n",
    "                predictions_path = f'0_sample_repo/imdb_{query_id}_sample/{robustness}/db_instance_{db_i}/{method}/outputs/evaluation/{query_id}/training_{training_size}/confidence_{confidence_threshold}/predictions/'\n",
    "                \n",
    "                # Find the file that matches the pattern starting with query_id and ending with _batch_0.csv\n",
    "                predictions_file = None\n",
    "                try:\n",
    "                    for filename in os.listdir(predictions_path):\n",
    "                        if filename.startswith(f'{query_id}_') and filename.endswith('_batch_0.csv'):\n",
    "                            predictions_file = os.path.join(predictions_path, filename)\n",
    "                            break\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Directory not found for {robustness} db{db_i} {method}, query_id={query_id}, training_size={training_size}\")\n",
    "                    continue\n",
    "                        \n",
    "                if predictions_file is None:\n",
    "                    print(f\"No prediction file found for {method}, query_id={query_id}, training_size={training_size}\")\n",
    "                    continue\n",
    "                \n",
    "                # Read and process the predictions file\n",
    "                predictions_df = pd.read_csv(predictions_file)\n",
    "                \n",
    "                # Read the testing data JSON\n",
    "                testing_json_path = f'0_sample_repo/imdb_{query_id}_sample/{robustness}/db_instance_{db_i}/{method}/inputs/testing/{query_id}_testing_original.json'\n",
    "                try:\n",
    "                    with open(testing_json_path, 'r') as f:\n",
    "                        query_data = json.load(f)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Testing JSON not found for query_id={query_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Process each prediction row\n",
    "                results = []\n",
    "                \n",
    "                for _, pred_row in predictions_df.iterrows():\n",
    "                    params = pred_row['params']\n",
    "                    plan_id = pred_row['plan_id']\n",
    "                    plan_content = pred_row['plan_content']\n",
    "                    \n",
    "                    # Convert params from string representation to list\n",
    "                    params_list = eval(params)\n",
    "                    \n",
    "                    # Get full query instance\n",
    "                    query = query_data[query_id]['query']\n",
    "                    \n",
    "                    # Replace parameters in query with actual values\n",
    "                    for i, param in enumerate(params_list):\n",
    "                        param = str(param).strip()\n",
    "                        pattern = re.compile(rf\"@param{i}\\b\")\n",
    "                        query = pattern.sub(param, query)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'query': query,\n",
    "                        'plan_id': plan_id,\n",
    "                        'plan_content': plan_content\n",
    "                    })\n",
    "                \n",
    "                # Save results to CSV\n",
    "                output_df = pd.DataFrame(results)\n",
    "                os.makedirs(f'0_sample_plans_{confidence_threshold}', exist_ok=True)\n",
    "                output_filename = f'0_sample_plans_{confidence_threshold}/{query_id}_{robustness}_db{db_i}_{method}_training_size_{training_size}.csv'\n",
    "                output_df.to_csv(output_filename, index=False)\n",
    "                print(f\"Generated: {output_filename}\")\n",
    "\n",
    "# Define separate lists for query_ids and training sizes\n",
    "query_ids = ['30-0']\n",
    "training_sizes = [50]\n",
    "confidence_thresholds = [\"0\"]\n",
    "\n",
    "# Generate all combinations using itertools.product\n",
    "combinations = list(product(query_ids, training_sizes, confidence_thresholds))\n",
    "\n",
    "# Display the total number of combinations to be processed\n",
    "print(f\"Processing {len(combinations)} combinations...\")\n",
    "\n",
    "# Process each combination\n",
    "for query_id, training_size, confidence_threshold in combinations:\n",
    "    print(f\"\\nProcessing query_id={query_id}, training_size={training_size}, confidence_threshold={confidence_threshold}\")\n",
    "    process_data(query_id, training_size, confidence_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original plans conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 combinations...\n",
      "\n",
      "Processing query_id=3-0, training_size=400, confidence_threshold=0\n",
      "Generated: 0_original_plans_0/3-0_workload_cardinality_training_size_400.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "def process_data(query_id, training_size, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Process data for a given query_id and training_size combination.\n",
    "    \n",
    "    Args:\n",
    "        query_id (str): The ID of the query to process\n",
    "        training_size (int): The training size to process\n",
    "    \"\"\"\n",
    "    # Define the methods to process - now using the new set of methods\n",
    "    # methods = ['cardinality', 'csv', 'kepler']\n",
    "    methods = ['cardinality']\n",
    "    \n",
    "    for method in methods:\n",
    "        # Form the predictions file path using the new structure\n",
    "        predictions_path = f'0_finished_repo/imdb_{query_id}_original/{method}/outputs/evaluation/{query_id}/training_{training_size}/confidence_{confidence_threshold}/predictions/'\n",
    "        \n",
    "        # Find the file that matches the pattern starting with query_id and ending with _batch_0.csv\n",
    "        predictions_file = None\n",
    "        try:\n",
    "            for filename in os.listdir(predictions_path):\n",
    "                if filename.startswith(f'{query_id}_') and filename.endswith('_batch_0.csv'):\n",
    "                    predictions_file = os.path.join(predictions_path, filename)\n",
    "                    break\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Directory not found for {method}, query_id={query_id}, training_size={training_size}\")\n",
    "            continue\n",
    "                \n",
    "        if predictions_file is None:\n",
    "            print(f\"No prediction file found for {method}, query_id={query_id}, training_size={training_size}\")\n",
    "            continue\n",
    "        \n",
    "        # Read and process the predictions file\n",
    "        predictions_df = pd.read_csv(predictions_file)\n",
    "        \n",
    "        # Read the testing data JSON\n",
    "        testing_json_path = f'0_finished_repo/imdb_{query_id}_original/{method}/inputs/testing/{query_id}_testing_original.json'\n",
    "        try:\n",
    "            with open(testing_json_path, 'r') as f:\n",
    "                query_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Testing JSON not found for query_id={query_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Process each prediction row\n",
    "        results = []\n",
    "        \n",
    "        for _, pred_row in predictions_df.iterrows():\n",
    "            params = pred_row['params']\n",
    "            plan_id = pred_row['plan_id']\n",
    "            plan_content = pred_row['plan_content']\n",
    "            \n",
    "            # Convert params from string representation to list\n",
    "            params_list = eval(params)\n",
    "            \n",
    "            # Get full query instance\n",
    "            query = query_data[query_id]['query']\n",
    "            \n",
    "            # Replace parameters in query with actual values\n",
    "            for i, param in enumerate(params_list):\n",
    "                param = str(param).strip()\n",
    "                pattern = re.compile(rf\"@param{i}\\b\")\n",
    "                query = pattern.sub(param, query)\n",
    "            \n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'plan_id': plan_id,\n",
    "                'plan_content': plan_content\n",
    "            })\n",
    "        \n",
    "        # Save results to CSV\n",
    "        output_df = pd.DataFrame(results)\n",
    "        os.makedirs(f'0_original_plans_{confidence_threshold}', exist_ok=True)\n",
    "        output_filename = f'0_original_plans_{confidence_threshold}/{query_id}_workload_{method}_training_size_{training_size}.csv'\n",
    "        output_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Generated: {output_filename}\")\n",
    "\n",
    "# Define separate lists for query_ids and training sizes\n",
    "query_ids = ['3-0']\n",
    "training_sizes = [400]\n",
    "confidence_thresholds = [\"0\"]\n",
    "\n",
    "# Generate all combinations using itertools.product\n",
    "combinations = list(product(query_ids, training_sizes, confidence_thresholds))\n",
    "\n",
    "# Display the total number of combinations to be processed\n",
    "print(f\"Processing {len(combinations)} combinations...\")\n",
    "\n",
    "# Process each combination\n",
    "for query_id, training_size, confidence_threshold in combinations:\n",
    "    print(f\"\\nProcessing query_id={query_id}, training_size={training_size}, confidence_threshold={confidence_threshold}\")\n",
    "    process_data(query_id, training_size, confidence_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### workload plans conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 combinations...\n",
      "\n",
      "Processing query_id=13-0, test_method=cardinality, train_method=kepler, training_size=50\n",
      "Generated: 0_workload_plans/13-0_cardinality_test_kepler_train_size_50.csv\n",
      "Generated: 0_workload_plans/13-0_cardinality_test_cardinality_train_size_50.csv\n",
      "\n",
      "Processing query_id=13-0, test_method=kepler, train_method=cardinality, training_size=50\n",
      "Generated: 0_workload_plans/13-0_kepler_test_cardinality_train_size_50.csv\n",
      "Generated: 0_workload_plans/13-0_kepler_test_kepler_train_size_50.csv\n",
      "\n",
      "Processing query_id=17-0, test_method=cardinality, train_method=kepler, training_size=50\n",
      "Generated: 0_workload_plans/17-0_cardinality_test_kepler_train_size_50.csv\n",
      "Generated: 0_workload_plans/17-0_cardinality_test_cardinality_train_size_50.csv\n",
      "\n",
      "Processing query_id=17-0, test_method=kepler, train_method=cardinality, training_size=50\n",
      "Generated: 0_workload_plans/17-0_kepler_test_cardinality_train_size_50.csv\n",
      "Generated: 0_workload_plans/17-0_kepler_test_kepler_train_size_50.csv\n",
      "\n",
      "Processing query_id=19-0, test_method=cardinality, train_method=kepler, training_size=50\n",
      "Generated: 0_workload_plans/19-0_cardinality_test_kepler_train_size_50.csv\n",
      "Generated: 0_workload_plans/19-0_cardinality_test_cardinality_train_size_50.csv\n",
      "\n",
      "Processing query_id=19-0, test_method=kepler, train_method=cardinality, training_size=50\n",
      "Generated: 0_workload_plans/19-0_kepler_test_cardinality_train_size_50.csv\n",
      "Generated: 0_workload_plans/19-0_kepler_test_kepler_train_size_50.csv\n",
      "\n",
      "Processing query_id=23-0, test_method=cardinality, train_method=kepler, training_size=50\n",
      "Generated: 0_workload_plans/23-0_cardinality_test_kepler_train_size_50.csv\n",
      "Generated: 0_workload_plans/23-0_cardinality_test_cardinality_train_size_50.csv\n",
      "\n",
      "Processing query_id=23-0, test_method=kepler, train_method=cardinality, training_size=50\n",
      "Generated: 0_workload_plans/23-0_kepler_test_cardinality_train_size_50.csv\n",
      "Generated: 0_workload_plans/23-0_kepler_test_kepler_train_size_50.csv\n",
      "\n",
      "Processing query_id=33-0, test_method=cardinality, train_method=kepler, training_size=50\n",
      "Generated: 0_workload_plans/33-0_cardinality_test_kepler_train_size_50.csv\n",
      "Generated: 0_workload_plans/33-0_cardinality_test_cardinality_train_size_50.csv\n",
      "\n",
      "Processing query_id=33-0, test_method=kepler, train_method=cardinality, training_size=50\n",
      "Generated: 0_workload_plans/33-0_kepler_test_cardinality_train_size_50.csv\n",
      "Generated: 0_workload_plans/33-0_kepler_test_kepler_train_size_50.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "def process_data(query_id, test_method, train_method, training_size, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Process and compare data from workload testing and original paths.\n",
    "    \n",
    "    Args:\n",
    "        query_id (str): The ID of the query to process\n",
    "        test_method (str): Testing method\n",
    "        train_method (str): Training method\n",
    "        training_size (int): The training size to process\n",
    "        confidence_threshold (str): Confidence threshold\n",
    "    \"\"\"\n",
    "    # Define paths for both workload and original data\n",
    "    workload_path = f'0_workload_testing/{query_id}/{test_method}_test_{train_method}_train/training_{training_size}/confidence_{confidence_threshold}/predictions/'\n",
    "    original_path = f'imdb_{query_id}_original/{test_method}/outputs/evaluation/{query_id}/training_{training_size}/confidence_{confidence_threshold}/predictions/'\n",
    "    \n",
    "    # Find matching files in both directories\n",
    "    try:\n",
    "        workload_file = None\n",
    "        original_file = None\n",
    "        \n",
    "        for filename in os.listdir(workload_path):\n",
    "            if filename.startswith(f'{query_id}_') and filename.endswith('_batch_0.csv'):\n",
    "                workload_file = os.path.join(workload_path, filename)\n",
    "                break\n",
    "                \n",
    "        for filename in os.listdir(original_path):\n",
    "            if filename.startswith(f'{query_id}_') and filename.endswith('_batch_0.csv'):\n",
    "                original_file = os.path.join(original_path, filename)\n",
    "                break\n",
    "                \n",
    "        if workload_file is None or original_file is None:\n",
    "            print(f\"Files not found for query_id={query_id}\")\n",
    "            return\n",
    "            \n",
    "        # Read both CSV files\n",
    "        workload_df = pd.read_csv(workload_file)\n",
    "        original_df = pd.read_csv(original_file)\n",
    "        \n",
    "        # Query data\n",
    "        # f'imdb_{query_id}_original/{method}/inputs/testing/{query_id}_testing_original.json'\n",
    "        query_json_path = f'imdb_{query_id}_original/{test_method}/inputs/testing/{query_id}_testing_original.json'\n",
    "        \n",
    "        try:\n",
    "            with open(query_json_path, 'r') as f:\n",
    "                query_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Testing JSON not found for query_id={query_id}\")\n",
    "            return\n",
    "        \n",
    "        # Compare params order\n",
    "        workload_params = workload_df['params'].tolist()\n",
    "        original_params = original_df['params'].tolist()\n",
    "        for i, (w_param, o_param) in enumerate(zip(workload_params, original_params)):\n",
    "            if w_param != o_param:\n",
    "                print(f\"Mismatch at position {i}:\")\n",
    "                print(f\"Workload : {w_param}\")\n",
    "                print(f\"Original: {o_param}\")\n",
    "                print(\"---\")\n",
    "                raise ValueError(\"ERROR\")\n",
    "\n",
    "        \n",
    "        # Initialize separate result lists for workload and original\n",
    "        workload_results = []\n",
    "        original_results = []\n",
    "        \n",
    "        # Process workload results\n",
    "        for _, row in workload_df.iterrows():\n",
    "            params = row['params']\n",
    "            params_list = eval(params)\n",
    "            query = query_data[query_id]['query']\n",
    "            \n",
    "            for i, param in enumerate(params_list):\n",
    "                param = str(param).strip()\n",
    "                pattern = re.compile(rf\"@param{i}\\b\")\n",
    "                query = pattern.sub(param, query)\n",
    "            \n",
    "            workload_results.append({\n",
    "                'query': query,\n",
    "                'plan_id': row['plan_id'],\n",
    "                'plan_content': row['plan_content']\n",
    "            })\n",
    "            \n",
    "        # Process original results\n",
    "        for _, row in original_df.iterrows():\n",
    "            params = row['params']\n",
    "            params_list = eval(params)\n",
    "            query = query_data[query_id]['query']\n",
    "            \n",
    "            for i, param in enumerate(params_list):\n",
    "                param = str(param).strip()\n",
    "                pattern = re.compile(rf\"@param{i}\\b\")\n",
    "                query = pattern.sub(param, query)\n",
    "            \n",
    "            original_results.append({\n",
    "                'query': query,\n",
    "                'plan_id': row['plan_id'],\n",
    "                'plan_content': row['plan_content']\n",
    "            })\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs('0_workload_plans', exist_ok=True)\n",
    "        \n",
    "        # Save workload results\n",
    "        workload_output = pd.DataFrame(workload_results)\n",
    "        workload_filename = f'0_workload_plans/{query_id}_{test_method}_test_{train_method}_train_size_{training_size}.csv'\n",
    "        workload_output.to_csv(workload_filename, index=False)\n",
    "        \n",
    "        # Save original results\n",
    "        original_output = pd.DataFrame(original_results)\n",
    "        original_filename = f'0_workload_plans/{query_id}_{test_method}_test_{test_method}_train_size_{training_size}.csv'\n",
    "        original_output.to_csv(original_filename, index=False)\n",
    "        \n",
    "        print(f\"Generated: {workload_filename}\")\n",
    "        print(f\"Generated: {original_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query_id={query_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "##############\n",
    "query_ids = ['13-0', '17-0', '19-0', '23-0', '33-0']\n",
    "test_methods = ['cardinality', 'kepler']\n",
    "train_methods = ['cardinality', 'kepler']\n",
    "confidence_threshold = \"0\"\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = []\n",
    "for query_id in query_ids:\n",
    "    for test_method in test_methods:\n",
    "        for train_method in train_methods:\n",
    "            if test_method != train_method:\n",
    "                combinations.append((query_id, test_method, train_method, training_sizes[0]))\n",
    "\n",
    "print(f\"Processing {len(combinations)} combinations...\")\n",
    "\n",
    "# Process each combination\n",
    "for query_id, test_method, train_method, training_size in combinations:\n",
    "    print(f\"\\nProcessing query_id={query_id}, test_method={test_method}, train_method={train_method}, training_size={training_size}\")\n",
    "    process_data(query_id, test_method, train_method, training_size, confidence_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mixture plans conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 combinations...\n",
      "\n",
      "Processing query_id=3-0, training_size=400\n",
      "Generated: 0_mixture_plans/3-0_workload_cardinality_training_size_400.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "def process_data(query_id, training_size):\n",
    "    \"\"\"\n",
    "    Process data for a given query_id and training_size combination.\n",
    "    \n",
    "    Args:\n",
    "        query_id (str): The ID of the query to process\n",
    "        training_size (int): The training size to process\n",
    "    \"\"\"\n",
    "    # Define the methods to process - now using the new set of methods\n",
    "    # methods = ['cardinality', 'csv', 'kepler']\n",
    "    methods = ['cardinality']\n",
    "    \n",
    "    for method in methods:\n",
    "        # Form the predictions file path using the new structure\n",
    "        predictions_path = f'0_mixture_test/{query_id}/{method}/training_{training_size}/confidence_0/predictions/'\n",
    "        \n",
    "        # Find the file that matches the pattern starting with query_id and ending with _batch_0.csv\n",
    "        predictions_file = None\n",
    "        try:\n",
    "            for filename in os.listdir(predictions_path):\n",
    "                if filename.startswith(f'{query_id}_') and filename.endswith('_batch_0.csv'):\n",
    "                    predictions_file = os.path.join(predictions_path, filename)\n",
    "                    break\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Directory not found for {method}, query_id={query_id}, training_size={training_size}\")\n",
    "            continue\n",
    "                \n",
    "        if predictions_file is None:\n",
    "            print(f\"No prediction file found for {method}, query_id={query_id}, training_size={training_size}\")\n",
    "            continue\n",
    "        \n",
    "        # Read and process the predictions file\n",
    "        predictions_df = pd.read_csv(predictions_file)\n",
    "        \n",
    "        # Read the testing data JSON\n",
    "        testing_json_path = f'0_mixture_test/{query_id}/{query_id}_mixture_test.json'\n",
    "        try:\n",
    "            with open(testing_json_path, 'r') as f:\n",
    "                query_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Testing JSON not found for query_id={query_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Process each prediction row\n",
    "        results = []\n",
    "        for _, pred_row in predictions_df.iterrows():\n",
    "            params = pred_row['params']\n",
    "            plan_id = pred_row['plan_id']\n",
    "            plan_content = pred_row['plan_content']\n",
    "            \n",
    "            # Convert params from string representation to list\n",
    "            params_list = eval(params)\n",
    "            \n",
    "            # Get full query instance\n",
    "            query = query_data[query_id]['query']\n",
    "            \n",
    "            # Replace parameters in query with actual values\n",
    "            for i, param in enumerate(params_list):\n",
    "                param = str(param).strip()\n",
    "                pattern = re.compile(rf\"@param{i}\\b\")\n",
    "                query = pattern.sub(param, query)\n",
    "            \n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'plan_id': plan_id,\n",
    "                'plan_content': plan_content\n",
    "            })\n",
    "        \n",
    "        # Save results to CSV\n",
    "        output_df = pd.DataFrame(results)\n",
    "        os.makedirs('0_mixture_plans', exist_ok=True)\n",
    "        output_filename = f'0_mixture_plans/{query_id}_workload_{method}_training_size_{training_size}.csv'\n",
    "        output_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Generated: {output_filename}\")\n",
    "\n",
    "# Define separate lists for query_ids and training sizes\n",
    "query_ids = ['3-0']\n",
    "training_sizes = [400]\n",
    "\n",
    "# Generate all combinations using itertools.product\n",
    "combinations = list(product(query_ids, training_sizes))\n",
    "\n",
    "# Display the total number of combinations to be processed\n",
    "print(f\"Processing {len(combinations)} combinations...\")\n",
    "\n",
    "# Process each combination\n",
    "for query_id, training_size in combinations:\n",
    "    print(f\"\\nProcessing query_id={query_id}, training_size={training_size}\")\n",
    "    process_data(query_id, training_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original single testing param file generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created: 0_finished_repo/imdb_1-0_original/cardinality/inputs/testing/1-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_1-0_original/csv/inputs/testing/1-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_1-0_original/kepler/inputs/testing/1-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_2-0_original/cardinality/inputs/testing/2-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_2-0_original/csv/inputs/testing/2-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_2-0_original/kepler/inputs/testing/2-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_3-0_original/cardinality/inputs/testing/3-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_3-0_original/csv/inputs/testing/3-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_3-0_original/kepler/inputs/testing/3-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_4-0_original/cardinality/inputs/testing/4-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_4-0_original/csv/inputs/testing/4-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_4-0_original/kepler/inputs/testing/4-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_5-0_original/cardinality/inputs/testing/5-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_5-0_original/csv/inputs/testing/5-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_5-0_original/kepler/inputs/testing/5-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_6-0_original/cardinality/inputs/testing/6-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_6-0_original/csv/inputs/testing/6-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_6-0_original/kepler/inputs/testing/6-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_7-0_original/cardinality/inputs/testing/7-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_7-0_original/csv/inputs/testing/7-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_7-0_original/kepler/inputs/testing/7-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_8-0_original/cardinality/inputs/testing/8-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_8-0_original/csv/inputs/testing/8-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_8-0_original/kepler/inputs/testing/8-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_9-0_original/cardinality/inputs/testing/9-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_9-0_original/csv/inputs/testing/9-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_9-0_original/kepler/inputs/testing/9-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_10-0_original/cardinality/inputs/testing/10-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_10-0_original/csv/inputs/testing/10-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_10-0_original/kepler/inputs/testing/10-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_11-0_original/cardinality/inputs/testing/11-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_11-0_original/csv/inputs/testing/11-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_11-0_original/kepler/inputs/testing/11-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_12-0_original/cardinality/inputs/testing/12-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_12-0_original/csv/inputs/testing/12-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_12-0_original/kepler/inputs/testing/12-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_13-0_original/cardinality/inputs/testing/13-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_13-0_original/csv/inputs/testing/13-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_13-0_original/kepler/inputs/testing/13-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_14-0_original/cardinality/inputs/testing/14-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_14-0_original/csv/inputs/testing/14-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_14-0_original/kepler/inputs/testing/14-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_15-0_original/cardinality/inputs/testing/15-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_15-0_original/csv/inputs/testing/15-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_15-0_original/kepler/inputs/testing/15-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_16-0_original/cardinality/inputs/testing/16-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_16-0_original/csv/inputs/testing/16-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_16-0_original/kepler/inputs/testing/16-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_17-0_original/cardinality/inputs/testing/17-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_17-0_original/csv/inputs/testing/17-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_17-0_original/kepler/inputs/testing/17-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_18-0_original/cardinality/inputs/testing/18-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_18-0_original/csv/inputs/testing/18-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_18-0_original/kepler/inputs/testing/18-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_19-0_original/cardinality/inputs/testing/19-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_19-0_original/csv/inputs/testing/19-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_19-0_original/kepler/inputs/testing/19-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_20-0_original/cardinality/inputs/testing/20-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_20-0_original/csv/inputs/testing/20-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_20-0_original/kepler/inputs/testing/20-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_21-0_original/cardinality/inputs/testing/21-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_21-0_original/csv/inputs/testing/21-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_21-0_original/kepler/inputs/testing/21-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_22-0_original/cardinality/inputs/testing/22-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_22-0_original/csv/inputs/testing/22-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_22-0_original/kepler/inputs/testing/22-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_23-0_original/cardinality/inputs/testing/23-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_23-0_original/csv/inputs/testing/23-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_23-0_original/kepler/inputs/testing/23-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_24-0_original/cardinality/inputs/testing/24-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_24-0_original/csv/inputs/testing/24-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_24-0_original/kepler/inputs/testing/24-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_25-0_original/cardinality/inputs/testing/25-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_25-0_original/csv/inputs/testing/25-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_25-0_original/kepler/inputs/testing/25-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_26-0_original/cardinality/inputs/testing/26-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_26-0_original/csv/inputs/testing/26-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_26-0_original/kepler/inputs/testing/26-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_27-0_original/cardinality/inputs/testing/27-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_27-0_original/csv/inputs/testing/27-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_27-0_original/kepler/inputs/testing/27-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_28-0_original/cardinality/inputs/testing/28-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_28-0_original/csv/inputs/testing/28-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_28-0_original/kepler/inputs/testing/28-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_30-0_original/cardinality/inputs/testing/30-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_30-0_original/csv/inputs/testing/30-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_30-0_original/kepler/inputs/testing/30-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_31-0_original/cardinality/inputs/testing/31-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_31-0_original/csv/inputs/testing/31-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_31-0_original/kepler/inputs/testing/31-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_32-0_original/cardinality/inputs/testing/32-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_32-0_original/csv/inputs/testing/32-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_32-0_original/kepler/inputs/testing/32-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_33-0_original/cardinality/inputs/testing/33-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_33-0_original/csv/inputs/testing/33-0_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: 0_finished_repo/imdb_33-0_original/kepler/inputs/testing/33-0_testing_original_single.json\n",
      "Processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_json_file(query_id, method):\n",
    "    base_path = \"0_finished_repo\"\n",
    "    input_path = os.path.join(\n",
    "        base_path,\n",
    "        f\"imdb_{query_id}_original\",\n",
    "        method,\n",
    "        \"inputs\",\n",
    "        \"testing\",\n",
    "        f\"{query_id}_testing_original.json\"\n",
    "    )\n",
    "    \n",
    "    output_path = os.path.join(\n",
    "        base_path,\n",
    "        f\"imdb_{query_id}_original\",\n",
    "        method,\n",
    "        \"inputs\",\n",
    "        \"testing\",\n",
    "        f\"{query_id}_testing_original_single.json\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(input_path):\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "            \n",
    "        # read original file\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        if query_id not in data:\n",
    "            raise KeyError(f\"Query ID {query_id} not found in the JSON data\")\n",
    "            \n",
    "        if \"params\" not in data[query_id]:\n",
    "            raise KeyError(f\"'params' not found in data[{query_id}]\")\n",
    "            \n",
    "        # only keep the first one\n",
    "        if len(data[query_id][\"params\"]) > 0:\n",
    "            data[query_id][\"params\"] = [data[query_id][\"params\"][0]]\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"Successfully created: {output_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    for query_id in [f\"{i}-0\" for i in range(1, 34) if i != 29]:\n",
    "        for method in ['cardinality', 'csv', 'kepler']:\n",
    "            success = process_json_file(query_id, method)\n",
    "            if success:\n",
    "                print(\"Processing completed successfully\")\n",
    "            else:\n",
    "                print(\"Processing failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query template (1 predicate with multiple join_table_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: ['1-0', '10-0', '11-0', '12-0', '14-0', '15-0', '16-0', '18-0', '19-0', '20-0', '21-0', '22-0', '23-0', '24-0', '25-0', '26-0', '27-0', '28-0', '3-0', '30-0', '31-0', '33-0', '4-0', '5-0', '6-0', '7-0', '8-0', '9-0']\n",
      "Ignore: ['13-0', '17-0', '2-0', '32-0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Create query_ids using set comprehension\n",
    "query_ids = {f\"{i}-0\" for i in range(1, 34) if i != 29}\n",
    "overall = set()\n",
    "\n",
    "for query_id in query_ids:\n",
    "    file_path = f\"imdb_input/original_template/{query_id}.json\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        predicates = data[query_id]['predicates']\n",
    "        \n",
    "        # Check if any predicate has more than 1 join_tables_alias\n",
    "        for predicate in predicates:\n",
    "            if len(predicate['join_tables_alias']) > 1:\n",
    "                overall.add(query_id)\n",
    "                break\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for query {query_id}\")\n",
    "\n",
    "print(\"Overall:\", sorted(list(overall)))\n",
    "print(\"Ignore:\", sorted(list(query_ids - overall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query template (same table with multiple predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-0: [\"(mc.note LIKE '%(200%)%' OR mc.note LIKE '%(France)%') AND mc.note NOT LIKE '%(USA)%'\", \"ct.kind = 'production companies'\", \"it.info = 'votes'\"],\n",
      "Contains AND: True\n",
      "\n",
      "2-0: [\"cn.country_code = '[us]'\", \"k.keyword = 'marvel-cinematic-universe'\"],\n",
      "Contains AND: False\n",
      "\n",
      "3-0: [\"k.keyword LIKE '%sequel%'\", \"mi.info IN ('Horror', 'Action', 'Sci-Fi', 'Thriller', 'Crime', 'War')\", \"t.production_year > 2000\"],\n",
      "Contains AND: False\n",
      "\n",
      "4-0: [\"it.info = 'rating'\", \"k.keyword LIKE '%sequel%'\", \"mi_idx.info > '6.5'\", \"t.production_year > 2010\"],\n",
      "Contains AND: False\n",
      "\n",
      "5-0: [\"ct.kind = 'distributors'\", \"mc.note LIKE '%(VHS)%' AND mc.note LIKE '%(Japan)%'\", \"mi.info IN ('Horror', 'Action', 'Sci-Fi', 'Thriller', 'Crime', 'War')\", \"t.production_year > 2000\"],\n",
      "Contains AND: True\n",
      "\n",
      "6-0: [\"k.keyword = 'sequel'\", \"n.name LIKE '%G%'\", \"t.production_year > 2005\"],\n",
      "Contains AND: False\n",
      "\n",
      "7-0: [\"an.name LIKE 'H%'\", \"it.info = 'mini biography'\", \"lt.link = 'features'\", \"n.gender = 'm' OR (n.gender = 'f' AND n.name LIKE '%M%')\", \"pi.note = 'Volker Boehm'\"],\n",
      "Contains AND: True\n",
      "\n",
      "8-0: [\"ci.note = '(voice)'\", \"cn.country_code = '[us]'\", \"mc.note LIKE '%(worldwide)%' AND mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%'\", \"n.name LIKE '%K%' AND n.name NOT LIKE '%Yu%'\", \"rt.role = 'actress'\"],\n",
      "Contains AND: True\n",
      "\n",
      "9-0: [\"(mc.note LIKE '%(worldwide)%' OR mc.note LIKE '%(co-production)%')\", \"ci.note IN ('(voice)', '(voice: Japanese version)', '(voice) (uncredited)', '(voice: English version)')\", \"cn.country_code = '[us]'\", \"n.gender = 'f' AND n.name LIKE '%L%'\", \"rt.role = 'actress'\", \"t.production_year > 2000 AND t.production_year < 2010\"],\n",
      "Contains AND: True\n",
      "\n",
      "10-0: [\"ci.note LIKE '%(uncredited)%' AND ci.note LIKE '%(voice)%'\", \"cn.country_code = '[us]'\", \"rt.role = 'actress'\", \"t.production_year > 2000\"],\n",
      "Contains AND: True\n",
      "\n",
      "11-0: [\"(cn.name LIKE '%Film%' OR cn.name LIKE 'Twentieth Century Fox%') AND cn.country_code != '[pl]'\", \"ct.kind = 'production companies'\", \"k.keyword = 'sequel'\", \"lt.link LIKE '%follow%'\", \"t.production_year > 2000 AND t.production_year < 2010\"],\n",
      "Contains AND: True\n",
      "\n",
      "12-0: [\"cn.country_code = '[us]'\", \"ct.kind = 'production companies'\", \"it.info = 'countries'\", \"it.info = 'rating'\", \"mi.info IN ('Sweden', 'Norway', 'Germany', 'Denmark', 'Swedish', 'Denish', 'Norwegian', 'German')\", \"mi_idx.info > '9.0'\", \"t.production_year > 2000 AND t.production_year < 2010\"],\n",
      "Contains AND: True\n",
      "\n",
      "13-0: [\"cn.country_code = '[us]'\", \"ct.kind = 'production companies'\", \"it.info = 'votes'\", \"it.info = 'countries'\", \"kt.kind = 'movie'\"],\n",
      "Contains AND: False\n",
      "\n",
      "14-0: [\"it.info = 'genres'\", \"it.info = 'rating'\", \"k.keyword IN ('murder', 'murder-in-title', 'blood', 'violence')\", \"kt.kind = 'movie'\", \"mi.info IN ('Horror', 'Thriller')\", \"mi_idx.info < '8.5'\", \"t.production_year > 1950\"],\n",
      "Contains AND: False\n",
      "\n",
      "15-0: [\"cn.country_code = '[us]'\", \"it.info = 'release dates'\", \"mc.note LIKE '%(2006)%' AND mc.note LIKE '%(USA)%'\", \"mi.note LIKE '%internet%' AND mi.info LIKE 'USA:%201%'\", \"t.production_year > 2005\"],\n",
      "Contains AND: True\n",
      "\n",
      "16-0: [\"cn.country_code = '[us]'\", \"k.keyword = 'computer-animation'\", \"t.episode_nr >= 5 AND t.episode_nr < 100\"],\n",
      "Contains AND: True\n",
      "\n",
      "17-0: [\"cn.country_code = '[us]'\", \"k.keyword = 'character-name-in-title'\", \"n.name LIKE '%I%'\"],\n",
      "Contains AND: False\n",
      "\n",
      "18-0: [\"ci.note IN ('(writer)', '(head writer)', '(written by)', '(story)', '(story editor)')\", \"it.info = 'countries'\", \"it.info = 'rating'\", \"n.gender = 'f' AND n.name LIKE 'A%'\"],\n",
      "Contains AND: True\n",
      "\n",
      "19-0: [\"(mc.note LIKE '%(Japan)%' OR mc.note LIKE '%(200%)%')\", \"(mi.info LIKE 'Japan:%201%' OR mi.info LIKE 'USA:% 200%')\", \"ci.note IN ('(voice)', '(voice: Japanese version)', '(voice) (uncredited)', '(voice: English version)')\", \"cn.country_code = '[jp]'\", \"it.info = 'release dates'\", \"n.gender = 'f' AND n.name LIKE 'M%'\", \"rt.role = 'actress'\", \"t.production_year > 2009 AND t.production_year < 2010\"],\n",
      "Contains AND: True\n",
      "\n",
      "20-0: [\"(chn.name LIKE '%Man%' OR chn.name LIKE '%man%') AND chn.name NOT LIKE '%Sherlock%'\", \"cct.kind = 'cast'\", \"cct.kind LIKE '%complete%'\", \"k.keyword IN ('superhero', 'sequel', 'second-part', 'marvel-comics', 'based-on-comic', 'tv-special', 'fight', 'violence')\", \"kt.kind = 'movie'\", \"t.production_year > 2000\"],\n",
      "Contains AND: True\n",
      "\n",
      "21-0: [\"(cn.name LIKE '%Warner%' OR cn.name LIKE 'Lionsgate%') AND cn.country_code != '[pl]'\", \"ct.kind = 'production companies'\", \"k.keyword = 'character-name-in-title'\", \"lt.link LIKE '%follow%'\", \"mi.info IN ('Horror', 'Action', 'Sci-Fi', 'Thriller', 'Crime', 'War')\", \"t.production_year > 1990 AND t.production_year < 1995\"],\n",
      "Contains AND: True\n",
      "\n",
      "22-0: [\"cn.country_code != '[pl]'\", \"it.info = 'genres'\", \"it.info = 'votes'\", \"k.keyword IN ('murder', 'murder-in-title', 'blood', 'violence')\", \"kt.kind IN ('movie')\", \"mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%' AND mc.note LIKE '%(VHS)%'\", \"mi.info IN ('Drama', 'Horror', 'Western', 'Family')\", \"mi_idx.info < '7.0'\", \"t.production_year > 2000\"],\n",
      "Contains AND: True\n",
      "\n",
      "23-0: [\"(mi.info LIKE 'USA:% 200%' OR mi.info LIKE 'Japan:%2007%') AND mi.note LIKE '%internet%'\", \"cct.kind = 'complete+verified'\", \"cn.country_code = '[us]'\", \"it.info = 'release dates'\", \"kt.kind IN ('movie', 'episode')\", \"t.production_year > 2000\"],\n",
      "Contains AND: True\n",
      "\n",
      "24-0: [\"(mi.info LIKE 'USA:% 200%' OR mi.info LIKE 'Japan:%201%')\", \"ci.note IN ('(voice)', '(voice: Japanese version)', '(voice) (uncredited)', '(voice: English version)')\", \"cn.country_code = '[us]'\", \"it.info = 'release dates'\", \"k.keyword IN ('murder', 'murder-in-title', 'blood', 'violence')\", \"n.gender = 'f' AND n.name LIKE '%I%'\", \"rt.role = 'actress'\", \"t.production_year > 2000\"],\n",
      "Contains AND: True\n",
      "\n",
      "25-0: [\"ci.note IN ('(writer)', '(head writer)', '(written by)', '(story)', '(story editor)')\", \"it.info = 'genres'\", \"it.info = 'rating'\", \"k.keyword IN ('murder', 'murder-in-title', 'blood', 'violence')\", \"mi.info = 'Horror'\", \"n.gender = 'm'\"],\n",
      "Contains AND: False\n",
      "\n",
      "26-0: [\"(chn.name LIKE '%man%' OR chn.name LIKE '%Iron%Man%')\", \"cct.kind = 'cast'\", \"cct.kind = 'complete+verified'\", \"it.info = 'votes'\", \"k.keyword IN ('superhero', 'sequel', 'second-part', 'marvel-comics', 'based-on-comic', 'tv-special', 'fight', 'violence')\", \"kt.kind = 'movie'\", \"mi_idx.info > '8.0'\", \"t.production_year > 2005\"],\n",
      "Contains AND: False\n",
      "\n",
      "27-0: [\"(cn.name LIKE '%Film%' OR cn.name LIKE 'Lionsgate%') AND cn.country_code != '[pl]'\", \"cct.kind IN ('cast', 'crew')\", \"cct.kind = 'complete+verified'\", \"ct.kind = 'production companies'\", \"k.keyword = 'sequel'\", \"lt.link LIKE '%follow%'\", \"mi.info IN ('Sweden', 'Norway', 'Germany', 'Denmark', 'Swedish', 'Denish', 'Norwegian', 'German', 'USA', 'American')\", \"t.production_year > 1990 AND t.production_year < 2008\"],\n",
      "Contains AND: True\n",
      "\n",
      "28-0: [\"cct.kind = 'cast'\", \"cct.kind != 'complete+verified'\", \"cn.country_code != '[pl]'\", \"it.info = 'genres'\", \"it.info = 'rating'\", \"k.keyword IN ('murder', 'violence', 'blood', 'gore', 'death', 'female-nudity', 'hospital')\", \"kt.kind IN ('movie', 'episode')\", \"mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%' AND mc.note LIKE '%(France)%'\", \"mi.info IN ('Horror', 'Action', 'Sci-Fi', 'Thriller', 'Crime', 'War')\", \"mi_idx.info < '3.0'\", \"t.production_year > 2000\"],\n",
      "Contains AND: True\n",
      "\n",
      "30-0: [\"cct.kind IN ('cast', 'crew')\", \"cct.kind = 'complete+verified'\", \"ci.note IN ('(writer)', '(head writer)', '(written by)', '(story)', '(story editor)')\", \"it.info = 'countries'\", \"it.info = 'rating'\", \"k.keyword IN ('murder', 'violence', 'blood', 'gore', 'death', 'female-nudity', 'hospital')\", \"mi.info IN ('Sweden', 'Germany', 'Swedish', 'German')\", \"n.gender = 'm'\", \"t.production_year > 1990\"],\n",
      "Contains AND: False\n",
      "\n",
      "31-0: [\"ci.note IN ('(writer)', '(head writer)', '(written by)', '(story)', '(story editor)')\", \"cn.name LIKE 'Twentieth Century Fox%'\", \"it.info = 'genres'\", \"it.info = 'votes'\", \"k.keyword IN ('hero', 'martial-arts', 'hand-to-hand-combat')\", \"mi.info IN ('Horror', 'Action', 'Sci-Fi', 'Thriller', 'Crime', 'War')\", \"n.gender = 'm'\"],\n",
      "Contains AND: False\n",
      "\n",
      "32-0: [\"k.keyword = 'sequel'\"],\n",
      "Contains AND: False\n",
      "\n",
      "33-0: [\"cn.country_code = '[us]'\", \"it.info = 'rating'\", \"it.info = 'votes'\", \"kt.kind IN ('movie', 'episode')\", \"kt.kind IN ('tv series','episode')\", \"lt.link IN ('references', 'referenced in', 'features', 'featured in')\", \"mi_idx.info < '8.5'\", \"t.production_year > 2008 AND t.production_year < 2010\"],\n",
      "Contains AND: True\n",
      "\n",
      "Overall: ['1-0', '5-0', '7-0', '8-0', '9-0', '10-0', '11-0', '12-0', '15-0', '16-0', '18-0', '19-0', '20-0', '21-0', '22-0', '23-0', '24-0', '27-0', '28-0', '33-0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "query_ids = [f\"{i}-0\" for i in range(1, 34) if i != 29]\n",
    "overall = []\n",
    "\n",
    "for query_id in query_ids:\n",
    "    file_path = f\"0_finished_repo/imdb_{query_id}_original/csv/inputs/PQO/join_predicates/{query_id}_50_training.txt\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            contains_and = \"AND\" in first_line\n",
    "            print(f\"{query_id}: {first_line}\")\n",
    "            print(f\"Contains AND: {contains_and}\\n\")\n",
    "            if contains_and: overall.append(query_id)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for query {query_id}\")\n",
    "\n",
    "print(\"Overall:\", overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query template OR condition check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "found 12 contains OR predicate:\n",
      "\n",
      "name: 1-0\n",
      "query: SELECT *\n",
      " FROM company_type AS ct,\n",
      " info_type AS it,\n",
      " movie_companies AS mc,\n",
      " movie_info_idx AS mi_idx,\n",
      " title AS t\n",
      " WHERE ct.id = mc.company_type_id\n",
      " AND t.id = mc.movie_id\n",
      " AND t.id = mi_idx.movie_id\n",
      " AND mc.movie_id = mi_idx.movie_id\n",
      " AND it.id = mi_idx.info_type_id\n",
      " AND ct.kind = '@param0'\n",
      " AND it.info = '@param1'\n",
      " AND mc.note NOT LIKE '@param2'\n",
      " AND (mc.note LIKE '@param3' OR mc.note LIKE '@param4');\n",
      "\n",
      "name: 11-0\n",
      "query: SELECT *\n",
      "FROM company_name AS cn,\n",
      " company_type AS ct,\n",
      " keyword AS k,\n",
      " link_type AS lt,\n",
      " movie_companies AS mc,\n",
      " movie_keyword AS mk,\n",
      " movie_link AS ml,\n",
      " title AS t\n",
      "WHERE lt.id = ml.link_type_id\n",
      " AND ml.movie_id = t.id\n",
      " AND t.id = mk.movie_id\n",
      " AND mk.keyword_id = k.id\n",
      " AND t.id = mc.movie_id\n",
      " AND mc.company_type_id = ct.id\n",
      " AND mc.company_id = cn.id\n",
      " AND ml.movie_id = mk.movie_id\n",
      " AND ml.movie_id = mc.movie_id\n",
      " AND mk.movie_id = mc.movie_id\n",
      "AND cn.country_code != '@param0'\n",
      "AND (cn.name LIKE '@param1' OR cn.name LIKE '@param2')\n",
      "AND ct.kind = '@param3'\n",
      "AND k.keyword = '@param4'\n",
      "AND lt.link LIKE '@param5'\n",
      "AND mc.note IS NULL\n",
      "AND t.production_year BETWEEN @param6 AND @param7;\n",
      "\n",
      "name: 19-0\n",
      "query: SELECT *\n",
      "FROM aka_name AS an,\n",
      "   char_name AS chn,\n",
      "   cast_info AS ci,\n",
      "   company_name AS cn,\n",
      "   info_type AS it,\n",
      "  movie_companies AS mc,\n",
      "   movie_info AS mi,\n",
      "   name AS n,\n",
      "   role_type AS rt,\n",
      "   title AS t\n",
      "WHERE t.id = mi.movie_id\n",
      "AND t.id = mc.movie_id\n",
      "AND t.id = ci.movie_id\n",
      "AND mc.movie_id = ci.movie_id\n",
      "AND mc.movie_id = mi.movie_id\n",
      "AND mi.movie_id = ci.movie_id\n",
      "AND cn.id = mc.company_id\n",
      "AND it.id = mi.info_type_id\n",
      "AND n.id = ci.person_id\n",
      "AND rt.id = ci.role_id\n",
      "AND n.id = an.person_id\n",
      "AND ci.person_id = an.person_id\n",
      "AND chn.id = ci.person_role_id\n",
      "AND ci.note IN ('@param0')\n",
      "AND cn.country_code = '@param1'\n",
      "AND it.info = '@param2'\n",
      "AND mc.note IS NOT NULL\n",
      "AND (mc.note LIKE '@param3' OR mc.note LIKE '@param4')\n",
      "AND mi.info IS NOT NULL\n",
      "AND (mi.info LIKE '@param5' OR mi.info LIKE '@param6')\n",
      "AND n.gender = '@param7'\n",
      "AND n.name LIKE '@param8'\n",
      "AND rt.role = '@param9'\n",
      "AND t.production_year BETWEEN @param10 AND @param11;\n",
      "\n",
      "name: 20-0\n",
      "query: SELECT *\n",
      "FROM complete_cast AS cc,\n",
      "   comp_cast_type AS cct1,\n",
      "   comp_cast_type AS cct2,\n",
      "   char_name AS chn,\n",
      "   cast_info AS ci,\n",
      "   keyword AS k,\n",
      "   kind_type AS kt,\n",
      "   movie_keyword AS mk,\n",
      "   name AS n,\n",
      "   title AS t\n",
      "WHERE kt.id = t.kind_id\n",
      "AND t.id = mk.movie_id\n",
      "AND t.id = ci.movie_id\n",
      "AND t.id = cc.movie_id\n",
      "AND mk.movie_id = ci.movie_id\n",
      "AND mk.movie_id = cc.movie_id\n",
      "AND ci.movie_id = cc.movie_id\n",
      "AND chn.id = ci.person_role_id\n",
      "AND n.id = ci.person_id\n",
      "AND k.id = mk.keyword_id\n",
      "AND cct1.id = cc.subject_id\n",
      "AND cct2.id = cc.status_id\n",
      "AND cct1.kind = '@param0'\n",
      "AND cct2.kind LIKE '@param1'\n",
      "AND chn.name NOT LIKE '@param2'\n",
      "AND (chn.name LIKE '@param3' OR chn.name LIKE '@param4')\n",
      "AND k.keyword IN ('@param5')\n",
      "AND kt.kind = '@param6'\n",
      "AND t.production_year > @param7;\n",
      "\n",
      "name: 21-0\n",
      "query: SELECT *\n",
      "FROM company_name AS cn,\n",
      "   company_type AS ct,\n",
      "   keyword AS k,\n",
      "   link_type AS lt,\n",
      "   movie_companies AS mc,\n",
      "   movie_info AS mi,\n",
      "   movie_keyword AS mk,\n",
      "   movie_link AS ml,\n",
      "   title AS t\n",
      "WHERE lt.id = ml.link_type_id\n",
      "AND ml.movie_id = t.id\n",
      "AND t.id = mk.movie_id\n",
      "AND mk.keyword_id = k.id\n",
      "AND t.id = mc.movie_id\n",
      "AND mc.company_type_id = ct.id\n",
      "AND mc.company_id = cn.id\n",
      "AND mi.movie_id = t.id\n",
      "AND ml.movie_id = mk.movie_id\n",
      "AND ml.movie_id = mc.movie_id\n",
      "AND mk.movie_id = mc.movie_id\n",
      "AND ml.movie_id = mi.movie_id\n",
      "AND mk.movie_id = mi.movie_id\n",
      "AND mc.movie_id = mi.movie_id\n",
      "AND cn.country_code != '@param0'\n",
      "AND (cn.name LIKE '@param1' OR cn.name LIKE '@param2')\n",
      "AND ct.kind = '@param3'\n",
      "AND k.keyword = '@param4'\n",
      "AND lt.link LIKE '@param5'\n",
      "AND mc.note IS NULL\n",
      "AND mi.info IN ('@param6')\n",
      "AND t.production_year BETWEEN @param7 AND @param8;\n",
      "\n",
      "name: 23-0\n",
      "query: SELECT *\n",
      "FROM complete_cast AS cc,\n",
      "   comp_cast_type AS cct,\n",
      "   company_name AS cn,\n",
      "   company_type AS ct,\n",
      "   info_type AS it,\n",
      "   keyword AS k,\n",
      "   kind_type AS kt,\n",
      "   movie_companies AS mc,\n",
      "   movie_info AS mi,\n",
      "   movie_keyword AS mk,\n",
      "   title AS t\n",
      "WHERE kt.id = t.kind_id\n",
      "AND t.id = mi.movie_id\n",
      "AND t.id = mk.movie_id\n",
      "AND t.id = mc.movie_id\n",
      "AND t.id = cc.movie_id\n",
      "AND mk.movie_id = mi.movie_id\n",
      "AND mk.movie_id = mc.movie_id\n",
      "AND mk.movie_id = cc.movie_id\n",
      "AND mi.movie_id = mc.movie_id\n",
      "AND mi.movie_id = cc.movie_id\n",
      "AND mc.movie_id = cc.movie_id\n",
      "AND k.id = mk.keyword_id\n",
      "AND it.id = mi.info_type_id\n",
      "AND cn.id = mc.company_id\n",
      "AND ct.id = mc.company_type_id\n",
      "AND cct.id = cc.status_id\n",
      "AND cct.kind = '@param0'\n",
      "AND cn.country_code = '@param1'\n",
      "AND it.info = '@param2'\n",
      "AND kt.kind IN ('@param3')\n",
      "AND mi.note LIKE '@param4'\n",
      "AND mi.info IS NOT NULL\n",
      "AND  (mi.info LIKE '@param5' OR mi.info LIKE '@param6')\n",
      "AND t.production_year > @param7;\n",
      "\n",
      "name: 24-0\n",
      "query: SELECT *\n",
      "FROM aka_name AS an,\n",
      "   char_name AS chn,\n",
      "   cast_info AS ci,\n",
      "   company_name AS cn,\n",
      "   info_type AS it,\n",
      "   keyword AS k,\n",
      "   movie_companies AS mc,\n",
      "   movie_info AS mi,\n",
      "   movie_keyword AS mk,\n",
      "   name AS n,\n",
      "   role_type AS rt,\n",
      "   title AS t\n",
      "WHERE t.id = mi.movie_id\n",
      "AND t.id = mc.movie_id\n",
      "AND t.id = ci.movie_id\n",
      "AND t.id = mk.movie_id\n",
      "AND mc.movie_id = ci.movie_id\n",
      "AND mc.movie_id = mi.movie_id\n",
      "AND mc.movie_id = mk.movie_id\n",
      "AND mi.movie_id = ci.movie_id\n",
      "AND mi.movie_id = mk.movie_id\n",
      "AND ci.movie_id = mk.movie_id\n",
      "AND cn.id = mc.company_id\n",
      "AND it.id = mi.info_type_id\n",
      "AND n.id = ci.person_id\n",
      "AND rt.id = ci.role_id\n",
      "AND n.id = an.person_id\n",
      "AND ci.person_id = an.person_id\n",
      "AND chn.id = ci.person_role_id\n",
      "AND k.id = mk.keyword_id\n",
      "AND ci.note IN ('@param0')\n",
      "AND cn.country_code = '@param1'\n",
      "AND it.info = '@param2'\n",
      "AND k.keyword IN ('@param3')\n",
      "AND mi.info IS NOT NULL\n",
      "AND (mi.info LIKE '@param4' OR mi.info LIKE '@param5')\n",
      "AND n.gender = '@param6'\n",
      "AND n.name LIKE '@param7'\n",
      "AND rt.role = '@param8'\n",
      "AND t.production_year > @param9;\n",
      "\n",
      "name: 26-0\n",
      "query: SELECT *\n",
      "FROM complete_cast AS cc,\n",
      "   comp_cast_type AS cct1,\n",
      "   comp_cast_type AS cct2,\n",
      "   char_name AS chn,\n",
      "   cast_info AS ci,\n",
      "   info_type AS it2,\n",
      "   keyword AS k,\n",
      "   kind_type AS kt,\n",
      "   movie_info_idx AS mi_idx,\n",
      "   movie_keyword AS mk,\n",
      "   name AS n,\n",
      "   title AS t\n",
      "WHERE kt.id = t.kind_id\n",
      "AND t.id = mk.movie_id\n",
      "AND t.id = ci.movie_id\n",
      "AND t.id = cc.movie_id\n",
      "AND t.id = mi_idx.movie_id\n",
      "AND mk.movie_id = ci.movie_id\n",
      "AND mk.movie_id = cc.movie_id\n",
      "AND mk.movie_id = mi_idx.movie_id\n",
      "AND ci.movie_id = cc.movie_id\n",
      "AND ci.movie_id = mi_idx.movie_id\n",
      "AND cc.movie_id = mi_idx.movie_id\n",
      "AND chn.id = ci.person_role_id\n",
      "AND n.id = ci.person_id\n",
      "AND k.id = mk.keyword_id\n",
      "AND cct1.id = cc.subject_id\n",
      "AND cct2.id = cc.status_id\n",
      "AND it2.id = mi_idx.info_type_id\n",
      "AND cct1.kind = '@param0'\n",
      "AND cct2.kind LIKE '@param1'\n",
      "AND chn.name IS NOT NULL\n",
      "AND (chn.name LIKE '@param2' OR chn.name LIKE '@param3')\n",
      "AND it2.info = '@param4'\n",
      "AND k.keyword IN ('@param5')\n",
      "AND kt.kind = '@param6'\n",
      "AND mi_idx.info > '@param7'\n",
      "AND t.production_year > @param8;\n",
      "\n",
      "name: 27-0\n",
      "query: SELECT *\n",
      "FROM complete_cast AS cc,\n",
      "   comp_cast_type AS cct1,\n",
      "   comp_cast_type AS cct2,\n",
      "   company_name AS cn,\n",
      "   company_type AS ct,\n",
      "   keyword AS k,\n",
      "   link_type AS lt,\n",
      "   movie_companies AS mc,\n",
      "   movie_info AS mi,\n",
      "   movie_keyword AS mk,\n",
      "   movie_link AS ml,\n",
      "   title AS t\n",
      "WHERE lt.id = ml.link_type_id\n",
      "AND ml.movie_id = t.id\n",
      "AND t.id = mk.movie_id\n",
      "AND mk.keyword_id = k.id\n",
      "AND t.id = mc.movie_id\n",
      "AND mc.company_type_id = ct.id\n",
      "AND mc.company_id = cn.id\n",
      "AND mi.movie_id = t.id\n",
      "AND t.id = cc.movie_id\n",
      "AND cct1.id = cc.subject_id\n",
      "AND cct2.id = cc.status_id\n",
      "AND ml.movie_id = mk.movie_id\n",
      "AND ml.movie_id = mc.movie_id\n",
      "AND mk.movie_id = mc.movie_id\n",
      "AND ml.movie_id = mi.movie_id\n",
      "AND mk.movie_id = mi.movie_id\n",
      "AND mc.movie_id = mi.movie_id\n",
      "AND ml.movie_id = cc.movie_id\n",
      "AND mk.movie_id = cc.movie_id\n",
      "AND mc.movie_id = cc.movie_id\n",
      "AND mi.movie_id = cc.movie_id\n",
      "AND cct1.kind IN ('@param0')\n",
      "AND cct2.kind = '@param1'\n",
      "AND cn.country_code != '@param2'\n",
      "AND (cn.name LIKE '@param3' OR cn.name LIKE '@param4')\n",
      "AND ct.kind = '@param5'\n",
      "AND k.keyword = '@param6'\n",
      "AND lt.link LIKE '@param7'\n",
      "AND mc.note IS NULL\n",
      "AND mi.info IN ('@param8')\n",
      "AND t.production_year BETWEEN @param9 AND @param10;\n",
      "\n",
      "name: 29-0\n",
      "query: SELECT *\n",
      "FROM aka_name AS an,\n",
      "    complete_cast AS cc,\n",
      "    comp_cast_type AS cct1,\n",
      "    comp_cast_type AS cct2,\n",
      "    char_name AS chn,\n",
      "    cast_info AS ci,\n",
      "    company_name AS cn,\n",
      "    info_type AS it,\n",
      "    info_type AS it3,\n",
      "    keyword AS k,\n",
      "    movie_companies AS mc,\n",
      "    movie_info AS mi,\n",
      "    movie_keyword AS mk,\n",
      "    name AS n,\n",
      "    person_info AS pi,\n",
      "    role_type AS rt,\n",
      "    title AS t\n",
      "WHERE t.id = mi.movie_id\n",
      "AND t.id = mc.movie_id\n",
      "AND t.id = ci.movie_id\n",
      "AND t.id = mk.movie_id\n",
      "AND t.id = cc.movie_id\n",
      "AND mc.movie_id = ci.movie_id\n",
      "AND mc.movie_id = mi.movie_id\n",
      "AND mc.movie_id = mk.movie_id\n",
      "AND mc.movie_id = cc.movie_id\n",
      "AND mi.movie_id = ci.movie_id\n",
      "AND mi.movie_id = mk.movie_id\n",
      "AND mi.movie_id = cc.movie_id\n",
      "AND ci.movie_id = mk.movie_id\n",
      "AND ci.movie_id = cc.movie_id\n",
      "AND mk.movie_id = cc.movie_id\n",
      "AND cn.id = mc.company_id\n",
      "AND it.id = mi.info_type_id\n",
      "AND n.id = ci.person_id\n",
      "AND rt.id = ci.role_id\n",
      "AND n.id = an.person_id\n",
      "AND ci.person_id = an.person_id\n",
      "AND chn.id = ci.person_role_id\n",
      "AND n.id = pi.person_id\n",
      "AND ci.person_id = pi.person_id\n",
      "AND it3.id = pi.info_type_id\n",
      "AND k.id = mk.keyword_id\n",
      "AND cct1.id = cc.subject_id\n",
      "AND cct2.id = cc.status_id\n",
      "AND cct1.kind = '@param0'\n",
      "AND cct2.kind = '@param1'\n",
      "AND chn.name = '@param2'\n",
      "AND ci.note IN ('@param3')\n",
      "AND cn.country_code = '@param4'\n",
      "AND it.info = '@param5'\n",
      "AND it3.info = '@param6'\n",
      "AND k.keyword = '@param7'\n",
      "AND mi.info IS NOT NULL\n",
      "AND (mi.info LIKE '@param8' OR mi.info LIKE '@param9')\n",
      "AND n.gender = '@param10'\n",
      "AND n.name LIKE '@param11'\n",
      "AND rt.role = '@param12'\n",
      "AND t.title = '@param13'\n",
      "AND t.production_year BETWEEN @param14 AND @param15;\n",
      "\n",
      "name: 7-0\n",
      "query: SELECT *\n",
      " FROM aka_name AS an,\n",
      " cast_info AS ci,\n",
      " info_type AS it,\n",
      " link_type AS lt,\n",
      " movie_link AS ml,\n",
      " name AS n,\n",
      " person_info AS pi,\n",
      " title AS t\n",
      " WHERE n.id = an.person_id\n",
      " AND n.id = pi.person_id\n",
      " AND ci.person_id = n.id\n",
      " AND t.id = ci.movie_id\n",
      " AND ml.linked_movie_id = t.id\n",
      " AND lt.id = ml.link_type_id\n",
      " AND it.id = pi.info_type_id\n",
      " AND pi.person_id = an.person_id\n",
      " AND pi.person_id = ci.person_id\n",
      " AND an.person_id = ci.person_id\n",
      " AND ci.movie_id = ml.linked_movie_id\n",
      " AND an.name LIKE '@param0'\n",
      " AND it.info = '@param1'\n",
      " AND lt.link = '@param2'\n",
      " AND (n.gender = '@param3' OR (n.gender = '@param4' AND n.name LIKE '@param5'))\n",
      " AND pi.note = '@param6';\n",
      "\n",
      "name: 9-0\n",
      "query: SELECT *\n",
      "FROM aka_name AS an,\n",
      " char_name AS chn,\n",
      " cast_info AS ci,\n",
      " company_name AS cn,\n",
      " movie_companies AS mc,\n",
      " name AS n,\n",
      " role_type AS rt,\n",
      " title AS t\n",
      "WHERE ci.movie_id = t.id\n",
      " AND t.id = mc.movie_id\n",
      " AND ci.movie_id = mc.movie_id\n",
      " AND mc.company_id = cn.id\n",
      " AND ci.role_id = rt.id\n",
      " AND n.id = ci.person_id\n",
      " AND chn.id = ci.person_role_id\n",
      " AND an.person_id = n.id\n",
      " AND an.person_id = ci.person_id\n",
      "AND ci.note IN ('@param0')\n",
      "AND cn.country_code = '@param1'\n",
      "AND mc.note IS NOT NULL \n",
      "AND (mc.note LIKE '@param2' OR mc.note LIKE '@param3')\n",
      "AND n.gender = '@param4'\n",
      "AND n.name LIKE '@param5'\n",
      "AND rt.role = '@param6'\n",
      "AND t.production_year BETWEEN @param7 AND @param8;\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def search_or_queries(directory_path):\n",
    "    matches = []   \n",
    "    search_pattern = os.path.join(directory_path, '*.json')\n",
    "\n",
    "    for json_file in glob.glob(search_pattern):\n",
    "        try:\n",
    "            query_id = os.path.basename(json_file)[:-5]\n",
    "            \n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            if (query_id in data and \n",
    "                \"query\" in data[query_id] and \n",
    "                \" OR \" in data[query_id][\"query\"]):\n",
    "                matches.append((query_id, data[query_id][\"query\"]))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR {json_file}: {str(e)}\")\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def main():\n",
    "    directory = \"imdb_input/original_template\"\n",
    "    results = search_or_queries(directory)\n",
    "    \n",
    "    print(f\"\\nfound {len(results)} contains OR predicate:\")\n",
    "    for query_id, query in sorted(results):\n",
    "        print(f\"\\nname: {query_id}\")\n",
    "        print(f\"query: {query}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pqo table alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"22-0\": ['cn', 'it1', 'it2', 'k', 'kt', 'mc', 'mc', 'mi', 'mi_idx', 't']\n",
      "\"22-0\": ['cn', 'it', 'it', 'k', 'kt', 'mc', 'mi', 'mi_idx', 't']\n",
      "\n",
      "\"23-0\": ['cct', 'cn', 'it', 'kt', 'mi', 'mi', 'mi', 't']\n",
      "\"23-0\": ['mi', 'cct', 'cn', 'it', 'kt', 't']\n",
      "\n",
      "\"24-0\": ['ci', 'cn', 'it', 'k', 'mi', 'mi', 'n', 'n', 'rt', 't']\n",
      "\"24-0\": ['mi', 'ci', 'cn', 'it', 'k', 'n', 'rt', 't']\n",
      "\n",
      "\"25-0\": ['ci', 'it1', 'it2', 'k', 'mi', 'n']\n",
      "\"25-0\": ['ci', 'it', 'it', 'k', 'mi', 'n']\n",
      "\n",
      "\"26-0\": ['cct1', 'cct2', 'chn', 'chn', 'it2', 'k', 'kt', 'mi_idx', 't']\n",
      "\"26-0\": ['chn', 'cct', 'cct', 'it', 'k', 'kt', 'mi_idx', 't']\n",
      "\n",
      "\"27-0\": ['cct1', 'cct2', 'cn', 'cn', 'cn', 'ct', 'k', 'lt', 'mi', 't', 't']\n",
      "\"27-0\": ['cn', 'cct', 'cct', 'ct', 'k', 'lt', 'mi', 't']\n",
      "\n",
      "\"28-0\": ['cct1', 'cct2', 'cn', 'it1', 'it2', 'k', 'kt', 'mc', 'mc', 'mi', 'mi_idx', 't']\n",
      "\"28-0\": ['cct', 'cct', 'cn', 'it', 'it', 'k', 'kt', 'mc', 'mi', 'mi_idx', 't']\n",
      "\n",
      "\"30-0\": ['cct1', 'cct2', 'ci', 'it1', 'it2', 'k', 'mi', 'n', 't']\n",
      "\"30-0\": ['cct', 'cct', 'ci', 'it', 'it', 'k', 'mi', 'n', 't']\n",
      "\n",
      "\"31-0\": ['ci', 'cn', 'it1', 'it2', 'k', 'mi', 'n']\n",
      "\"31-0\": ['ci', 'cn', 'it', 'it', 'k', 'mi', 'n']\n",
      "\n",
      "\"32-0\": ['k']\n",
      "\"32-0\": ['k']\n",
      "\n",
      "\"33-0\": ['cn1', 'it1', 'it2', 'kt1', 'kt2', 'lt', 'mi_idx2', 't2', 't2']\n",
      "\"33-0\": ['cn', 'it', 'it', 'kt', 'kt', 'lt', 'mi_idx', 't']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "def get_aliases(query_id):\n",
    "    file_path = f\"imdb_{query_id}_original/cardinality/inputs/testing/{query_id}_testing_original.json\"\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    predicates = data[query_id][\"predicates\"]\n",
    "    \n",
    "    alias_list = []\n",
    "    for pred in predicates:\n",
    "        alias_list.append(pred.get(\"original_alias\", pred[\"alias\"]))\n",
    "    \n",
    "    print(f'\"{query_id}\": {alias_list}')\n",
    "    \n",
    "    join_file_path = f\"imdb_{query_id}_original/cardinality/inputs/PQO/join_predicates/{query_id}_50_training.txt\"\n",
    "    \n",
    "    with open(join_file_path, 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        predicates_list = ast.literal_eval(first_line)\n",
    "        table_names = []\n",
    "        for pred in predicates_list:\n",
    "            if isinstance(pred, str):\n",
    "                table = pred.split('.')[0].strip('()\"')\n",
    "                table_names.append(table)\n",
    "            elif isinstance(pred, list):\n",
    "                for p in pred:\n",
    "                    if isinstance(p, str) and '.' in p:\n",
    "                        table = p.split('.')[0].strip('()\"')\n",
    "                        table_names.append(table)\n",
    "        \n",
    "    print(f'\"{query_id}\": {table_names}\\n')\n",
    "    \n",
    "    return alias_list, table_names\n",
    "\n",
    "for query_id in [\"22-0\", \"23-0\", \"24-0\", \"25-0\", \"26-0\", \"27-0\", \"28-0\", \"30-0\", \"31-0\", \"32-0\", \"33-0\"]:\n",
    "    aliases, tables = get_aliases(query_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mixture query generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['33-0']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def process_files(query_id):\n",
    "    # Set up base path and each component's paths\n",
    "    base_path = f\"imdb_{query_id}_original\"\n",
    "    \n",
    "    # Each component has its own directory with a testing file\n",
    "    card_path = Path(base_path) / \"cardinality\"\n",
    "    csv_path = Path(base_path) / \"csv\"\n",
    "    kepler_path = Path(base_path) / \"kepler\"\n",
    "    \n",
    "    # Testing file path is relative to each component directory\n",
    "    test_path = f\"inputs/testing/{query_id}_testing_original.json\"\n",
    "    \n",
    "    # Read data from cardinality + its testing file\n",
    "    with open(card_path / test_path) as f:\n",
    "        card_test_data = json.load(f)\n",
    "    \n",
    "    # Read data from csv + its testing file\n",
    "    with open(csv_path / test_path) as f:\n",
    "        csv_test_data = json.load(f)\n",
    "        \n",
    "    # Read data from kepler + its testing file\n",
    "    with open(kepler_path / test_path) as f:\n",
    "        kepler_test_data = json.load(f)\n",
    "\n",
    "    # Verify consistency of query and predicates across all datasets\n",
    "    if not verify_consistency([card_test_data, csv_test_data, kepler_test_data], query_id):\n",
    "        raise ValueError(f\"Inconsistent query or predicates for {query_id}\")\n",
    "\n",
    "    # Merge params from all sources\n",
    "    all_params = (card_test_data[query_id]['params'] + \n",
    "                 csv_test_data[query_id]['params'] + \n",
    "                 kepler_test_data[query_id]['params'])\n",
    "    \n",
    "    # Shuffle and sample 200 sets of params (600 total)\n",
    "    random.seed(2024)\n",
    "    random.shuffle(all_params)\n",
    "    sampled_params = all_params[:200]\n",
    "    \n",
    "    # Build result in required format\n",
    "    result = {\n",
    "        query_id: {\n",
    "            \"query\": card_test_data[query_id]['query'],  # They should all be the same at this point\n",
    "            \"predicates\": card_test_data[query_id]['predicates'],\n",
    "            \"params\": sampled_params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def verify_consistency(data_list, query_id):\n",
    "    \"\"\"\n",
    "    Verifies that query and predicates are consistent, but only checks predicates\n",
    "    between the first two datasets (cardinality and csv).\n",
    "    \n",
    "    Args:\n",
    "        data_list: List containing [card_test_data, csv_test_data, kepler_test_data]\n",
    "        query_id: The query ID being processed\n",
    "    Returns:\n",
    "        True if consistent according to our rules, False otherwise\n",
    "    \"\"\"\n",
    "    if len(data_list) < 3:  # We need all three datasets\n",
    "        return False\n",
    "    \n",
    "    # Unpack for clarity\n",
    "    card_data, csv_data, kepler_data = data_list[0], data_list[1], data_list[2]\n",
    "    \n",
    "    # Check query consistency across all three datasets\n",
    "    query_consistent = (\n",
    "        card_data[query_id]['query'] == csv_data[query_id]['query'] == \n",
    "        kepler_data[query_id]['query']\n",
    "    )\n",
    "    \n",
    "    # Check predicates only between cardinality and csv data\n",
    "    predicates_consistent = (\n",
    "        card_data[query_id]['predicates'] == csv_data[query_id]['predicates']\n",
    "    )\n",
    "    \n",
    "    return query_consistent and predicates_consistent\n",
    "\n",
    "def process_all_queries():\n",
    "    # Get all query IDs from directory names\n",
    "    pattern = \"imdb_*_original\"\n",
    "    all_dirs = glob.glob(pattern)\n",
    "    query_ids = [d.split('_')[1] for d in all_dirs]\n",
    "    print(query_ids)\n",
    "    \n",
    "    # Process each query and save its result separately\n",
    "    for qid in query_ids:\n",
    "        # Get the result for this query\n",
    "        result = process_files(qid)\n",
    "        \n",
    "        # Create filename for this query's result\n",
    "        output_dir = Path(f\"0_mixture_test/{qid}\")\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_filename = output_dir / f\"{qid}_mixture_test.json\"\n",
    "        \n",
    "        # Save this query's result to its own file\n",
    "        with open(output_filename, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "# Execute the processing\n",
    "# process_all_queries()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yang_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
