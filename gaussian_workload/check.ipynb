{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check valid (return > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found unique prefixes:\n",
      ";_\n",
      "c_\n",
      "ca_\n",
      "cc_\n",
      "cd1.cd_\n",
      "cd_\n",
      "cr_\n",
      "cs_\n",
      "d1.d_\n",
      "d2.d_\n",
      "d3.d_\n",
      "d_\n",
      "hd_\n",
      "i_\n",
      "ib_\n",
      "item1.i_\n",
      "item2.i_\n",
      "s1.ss_\n",
      "s2.ss_\n",
      "s_\n",
      "sm_\n",
      "ss_\n",
      "w_\n",
      "ws_\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def process_tpl_files(directory):\n",
    "    unique_prefixes = set()\n",
    "    pattern = re.compile(r'query_condition_\\d+\\.tpl$')\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if pattern.match(file):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                with open(file_path, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    lines = content.split('\\n')\n",
    "                    \n",
    "                    # remove define stmt\n",
    "                    non_define_lines = [line.strip() for line in lines \n",
    "                                      if line.strip() and not line.strip().startswith('define')]\n",
    "                    \n",
    "                    for line in non_define_lines:\n",
    "                        unique_prefixes.add(line.split('_')[0] + '_')\n",
    "    print(\"Found unique prefixes:\")\n",
    "    for prefix in sorted(unique_prefixes):\n",
    "        print(prefix)\n",
    "    \n",
    "    return unique_prefixes\n",
    "\n",
    "directory = \"raw_tpl_files\"\n",
    "# prefixes = process_tpl_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_center': 'cc_', 'catalog_page': 'cp_', 'catalog_returns': 'cr_', 'catalog_sales': 'cs_', 'customer': 'c_', 'customer_address': 'ca_', 'customer_demographics': 'cd_', 'date_dim': 'dd_', 'dbgen_version': 'dv_', 'household_demographics': 'hd_', 'income_band': 'ib_', 'inventory': 'i_', 'item': 'i_', 'promotion': 'p_', 'reason': 'r_', 'ship_mode': 'sm_', 'store': 's_', 'store_returns': 'sr_', 'store_sales': 'ss_', 'time_dim': 'td_', 'warehouse': 'w_', 'web_page': 'wp_', 'web_returns': 'wr_', 'web_sales': 'ws_', 'web_site': 'ws_'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Get all .dat files and extract table names\n",
    "tables = sorted([os.path.basename(f)[:-4] for f in glob.glob('/home/lsh/test_kepler/datasets/tpcds/*.dat')])\n",
    "\n",
    "# Create dictionary with concatenated prefixes as values\n",
    "table_dict = {table: ''.join(word[0] for word in table.split('_')) + '_' for table in tables}\n",
    "\n",
    "# with open('table_dict.json', 'w') as f:\n",
    "#     json.dump(table_dict, f)\n",
    "#     print(table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'013': ['store_sales', 'store', 'customer_demographics', 'household_demographics', 'customer_address', 'date_dim'], '018': ['catalog_sales', 'customer_demographics', 'customer', 'customer_address', 'date_dim', 'item'], '019': ['date_dim', 'store_sales', 'item', 'customer', 'customer_address', 'store'], '025': ['store_sales', 'store_returns', 'catalog_sales', 'date_dim d1', 'date_dim d2', 'date_dim d3', 'store', 'item'], '027': ['store_sales', 'customer_demographics', 'date_dim', 'store', 'item'], '040': ['catalog_sales\\nleft outer join catalog_returns on (cs_order_number = cr_order_number\\n                                    and cs_item_sk = cr_item_sk)', 'warehouse', 'item', 'date_dim'], '050': ['store_sales', 'store_returns', 'store', 'date_dim d1', 'date_dim d2'], '072': ['catalog_sales\\njoin inventory on (cs_item_sk = inv_item_sk)\\njoin warehouse on (w_warehouse_sk=inv_warehouse_sk)\\njoin item on (i_item_sk = cs_item_sk)\\njoin customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk)\\njoin household_demographics on (cs_bill_hdemo_sk = hd_demo_sk)\\njoin date_dim d1 on (cs_sold_date_sk = d1.d_date_sk)\\njoin date_dim d2 on (inv_date_sk = d2.d_date_sk)\\njoin date_dim d3 on (cs_ship_date_sk = d3.d_date_sk)\\nleft outer join promotion on (cs_promo_sk=p_promo_sk)\\nleft outer join catalog_returns on (cr_item_sk = cs_item_sk\\n                                    and cr_order_number = cs_order_number)'], '084': ['customer', 'customer_address', 'customer_demographics', 'household_demographics', 'income_band', 'store_returns'], '085': ['web_sales', 'web_returns', 'web_page', 'customer_demographics cd1', 'customer_demographics cd2', 'customer_address', 'date_dim', 'reason'], '091': ['call_center', 'catalog_returns', 'date_dim', 'customer', 'customer_address', 'customer_demographics', 'household_demographics'], '099': ['catalog_sales', 'warehouse', 'ship_mode', 'call_center', 'date_dim'], '100': ['item as item1', 'item as item2', 'store_sales as s1', 'store_sales as s2', 'date_dim', 'customer', 'customer_address', 'customer_demographics'], '101': ['store_sales', 'store_returns', 'web_sales', 'date_dim d1', 'date_dim d2', 'item', 'customer', 'customer_address', 'household_demographics'], '102': ['store_sales', 'web_sales', 'date_dim d1', 'date_dim d2', 'customer', 'inventory', 'store', 'warehouse', 'item', 'customer_demographics', 'household_demographics', 'customer_address']}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_tables_from_query(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        tables_str = content.lower().split('select')[1].split('from')[1].split('where')[0]\n",
    "        tables = [\n",
    "            table.strip() \n",
    "            for table in tables_str.split(',')\n",
    "            if table.strip()\n",
    "        ]\n",
    "        \n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_all_queries(directory_path):\n",
    "    result = {}\n",
    "    path = Path(directory_path)\n",
    "    \n",
    "    for file_path in path.glob(\"query*_spj.tpl\"):\n",
    "        try:\n",
    "            query_id = re.search(r'query(\\d+)_spj\\.tpl', file_path.name)\n",
    "            if query_id:\n",
    "                query_id = query_id.group(1)\n",
    "                # print(query_id)\n",
    "                tables = extract_tables_from_query(str(file_path))\n",
    "                if tables:\n",
    "                    result[query_id] = tables\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return result\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     tpl_directory = \"raw_tpl_files\"\n",
    "#     result_dict = process_all_queries(tpl_directory)\n",
    "    \n",
    "#     sorted_dict = dict(sorted(result_dict.items(), key=lambda x: int(x[0])))\n",
    "    \n",
    "#     with open('query_table.json', 'w') as f:\n",
    "#         json.dump(sorted_dict, f, indent=4)\n",
    "#     print(sorted_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate PQO files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of query_id: 13\n",
      "Key: ['013', '018', '019', '025', '027', '040', '050', '072', '084', '085', '091', '099', '100']\n",
      "Instance length: {251}\n",
      "{'013': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 258, 259, 260, 261, 263, 264, 265, 266], '018': [6, 7, 13, 14, 17, 19, 22, 24, 28, 30, 32, 35, 38, 40, 41, 44, 48, 49, 50, 52, 56, 58, 60, 68, 70, 78, 79, 80, 81, 82, 87, 88, 91, 94, 102, 103, 104, 112, 113, 114, 115, 116, 117, 118, 119, 120, 125, 131, 133, 144, 146, 151, 153, 157, 158, 159, 161, 162, 164, 166, 168, 171, 173, 175, 183, 185, 188, 189, 190, 191, 192, 193, 195, 202, 203, 206, 208, 214, 216, 219, 220, 222, 223, 229, 232, 233, 235, 236, 243, 245, 246, 247, 249, 250, 252, 257, 262, 268, 270, 278, 282, 283, 286, 288, 289, 294, 297, 304, 307, 310, 314, 317, 319, 320, 323, 325, 327, 328, 329, 330, 333, 336, 342, 344, 350, 355, 358, 360, 373, 375, 379, 381, 384, 386, 389, 390, 391, 395, 396, 402, 404, 409, 410, 411, 415, 416, 417, 436, 443, 446, 449, 451, 452, 455, 457, 460, 471, 472, 474, 475, 480, 481, 484, 486, 489, 492, 495, 499, 500, 505, 512, 513, 515, 516, 521, 532, 535, 538, 542, 545, 549, 551, 557, 563, 566, 570, 571, 575, 579, 589, 591, 594, 599, 603, 612, 613, 617, 618, 619, 621, 627, 630, 634, 640, 641, 644, 647, 653, 657, 663, 665, 668, 673, 674, 682, 690, 692, 693, 695, 696, 697, 708, 709, 710, 713, 714, 716, 718, 719, 726, 727, 728, 730, 733, 735, 738, 739, 741, 746, 754, 755, 761, 765, 767, 768, 769, 780, 781, 782, 783, 785], '019': [5, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 25, 27, 29, 30, 31, 32, 34, 35, 36, 37, 38, 40, 43, 46, 47, 51, 54, 56, 57, 58, 59, 60, 61, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 78, 80, 81, 82, 83, 84, 85, 92, 94, 95, 97, 98, 100, 101, 102, 105, 106, 107, 109, 110, 111, 116, 121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 140, 141, 142, 144, 145, 146, 149, 152, 153, 154, 155, 156, 157, 159, 161, 162, 164, 165, 167, 168, 171, 173, 174, 176, 177, 178, 180, 182, 184, 186, 188, 190, 191, 192, 193, 195, 198, 200, 202, 205, 209, 211, 212, 214, 215, 216, 217, 218, 221, 222, 223, 225, 227, 230, 233, 234, 235, 236, 237, 239, 240, 242, 244, 247, 249, 250, 251, 253, 255, 256, 258, 259, 260, 261, 262, 263, 265, 266, 268, 270, 272, 273, 274, 276, 278, 279, 281, 284, 285, 286, 287, 288, 289, 290, 291, 292, 296, 299, 300, 305, 309, 311, 314, 315, 316, 317, 320, 321, 322, 325, 326, 327, 328, 329, 331, 333, 334, 335, 337, 340, 343, 344, 345, 346, 348, 351, 354, 356, 357, 358, 360, 362, 363, 364, 365, 369, 370, 371, 372, 373, 374, 375, 377, 378, 379, 380, 381, 385, 386, 387, 389, 390, 393, 395, 397, 398, 399, 400, 401, 402, 403, 404], '025': [0, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 26, 27, 30, 31, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 45, 46, 47, 49, 50, 51, 54, 56, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 81, 82, 83, 85, 88, 89, 92, 93, 95, 97, 101, 102, 103, 107, 108, 109, 110, 111, 112, 113, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 143, 144, 146, 148, 150, 151, 152, 154, 155, 156, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 179, 180, 182, 183, 184, 186, 187, 188, 193, 194, 195, 196, 197, 198, 201, 202, 205, 208, 209, 210, 211, 213, 214, 215, 216, 219, 222, 223, 224, 225, 226, 229, 232, 233, 234, 235, 236, 237, 238, 240, 241, 243, 244, 246, 247, 248, 249, 251, 252, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 268, 269, 271, 272, 273, 274, 275, 276, 277, 280, 285, 286, 287, 288, 290, 292, 293, 294, 295, 296, 297, 300, 302, 304, 305, 306, 311, 312, 313, 316, 317, 318, 319, 320, 321, 324, 326, 327, 328, 332, 333, 335, 336, 337, 339, 340, 341, 342, 343, 344, 345, 346, 348, 351, 352, 353, 355, 356, 358, 359], '027': [0, 1, 16, 22, 30, 32, 34, 44, 47, 49, 51, 52, 55, 61, 68, 72, 76, 93, 95, 98, 115, 126, 135, 137, 140, 144, 155, 162, 165, 187, 202, 205, 221, 242, 243, 245, 247, 250, 257, 259, 261, 266, 283, 285, 287, 317, 326, 340, 345, 347, 352, 372, 378, 409, 411, 425, 440, 452, 456, 466, 473, 474, 480, 484, 497, 498, 501, 503, 507, 511, 517, 523, 524, 533, 553, 559, 567, 577, 588, 594, 597, 602, 609, 617, 623, 625, 626, 630, 633, 639, 650, 656, 661, 668, 673, 674, 676, 677, 681, 682, 686, 699, 710, 716, 720, 735, 743, 748, 755, 759, 765, 776, 780, 782, 787, 801, 807, 817, 822, 824, 834, 839, 840, 851, 857, 860, 871, 873, 882, 885, 898, 909, 911, 915, 921, 923, 942, 951, 953, 961, 965, 970, 974, 976, 977, 983, 992, 996, 999, 1017, 1027, 1038, 1039, 1048, 1051, 1053, 1058, 1072, 1096, 1102, 1121, 1123, 1127, 1134, 1156, 1164, 1166, 1173, 1178, 1180, 1184, 1196, 1206, 1214, 1222, 1226, 1231, 1238, 1246, 1247, 1266, 1268, 1282, 1287, 1291, 1295, 1303, 1306, 1307, 1313, 1314, 1318, 1320, 1324, 1326, 1345, 1350, 1352, 1367, 1386, 1390, 1396, 1398, 1405, 1423, 1426, 1427, 1432, 1439, 1448, 1453, 1468, 1483, 1485, 1486, 1498, 1499, 1510, 1511, 1512, 1517, 1522, 1526, 1532, 1533, 1536, 1538, 1539, 1545, 1552, 1566, 1580, 1585, 1592, 1609, 1611, 1626, 1628, 1638, 1640, 1642, 1667, 1674, 1676, 1680, 1697, 1699, 1703, 1712, 1724, 1740], '040': [1, 8, 9, 13, 14, 15, 23, 24, 26, 28, 31, 32, 43, 44, 47, 51, 55, 56, 58, 60, 67, 70, 71, 74, 77, 86, 88, 89, 93, 94, 100, 103, 114, 118, 122, 126, 129, 130, 131, 134, 143, 145, 148, 157, 159, 162, 164, 165, 166, 173, 174, 175, 176, 178, 181, 182, 190, 192, 194, 196, 197, 198, 199, 201, 202, 203, 206, 207, 208, 211, 212, 216, 223, 229, 230, 232, 233, 234, 235, 238, 239, 242, 244, 248, 251, 252, 254, 255, 258, 259, 267, 271, 272, 275, 280, 281, 288, 289, 294, 295, 297, 299, 300, 301, 303, 304, 308, 309, 319, 324, 326, 327, 328, 332, 339, 340, 342, 346, 351, 354, 355, 358, 360, 361, 362, 363, 366, 369, 370, 371, 374, 378, 380, 383, 384, 385, 389, 391, 392, 395, 396, 399, 400, 401, 408, 409, 412, 417, 419, 424, 427, 434, 436, 441, 445, 446, 450, 454, 455, 456, 457, 459, 462, 465, 468, 469, 474, 478, 482, 486, 487, 488, 489, 491, 504, 505, 506, 509, 516, 521, 522, 523, 527, 528, 534, 535, 537, 544, 548, 552, 554, 557, 559, 561, 562, 564, 565, 570, 571, 573, 577, 578, 579, 587, 597, 598, 601, 602, 606, 607, 608, 611, 612, 617, 625, 634, 638, 641, 645, 646, 648, 651, 657, 659, 661, 665, 667, 669, 670, 672, 674, 678, 679, 681, 687, 693, 695, 697, 699, 702, 703, 704, 705, 707, 708, 709, 711, 712, 714, 715, 716], '050': [0, 1, 2, 5, 8, 9, 16, 18, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 34, 35, 37, 43, 44, 45, 48, 51, 52, 57, 58, 63, 64, 71, 72, 79, 82, 89, 90, 93, 95, 99, 105, 107, 109, 125, 131, 135, 137, 140, 141, 145, 147, 152, 155, 156, 157, 161, 164, 167, 170, 173, 176, 178, 181, 182, 184, 189, 190, 193, 199, 205, 207, 208, 211, 213, 216, 217, 219, 221, 223, 225, 227, 228, 233, 234, 235, 239, 241, 242, 243, 256, 258, 268, 269, 277, 281, 283, 284, 285, 287, 289, 294, 295, 298, 299, 301, 303, 305, 307, 308, 311, 327, 329, 331, 334, 340, 341, 342, 344, 345, 352, 355, 362, 368, 370, 371, 373, 376, 378, 379, 382, 383, 392, 393, 397, 399, 403, 404, 406, 407, 414, 422, 426, 430, 435, 436, 442, 443, 445, 451, 452, 457, 460, 461, 462, 465, 466, 468, 470, 474, 481, 482, 485, 486, 489, 496, 497, 504, 506, 508, 510, 513, 516, 518, 519, 520, 524, 531, 532, 535, 538, 539, 540, 545, 549, 551, 552, 555, 556, 563, 571, 575, 577, 580, 581, 583, 584, 588, 592, 593, 594, 595, 597, 599, 602, 603, 606, 612, 613, 615, 616, 617, 618, 626, 627, 629, 630, 632, 639, 640, 641, 646, 649, 651, 653, 655, 656, 657, 658, 664, 665, 666, 669, 675, 677, 685, 688, 690, 691, 697, 699, 701, 706, 707, 709, 714, 715, 721, 724, 725, 731, 734], '072': [1, 2, 3, 6, 9, 10, 11, 12, 13, 14, 15, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 56, 58, 60, 61, 62, 65, 66, 67, 69, 70, 72, 73, 74, 75, 77, 79, 80, 81, 84, 86, 87, 91, 92, 93, 94, 96, 97, 101, 102, 103, 104, 106, 107, 108, 109, 110, 113, 116, 118, 119, 120, 121, 122, 123, 127, 128, 131, 132, 133, 134, 135, 136, 137, 139, 140, 142, 144, 145, 148, 149, 150, 151, 155, 157, 160, 162, 163, 164, 165, 167, 168, 172, 173, 174, 176, 177, 178, 179, 183, 184, 185, 186, 187, 188, 189, 190, 192, 193, 194, 197, 198, 199, 200, 202, 203, 204, 205, 207, 209, 211, 212, 213, 214, 216, 217, 219, 222, 224, 225, 226, 227, 228, 229, 230, 231, 233, 235, 236, 237, 238, 240, 242, 244, 245, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 263, 264, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 283, 285, 288, 290, 291, 292, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 308, 309, 310, 311, 312, 314, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 328, 333, 334, 335, 336, 337, 338, 339, 341, 344, 345, 349, 350, 352, 353, 355, 357, 358, 359, 360], '084': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], '085': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 106, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], '091': [0, 1, 2, 3, 4, 5, 7, 8, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 24, 25, 26, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 41, 45, 47, 48, 49, 52, 54, 55, 56, 57, 59, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 142, 144, 146, 147, 148, 149, 150, 151, 152, 153, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 180, 181, 182, 183, 184, 185, 186, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 205, 206, 208, 209, 210, 213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 238, 239, 240, 242, 243, 244, 246, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 268, 269, 270, 272, 273, 277, 279, 280, 281, 282, 283, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 299, 300, 302, 303, 304, 305, 308, 309, 310, 311, 314, 315, 316, 318, 319, 320, 321], '099': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], '100': [0, 3, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 32, 33, 35, 36, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 58, 60, 61, 63, 65, 66, 67, 68, 70, 73, 74, 75, 77, 79, 81, 82, 84, 85, 86, 87, 89, 90, 92, 93, 95, 96, 98, 100, 102, 103, 104, 106, 107, 108, 110, 112, 113, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 130, 133, 135, 136, 139, 140, 141, 142, 143, 144, 145, 146, 147, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 173, 175, 176, 177, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 194, 196, 197, 198, 199, 200, 201, 203, 204, 205, 207, 208, 209, 210, 212, 213, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 229, 231, 232, 234, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 262, 263, 264, 267, 268, 269, 271, 272, 273, 274, 275, 280, 281, 282, 283, 284, 286, 287, 288, 289, 292, 294, 295, 296, 297, 298, 299, 302, 303, 305, 306, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 322, 323, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('gaussian_valid_251.csv')\n",
    "df['query_id'] = df['query_id'].astype(str).str.zfill(3)\n",
    "result_dict = df.groupby('query_id')['query_instance'].agg(list).to_dict()\n",
    "\n",
    "print(f\"Length of query_id: {len(result_dict)}\")\n",
    "print(\"Key:\", sorted(result_dict.keys()))\n",
    "print(\"Instance length:\", set([len(value) for key, value in sorted(result_dict.items(), key=lambda x: x[0])]))\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all query length: 13\n",
      "train length: {50}\n",
      "test length: {200}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "np.random.seed(2024)\n",
    "\n",
    "# split train & test\n",
    "train_test_dict = {}\n",
    "\n",
    "# shuffle instance list\n",
    "for query_id, instances in result_dict.items():\n",
    "    instances = np.array(instances)\n",
    "    np.random.shuffle(instances)\n",
    "    instances = instances.tolist()\n",
    "    \n",
    "    # 50 training + 200 testing\n",
    "    train_instances = instances[:50]\n",
    "    test_instances = instances[50:250]\n",
    "    \n",
    "    # save result\n",
    "    train_test_dict[query_id] = {\n",
    "        \"train\": train_instances,\n",
    "        \"test\": test_instances\n",
    "    }\n",
    "\n",
    "with open('0_train_test.json', 'w') as f:\n",
    "    json.dump(train_test_dict, f, indent=2)\n",
    "\n",
    "print(f\"all query length: {len(train_test_dict)}\")\n",
    "print(f\"train length: {set([len(value['train']) for key, value in sorted(train_test_dict.items(), key=lambda x: x[0])])}\")\n",
    "print(f\"test length: {set([len(value['test']) for key, value in sorted(train_test_dict.items(), key=lambda x: x[0])])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without table alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Query 099\n",
      "Training (50)\n",
      "Testing (200)...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_train_test_data(json_file: str = '0_train_test.json') -> Dict:\n",
    "    with open(json_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_table_dict(json_file: str = '0_table_dict.json') -> Dict:\n",
    "    with open(json_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def read_sql_file(file_path: str) -> str:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def process_condition_files(query_id: str, instance_list: List[str], mode: str, table_dict: Dict) -> Dict[str, List[str]]:\n",
    "    table_conditions = defaultdict(list)\n",
    "    \n",
    "    for i in instance_list:\n",
    "        current_instance_conditions = defaultdict(list)\n",
    "        \n",
    "        condition_file = f\"queries/query_condition_{query_id}/query_condition_{query_id}_{i}.sql\"\n",
    "        content = read_sql_file(condition_file)\n",
    "        \n",
    "        for line in content.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                table_key = line.strip().split('_')[0]\n",
    "                table_name = table_dict.get(table_key)\n",
    "                if table_name:\n",
    "                    current_instance_conditions[table_name].append(line.strip())\n",
    "        \n",
    "        for table_name, conditions in current_instance_conditions.items():\n",
    "            instance_str = '[\"' + '\", \"'.join(conditions) + '\"]'\n",
    "            table_conditions[table_name].append(instance_str)\n",
    "    \n",
    "    for table_name, instance_arrays in table_conditions.items():\n",
    "        output_file = f\"PQO/{query_id}/{query_id}_{len(instance_list)}_{mode}_{table_name}.txt\"\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(',\\n'.join(instance_arrays))\n",
    "    \n",
    "    return table_conditions\n",
    "\n",
    "def process_query_files(query_id: str, instance_list: List[str], mode: str):\n",
    "    queries_dict = {}\n",
    "    \n",
    "    for idx, i in enumerate(instance_list):\n",
    "        query_file = f\"queries/query{query_id}_spj/query{query_id}_spj_{i}.sql\"\n",
    "        content = read_sql_file(query_file)\n",
    "        \n",
    "        key = f\"{query_id}_{mode}_{len(instance_list)}_{idx}\"\n",
    "        queries_dict[key] = content\n",
    "    \n",
    "    # create json file for each query\n",
    "    output_file = f\"PQO/{query_id}/{query_id}_{mode}_{len(instance_list)}.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(queries_dict, f, indent=2)\n",
    "    \n",
    "    return queries_dict\n",
    "\n",
    "def main():\n",
    "    train_test_data = load_train_test_data()\n",
    "    table_dict = load_table_dict()\n",
    "    query_id = '099'\n",
    "    print(f\"Handling Query {query_id}\")\n",
    "\n",
    "    train_instances = train_test_data[query_id]['train']\n",
    "    test_instances = train_test_data[query_id]['test']\n",
    "\n",
    "    print(f\"Training ({len(train_instances)})\")\n",
    "    process_condition_files(query_id, train_instances, 'train', table_dict)\n",
    "    process_query_files(query_id, train_instances, 'training')\n",
    "    \n",
    "    print(f\"Testing ({len(test_instances)})...\")\n",
    "    process_condition_files(query_id, test_instances, 'test', table_dict)\n",
    "    process_query_files(query_id, test_instances, 'testing')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with table alias: 025, 050, 072, 085, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Query 025\n",
      "Training (50)\n",
      "Testing (200)...\n",
      "Handling Query 050\n",
      "Training (50)\n",
      "Testing (200)...\n",
      "Handling Query 072\n",
      "Training (50)\n",
      "Testing (200)...\n",
      "Handling Query 085\n",
      "Training (50)\n",
      "Testing (200)...\n",
      "Handling Query 100\n",
      "Training (50)\n",
      "Testing (200)...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_train_test_data(json_file: str = '0_train_test.json') -> Dict:\n",
    "    with open(json_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_table_dict(json_file: str = '0_table_dict.json') -> Dict:\n",
    "    with open(json_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def read_sql_file(file_path: str) -> str:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def process_condition_files(query_id: str, instance_list: List[str], mode: str, table_dict: Dict) -> Dict[str, List[str]]:\n",
    "    table_conditions = defaultdict(list)\n",
    "    \n",
    "    for i in instance_list:\n",
    "        current_instance_conditions = defaultdict(list)\n",
    "        \n",
    "        condition_file = f\"queries/query_condition_{query_id}/query_condition_{query_id}_{i}.sql\"\n",
    "        content = read_sql_file(condition_file)\n",
    "        \n",
    "        for line in content.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                parts = line.strip().split('_')[0]\n",
    "    \n",
    "                # check table alias\n",
    "                if '.' in parts:\n",
    "                    # e.g. d1.d_moy\n",
    "                    table_alias_number = parts.split('.')[0][-1]  # get 1\n",
    "                    table_key = parts.split('.')[1]  # get d\n",
    "                else:\n",
    "                    # original case\n",
    "                    table_key = parts\n",
    "                    table_alias_number = None\n",
    "                \n",
    "                table_name = table_dict.get(table_key)\n",
    "                if table_name:\n",
    "                    if table_alias_number is not None:\n",
    "                        current_instance_conditions[f\"{table_name}_{table_alias_number}\"].append(line.strip())\n",
    "                    else:\n",
    "                        current_instance_conditions[table_name].append(line.strip())\n",
    "        \n",
    "        for table_name, conditions in current_instance_conditions.items():\n",
    "            instance_str = '[\"' + '\", \"'.join(conditions) + '\"]'\n",
    "            table_conditions[table_name].append(instance_str)\n",
    "    \n",
    "    for table_name, instance_arrays in table_conditions.items():\n",
    "        output_file = f\"PQO/{query_id}/{query_id}_{len(instance_list)}_{mode}_{table_name}.txt\"\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(',\\n'.join(instance_arrays))\n",
    "    \n",
    "    return table_conditions\n",
    "\n",
    "def process_query_files(query_id: str, instance_list: List[str], mode: str):\n",
    "    queries_dict = {}\n",
    "    \n",
    "    for idx, i in enumerate(instance_list):\n",
    "        query_file = f\"queries/query{query_id}_spj/query{query_id}_spj_{i}.sql\"\n",
    "        content = read_sql_file(query_file)\n",
    "        \n",
    "        key = f\"{query_id}_{mode}_{len(instance_list)}_{idx}\"\n",
    "        queries_dict[key] = content\n",
    "    \n",
    "    # create json file for each query\n",
    "    output_file = f\"PQO/{query_id}/{query_id}_{mode}_{len(instance_list)}.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(queries_dict, f, indent=2)\n",
    "    \n",
    "    return queries_dict\n",
    "\n",
    "def main():\n",
    "    train_test_data = load_train_test_data()\n",
    "    table_dict = load_table_dict()\n",
    "    \n",
    "    for query_id in ['025', '050', '072', '085', '100']:\n",
    "        print(f\"Handling Query {query_id}\")\n",
    "\n",
    "        train_instances = train_test_data[query_id]['train']\n",
    "        test_instances = train_test_data[query_id]['test']\n",
    "\n",
    "        print(f\"Training ({len(train_instances)})\")\n",
    "        process_condition_files(query_id, train_instances, 'train', table_dict)\n",
    "        process_query_files(query_id, train_instances, 'training')\n",
    "        \n",
    "        print(f\"Testing ({len(test_instances)})...\")\n",
    "        process_condition_files(query_id, test_instances, 'test', table_dict)\n",
    "        process_query_files(query_id, test_instances, 'testing')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate query_id: param list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed files and saved output to PQO_gaussian_sql.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def read_sql_file(file_path):\n",
    "    \"\"\"\n",
    "    Read SQL file and process its content according to requirements.\n",
    "    Returns a list of statements after processing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Split by newlines and process each line\n",
    "        stmts = []\n",
    "        for line in content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            # Skip single characters\n",
    "            if len(line) <= 1:\n",
    "                continue\n",
    "            stmts.append(line)\n",
    "        \n",
    "        return stmts\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading SQL file {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_files():\n",
    "    # Read the input JSON file\n",
    "    try:\n",
    "        with open('0_train_test.json', 'r', encoding='utf-8') as f:\n",
    "            train_test_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading input JSON file: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Output dictionary\n",
    "    output_data = {}\n",
    "\n",
    "    # Process each query\n",
    "    for query_id, data in train_test_data.items():\n",
    "        output_data[query_id] = {\"train\": [], \"test\": []}\n",
    "        \n",
    "        # Process train instances\n",
    "        for idx, i in enumerate(data.get(\"train\", [])):\n",
    "            sql_file = f\"queries/query_condition_{query_id}/query_condition_{query_id}_{i}.sql\"\n",
    "            if os.path.exists(sql_file):\n",
    "                stmts = read_sql_file(sql_file)\n",
    "                output_data[query_id][\"train\"].append(stmts)\n",
    "            else:\n",
    "                print(f\"Warning: Train SQL file not found: {sql_file}\")\n",
    "        \n",
    "        # Process test instances\n",
    "        for idx, i in enumerate(data.get(\"test\", [])):\n",
    "            sql_file = f\"queries/query_condition_{query_id}/query_condition_{query_id}_{i}.sql\"\n",
    "            if os.path.exists(sql_file):\n",
    "                stmts = read_sql_file(sql_file)\n",
    "                output_data[query_id][\"test\"].append(stmts)\n",
    "            else:\n",
    "                print(f\"Warning: Test SQL file not found: {sql_file}\")\n",
    "\n",
    "    # Save the output JSON file\n",
    "    try:\n",
    "        with open('PQO_gaussian_sql.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "        print(\"Successfully processed files and saved output to PQO_gaussian_sql.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving output JSON file: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 013:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 6\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 6\n",
      "\n",
      "Query 018:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 6\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 6\n",
      "\n",
      "Query 019:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 5\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 5\n",
      "\n",
      "Query 025:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 3\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 3\n",
      "\n",
      "Query 027:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 4\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 4\n",
      "\n",
      "Query 040:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 4\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 4\n",
      "\n",
      "Query 050:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 3\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 3\n",
      "\n",
      "Query 072:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 5\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 5\n",
      "\n",
      "Query 084:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 2\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 2\n",
      "\n",
      "Query 085:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 7\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 7\n",
      "\n",
      "Query 091:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 3\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 3\n",
      "\n",
      "Query 099:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 5\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 5\n",
      "\n",
      "Query 100:\n",
      "Train list length: 50\n",
      "Train instances have consistent length: 6\n",
      "Test list length: 200\n",
      "Test instances have consistent length: 6\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_json_structure():\n",
    "    # Read the JSON file\n",
    "    try:\n",
    "        with open('PQO_gaussian_sql.json', 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading JSON file: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Analyze each query\n",
    "    for query_id, content in data.items():\n",
    "        print(f\"\\nQuery {query_id}:\")\n",
    "        \n",
    "        # Analyze train data\n",
    "        train_data = content.get('train', [])\n",
    "        train_len = len(train_data)\n",
    "        print(f\"Train list length: {train_len}\")\n",
    "        \n",
    "        # Check train instance lengths\n",
    "        train_instance_lengths = [len(instance) for instance in train_data]\n",
    "        train_lengths_consistent = len(set(train_instance_lengths)) == 1\n",
    "        \n",
    "        if train_lengths_consistent:\n",
    "            print(f\"Train instances have consistent length: {train_instance_lengths[0]}\")\n",
    "        else:\n",
    "            print(\"Train instances have varying lengths:\")\n",
    "            length_count = defaultdict(int)\n",
    "            for length in train_instance_lengths:\n",
    "                length_count[length] += 1\n",
    "            for length, count in sorted(length_count.items()):\n",
    "                print(f\"  Length {length}: {count} instances\")\n",
    "        \n",
    "        # Analyze test data\n",
    "        test_data = content.get('test', [])\n",
    "        test_len = len(test_data)\n",
    "        print(f\"Test list length: {test_len}\")\n",
    "        \n",
    "        # Check test instance lengths\n",
    "        test_instance_lengths = [len(instance) for instance in test_data]\n",
    "        test_lengths_consistent = len(set(test_instance_lengths)) == 1\n",
    "        \n",
    "        if test_lengths_consistent:\n",
    "            print(f\"Test instances have consistent length: {test_instance_lengths[0]}\")\n",
    "        else:\n",
    "            print(\"Test instances have varying lengths:\")\n",
    "            length_count = defaultdict(int)\n",
    "            for length in test_instance_lengths:\n",
    "                length_count[length] += 1\n",
    "            for length, count in sorted(length_count.items()):\n",
    "                print(f\"  Length {length}: {count} instances\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_json_structure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yang_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
