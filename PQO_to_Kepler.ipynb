{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need split.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. training_params.json and testing_params.json have been created.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_params(value, param_splits, param_index):\n",
    "    \"\"\"\n",
    "    Extract parameters based on the splitting rules:\n",
    "    - For 2 elements: split(ele[0])[1].split(ele[1])[0]\n",
    "    - For 3 elements: split(ele[0])[1].split(ele[1])[1].split(ele[2])[0]\n",
    "    Last two parameters should be converted to int\n",
    "    \"\"\"\n",
    "    if len(param_splits) == 2:\n",
    "        param_value = value.split(param_splits[0])[1].split(param_splits[1])[0]\n",
    "    elif len(param_splits) == 3:\n",
    "        param_value = value.split(param_splits[0])[1].split(param_splits[1])[1].split(param_splits[2])[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Convert last two parameters to int\n",
    "    if param_index >= 14:  # @param14 and @param15\n",
    "        return int(param_value)\n",
    "    return param_value\n",
    "\n",
    "def process_single_dataset(data, split_data):\n",
    "    \"\"\"Process a single dataset (either training or testing)\"\"\"\n",
    "    all_params = []\n",
    "    for key in data.keys():\n",
    "        value = data[key]\n",
    "        query_params = []\n",
    "        \n",
    "        # Extract parameters for each @param in order\n",
    "        for i in range(16):  # @param0 to @param15\n",
    "            param_name = f\"@param{i}\"\n",
    "            split_rules = split_data[param_name]\n",
    "            param_value = extract_params(value, split_rules, i)\n",
    "            if param_value is not None:\n",
    "                query_params.append(param_value)\n",
    "        \n",
    "        all_params.append(query_params)\n",
    "    return all_params\n",
    "\n",
    "def process_queries(method_path, split_data):\n",
    "    # Define input paths\n",
    "    training_path = os.path.join(method_path, 'inputs', 'PQO', 'query', '29-0_training_50.json')\n",
    "    testing_path = os.path.join(method_path, 'inputs', 'PQO', 'query', '29-0_testing.json')\n",
    "    \n",
    "    # Load JSON files\n",
    "    training_data = load_json_file(training_path)\n",
    "    testing_data = load_json_file(testing_path)\n",
    "    \n",
    "    # Process training and testing data separately\n",
    "    training_params = process_single_dataset(training_data, split_data)\n",
    "    testing_params = process_single_dataset(testing_data, split_data)\n",
    "    \n",
    "    # Save training results with Unicode escapes preserved\n",
    "    training_output = os.path.join(method_path, \"training_params.json\")\n",
    "    with open(training_output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(training_params, f, indent=4)\n",
    "        \n",
    "    # Save testing results with Unicode escapes preserved\n",
    "    testing_output = os.path.join(method_path, \"testing_params.json\")\n",
    "    with open(testing_output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(testing_params, f, indent=4)\n",
    "\n",
    "def main():\n",
    "    # Get method path from user\n",
    "    method_path = 'kepler'\n",
    "    \n",
    "    # Load split data\n",
    "    split_path = 'PQO_to_Kepler_split.json'\n",
    "    split_data = load_json_file(split_path)\n",
    "    \n",
    "    # Process the queries\n",
    "    process_queries(method_path, split_data)\n",
    "    print(\"Processing complete. training_params.json and testing_params.json have been created.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Frequency files have been created in kepler/frequency\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import collections\n",
    "\n",
    "def get_literal_frequencies(literals):\n",
    "    \"\"\"\n",
    "    calculate param with frequency\n",
    "    \"\"\"\n",
    "    frequency_dict = collections.defaultdict(int)\n",
    "    \n",
    "    for literal in literals:\n",
    "        frequency_dict[json.dumps(literal)] += 1\n",
    "    \n",
    "    return frequency_dict\n",
    "\n",
    "def process_params_and_store_frequency(query_id, train_params, test_params, output_dir, train_size_list=[50, 400]):\n",
    "    \"\"\"\n",
    "    Process params and store their frequencies in appropriate directories\n",
    "    \n",
    "    Args:\n",
    "        query_id: The ID of the query\n",
    "        train_params: List of parameter lists from training set\n",
    "        test_params: List of parameter lists from testing set\n",
    "        output_dir: Directory to store the output files\n",
    "        train_size_list: List of training sizes to process\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get distinct literals and their frequencies\n",
    "    distinct_train_literals = list(set(map(tuple, train_params)))\n",
    "    distinct_train_literals = [list(item) for item in distinct_train_literals]\n",
    "    \n",
    "    distinct_test_literals = list(set(map(tuple, test_params)))\n",
    "    distinct_test_literals = [list(item) for item in distinct_test_literals]\n",
    "    \n",
    "    # Calculate frequencies\n",
    "    train_literal_frequencies = get_literal_frequencies(train_params)\n",
    "    test_literal_frequencies = get_literal_frequencies(test_params)\n",
    "    \n",
    "    # Store all train_dicts in a dictionary, keyed by train_size\n",
    "    train_dict_dict = {}\n",
    "    \n",
    "    # Process full training set\n",
    "    full_train_dict = {json.dumps(literal): train_literal_frequencies[json.dumps(literal)] \n",
    "                      for literal in distinct_train_literals}\n",
    "    train_dict_dict[len(train_params)] = full_train_dict\n",
    "    \n",
    "    # Process test set\n",
    "    test_dict = {json.dumps(literal): test_literal_frequencies[json.dumps(literal)] \n",
    "                 for literal in distinct_test_literals}\n",
    "    \n",
    "    # Process different training sizes\n",
    "    for train_size in train_size_list:\n",
    "        if train_size > len(train_params):\n",
    "            print(f\"Warning: Requested train_size {train_size} is larger than available training data {len(train_params)}\")\n",
    "            continue\n",
    "            \n",
    "        # Get the subset of train_params based on the current train_size\n",
    "        train_subset = train_params[:train_size]\n",
    "        new_train_literal_freq = get_literal_frequencies(train_subset)\n",
    "\n",
    "        # Get distinct train literals for this subset\n",
    "        distinct_train_subset = list(set(map(tuple, train_subset)))\n",
    "        distinct_train_subset = [list(item) for item in distinct_train_subset]\n",
    "        \n",
    "        # Create a frequency dictionary for this subset\n",
    "        train_dict = {json.dumps(literal): new_train_literal_freq[json.dumps(literal)] \n",
    "                     for literal in distinct_train_subset}\n",
    "        \n",
    "        # Add the current train size dictionary\n",
    "        train_dict_dict[train_size] = train_dict\n",
    "    \n",
    "    # Create output directory\n",
    "    base_dir = \"frequency\"\n",
    "    output_dir_path = os.path.join(output_dir, base_dir)\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    # Save train frequencies for each size\n",
    "    for train_size, train_dict in train_dict_dict.items():\n",
    "        train_output_file = os.path.join(output_dir_path, f\"{query_id}_train_{train_size}_freq.json\")\n",
    "        with open(train_output_file, 'w') as train_file:\n",
    "            json.dump(train_dict, train_file, indent=4)\n",
    "\n",
    "    # Save test frequencies\n",
    "    test_output_file = os.path.join(output_dir_path, f\"{query_id}_test_freq.json\")\n",
    "    with open(test_output_file, 'w') as test_file:\n",
    "        json.dump(test_dict, test_file, indent=4)\n",
    "    \n",
    "    return train_dict_dict, test_dict\n",
    "\n",
    "def main():\n",
    "    # Load params\n",
    "    method_path = 'kepler'\n",
    "    training_params = json.load(open(os.path.join(method_path, 'training_params.json')))\n",
    "    testing_params = json.load(open(os.path.join(method_path, 'testing_params.json')))\n",
    "    \n",
    "    # Process for query 29-0\n",
    "    query_id = '29-0'\n",
    "    train_dict_dict, test_dict = process_params_and_store_frequency(\n",
    "        query_id=query_id,\n",
    "        train_params=training_params,\n",
    "        test_params=testing_params,\n",
    "        output_dir=method_path,\n",
    "        train_size_list=[50]\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing complete. Frequency files have been created in {os.path.join(method_path, 'frequency')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "def process_json_files(method):\n",
    "    # 读取原始JSON文件\n",
    "    with open('29-0.json', 'r') as f:\n",
    "        original_data = json.load(f)\n",
    "    \n",
    "    # 读取testing参数\n",
    "    params_path = os.path.join(method, 'training_params.json')\n",
    "    with open(params_path, 'r') as f:\n",
    "        training_params = json.load(f)\n",
    "        \n",
    "    params_path = os.path.join(method, 'testing_params.json')\n",
    "    with open(params_path, 'r', encoding='utf-8') as f:\n",
    "        testing_params = json.load(f)\n",
    "    \n",
    "    # 创建目录结构\n",
    "    training_dir = os.path.join(method, 'inputs', 'training')\n",
    "    testing_dir = os.path.join(method, 'inputs', 'testing')\n",
    "    os.makedirs(training_dir, exist_ok=True)\n",
    "    os.makedirs(testing_dir, exist_ok=True)\n",
    "    \n",
    "    # 处理training文件\n",
    "    training_data = original_data.copy()\n",
    "    training_data['29-0']['params'] = training_params  # 添加参数\n",
    "    \n",
    "    # 保存training文件\n",
    "    training_distinct_path = os.path.join(training_dir, '29-0_training_distinct_50.json')\n",
    "    training_original_path = os.path.join(training_dir, '29-0_training_original_50.json')\n",
    "    \n",
    "    with open(training_distinct_path, 'w') as f:\n",
    "        json.dump(training_data, f, indent=2)\n",
    "    with open(training_original_path, 'w') as f:\n",
    "        json.dump(training_data, f, indent=2)\n",
    "    \n",
    "    # 处理testing文件\n",
    "    testing_data = original_data.copy()\n",
    "    testing_data['29-0']['params'] = testing_params\n",
    "    \n",
    "    # 保存testing文件\n",
    "    testing_distinct_path = os.path.join(testing_dir, '29-0_testing_distinct.json')\n",
    "    testing_original_path = os.path.join(testing_dir, '29-0_testing_original.json')\n",
    "    testing_original_single_path = os.path.join(testing_dir, '29-0_testing_original_single.json')\n",
    "    \n",
    "    with open(testing_distinct_path, 'w') as f:\n",
    "        json.dump(testing_data, f, indent=2)\n",
    "    with open(testing_original_path, 'w') as f:\n",
    "        json.dump(testing_data, f, indent=2)\n",
    "    with open(testing_original_single_path, 'w') as f:\n",
    "        json.dump(testing_data, f, indent=2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    method = 'kepler'  # 替换为实际的方法名\n",
    "    process_json_files(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def process_query_instances(method, query_id=\"29-0\", count=251):\n",
    "    # Read the JSON file\n",
    "    file_path = f\"{method}/inputs/training/{query_id}_training_original_50.json\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    query = data[query_id][\"query\"]\n",
    "    param_list = data[query_id][\"params\"]\n",
    "    \n",
    "    query_instances = []\n",
    "    counter = 0\n",
    "    \n",
    "    for params in param_list:\n",
    "        test_query = query\n",
    "        \n",
    "        for i, param in enumerate(params):\n",
    "            param = str(param).strip()\n",
    "            pattern = re.compile(rf\"@param{i}\\b\")\n",
    "            test_query = pattern.sub(param, test_query)\n",
    "        \n",
    "        query_instances.append(test_query)\n",
    "        counter += 1\n",
    "        \n",
    "        if counter == count:\n",
    "            break\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open('query_instances.json', 'w') as f:\n",
    "        json.dump(query_instances, f, indent=2)\n",
    "    \n",
    "    return query_instances\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    method = \"kepler\"  # Replace with actual method name\n",
    "    instances = process_query_instances(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All queries match!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def compare_query_instances(method, query_id=\"29-0\"):\n",
    "    # Read the first JSON file (PQO file)\n",
    "    pqo_file_path = f\"{method}/inputs/PQO/query/{query_id}_training_50.json\"\n",
    "    with open(pqo_file_path, 'r') as f:\n",
    "        pqo_data = json.load(f)\n",
    "    pqo_queries = list(pqo_data.values())  # Get just the values\n",
    "    \n",
    "    # Read the second JSON file (generated instances)\n",
    "    instances_file_path = f\"query_instances.json\"\n",
    "    with open(instances_file_path, 'r') as f:\n",
    "        generated_queries = json.load(f)\n",
    "    \n",
    "    # Check if lengths match\n",
    "    if len(pqo_queries) != len(generated_queries):\n",
    "        print(f\"Length mismatch: PQO has {len(pqo_queries)} queries, Generated has {len(generated_queries)} queries\")\n",
    "        return False\n",
    "    \n",
    "    # Compare each query\n",
    "    for i, (pqo_query, gen_query) in enumerate(zip(pqo_queries, generated_queries)):\n",
    "        if pqo_query.strip() != gen_query.strip():\n",
    "            print(f\"Mismatch at index {i}:\")\n",
    "            print(f\"PQO query: {pqo_query}\")\n",
    "            print(f\"Generated query: {gen_query}\")\n",
    "            return False\n",
    "    \n",
    "    print(\"All queries match!\")\n",
    "    return True\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    method = \"kepler\"  # Replace with actual method name\n",
    "    are_equal = compare_query_instances(method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yang_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
