{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join_predicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join_predicates without table alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed query 084\n",
      "Successfully processed query 091\n",
      "Successfully processed query 099\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_table_dict(json_file: str = '0_table_dict.json') -> Dict:\n",
    "    with open(json_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_predicates(query_id: str):\n",
    "    table_dict = load_table_dict()\n",
    "    \n",
    "    predicate_file = f\"raw_predicate_tpl/query_condition_{query_id}.tpl\"\n",
    "    with open(predicate_file, 'r') as f:\n",
    "        predicate_content = f.read()\n",
    "    \n",
    "    # read train & test\n",
    "    training_file = f\"dsb_{query_id}_original/cardinality/inputs/training/{query_id}_training_original_50.json\"\n",
    "    testing_file = f\"dsb_{query_id}_original/cardinality/inputs/testing/{query_id}_testing_original.json\"\n",
    "    \n",
    "    with open(training_file, 'r') as f:\n",
    "        training_data = json.load(f)\n",
    "        training_params = training_data[query_id][\"params\"]\n",
    "    \n",
    "    with open(testing_file, 'r') as f:\n",
    "        testing_data = json.load(f)\n",
    "        testing_params = testing_data[query_id][\"params\"]\n",
    "    \n",
    "    process_mode(query_id, predicate_content, training_params, table_dict, \"50_training\")\n",
    "    process_mode(query_id, predicate_content, testing_params, table_dict, \"200_testing\")\n",
    "\n",
    "\n",
    "def process_mode(query_id: str, predicate_content: str, params: List[List], table_dict: Dict, mode: str):\n",
    "    table_conditions = defaultdict(list)\n",
    "    \n",
    "    for param_set in params:\n",
    "        current_instance_conditions = defaultdict(list)\n",
    "        current_content = predicate_content\n",
    "        \n",
    "        # substitute\n",
    "        for i, param in enumerate(param_set):\n",
    "            pattern = re.compile(rf\"@param{i}\\b\")\n",
    "            current_content = pattern.sub(str(param).strip(), current_content)\n",
    "        \n",
    "        # process conditions\n",
    "        for line in current_content.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                table_key = line.strip().split('_')[0]\n",
    "                table_name = table_dict.get(table_key)\n",
    "                if table_name:\n",
    "                    current_instance_conditions[table_name].append(line.strip())\n",
    "        \n",
    "        # for each table\n",
    "        for table_name, conditions in current_instance_conditions.items():\n",
    "            instance_str = '[\"' + '\", \"'.join(conditions) + '\"]'\n",
    "            table_conditions[table_name].append(instance_str)\n",
    "    \n",
    "    # output\n",
    "    output_dir = f\"dsb_{query_id}_original/cardinality/inputs/PQO/join_predicates\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for table_name, instance_arrays in table_conditions.items():\n",
    "        output_file = f\"{output_dir}/{query_id}_{mode}_{table_name}.txt\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(',\\n'.join(instance_arrays))\n",
    "\n",
    "def main():\n",
    "    query_ids = ['084', '091', '099']\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        try:\n",
    "            process_predicates(query_id)\n",
    "            print(f\"Successfully processed query {query_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {query_id}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed query 013\n",
      "Successfully processed query 018\n",
      "Successfully processed query 019\n",
      "Successfully processed query 027\n",
      "Successfully processed query 040\n",
      "Successfully processed query 084\n",
      "Successfully processed query 091\n",
      "Successfully processed query 099\n"
     ]
    }
   ],
   "source": [
    "# for mixture\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_table_dict(json_file: str = '0_table_dict.json') -> Dict:\n",
    "    with open(json_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_predicates(query_id: str):\n",
    "    table_dict = load_table_dict()\n",
    "    \n",
    "    predicate_file = f\"raw_predicate_tpl/query_condition_{query_id}.tpl\"\n",
    "    with open(predicate_file, 'r') as f:\n",
    "        predicate_content = f.read()\n",
    "    \n",
    "    # read train & test\n",
    "    testing_file = f\"0_mixture_test/{query_id}/{query_id}_mixture_test.json\"\n",
    "    \n",
    "    with open(testing_file, 'r') as f:\n",
    "        testing_data = json.load(f)\n",
    "        testing_params = testing_data[query_id][\"params\"]\n",
    "    \n",
    "    process_mode(query_id, predicate_content, testing_params, table_dict, \"200_testing\")\n",
    "\n",
    "\n",
    "def process_mode(query_id: str, predicate_content: str, params: List[List], table_dict: Dict, mode: str):\n",
    "    table_conditions = defaultdict(list)\n",
    "    \n",
    "    for param_set in params:\n",
    "        current_instance_conditions = defaultdict(list)\n",
    "        current_content = predicate_content\n",
    "        \n",
    "        # substitute\n",
    "        for i, param in enumerate(param_set):\n",
    "            pattern = re.compile(rf\"@param{i}\\b\")\n",
    "            current_content = pattern.sub(str(param).strip(), current_content)\n",
    "        \n",
    "        # process conditions\n",
    "        for line in current_content.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                table_key = line.strip().split('_')[0]\n",
    "                table_name = table_dict.get(table_key)\n",
    "                if table_name:\n",
    "                    current_instance_conditions[table_name].append(line.strip())\n",
    "        \n",
    "        # for each table\n",
    "        for table_name, conditions in current_instance_conditions.items():\n",
    "            instance_str = '[\"' + '\", \"'.join(conditions) + '\"]'\n",
    "            table_conditions[table_name].append(instance_str)\n",
    "    \n",
    "    # output\n",
    "    output_dir = f\"0_mixture_test/{query_id}/PQO\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for table_name, instance_arrays in table_conditions.items():\n",
    "        output_file = f\"{output_dir}/{query_id}_{mode}_{table_name}.txt\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(',\\n'.join(instance_arrays))\n",
    "\n",
    "def main():\n",
    "    query_ids = ['013', '018', '019', '027', '040', '084', '091', '099']\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        try:\n",
    "            process_predicates(query_id)\n",
    "            print(f\"Successfully processed query {query_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {query_id}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join_predicates with table alias: 025, 050, 072, 085, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed query 085\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_table_dict(json_file: str = '0_table_dict.json') -> Dict:\n",
    "    with open(json_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_predicates(query_id: str):\n",
    "    table_dict = load_table_dict()\n",
    "    \n",
    "    predicate_file = f\"raw_predicate_tpl/query_condition_{query_id}.tpl\"\n",
    "    with open(predicate_file, 'r') as f:\n",
    "        predicate_content = f.read()\n",
    "    \n",
    "    # read train & test\n",
    "    training_file = f\"dsb_{query_id}_original/cardinality/inputs/training/{query_id}_training_original_50.json\"\n",
    "    testing_file = f\"dsb_{query_id}_original/cardinality/inputs/testing/{query_id}_testing_original.json\"\n",
    "    \n",
    "    with open(training_file, 'r') as f:\n",
    "        training_data = json.load(f)\n",
    "        training_params = training_data[query_id][\"params\"]\n",
    "    \n",
    "    with open(testing_file, 'r') as f:\n",
    "        testing_data = json.load(f)\n",
    "        testing_params = testing_data[query_id][\"params\"]\n",
    "    \n",
    "    process_mode(query_id, predicate_content, training_params, table_dict, \"50_training\")\n",
    "    process_mode(query_id, predicate_content, testing_params, table_dict, \"200_testing\")\n",
    "\n",
    "\n",
    "def process_mode(query_id: str, predicate_content: str, params: List[List], table_dict: Dict, mode: str):\n",
    "    table_conditions = defaultdict(list)\n",
    "    \n",
    "    for param_set in params:\n",
    "        current_instance_conditions = defaultdict(list)\n",
    "        current_content = predicate_content\n",
    "        \n",
    "        # substitute\n",
    "        for i, param in enumerate(param_set):\n",
    "            pattern = re.compile(rf\"@param{i}\\b\")\n",
    "            current_content = pattern.sub(str(param).strip(), current_content)\n",
    "        \n",
    "        # process conditions\n",
    "        for line in current_content.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                parts = line.strip().split('_')[0]\n",
    "    \n",
    "                # check table alias\n",
    "                if '.' in parts:\n",
    "                    # e.g. d1.d_moy\n",
    "                    table_alias_number = parts.split('.')[0][-1]  # get 1\n",
    "                    table_key = parts.split('.')[1]  # get d\n",
    "                else:\n",
    "                    # original case\n",
    "                    table_key = parts\n",
    "                    table_alias_number = None\n",
    "                    \n",
    "                table_name = table_dict.get(table_key)\n",
    "                if table_name:\n",
    "                    if table_alias_number is not None:\n",
    "                        current_instance_conditions[f\"{table_name}_{table_alias_number}\"].append(line.strip())\n",
    "                    else:\n",
    "                        current_instance_conditions[table_name].append(line.strip())\n",
    "        \n",
    "        # for each table\n",
    "        for table_name, conditions in current_instance_conditions.items():\n",
    "            instance_str = '[\"' + '\", \"'.join(conditions) + '\"]'\n",
    "            table_conditions[table_name].append(instance_str)\n",
    "    \n",
    "    # output\n",
    "    output_dir = f\"dsb_{query_id}_original/cardinality/inputs/PQO/join_predicates\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for table_name, instance_arrays in table_conditions.items():\n",
    "        output_file = f\"{output_dir}/{query_id}_{mode}_{table_name}.txt\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(',\\n'.join(instance_arrays))\n",
    "\n",
    "def main():\n",
    "    query_ids = ['085']\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        try:\n",
    "            process_predicates(query_id)\n",
    "            print(f\"Successfully processed query {query_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {query_id}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed query 025\n",
      "Successfully processed query 050\n",
      "Successfully processed query 072\n",
      "Successfully processed query 085\n",
      "Successfully processed query 100\n"
     ]
    }
   ],
   "source": [
    "# for mixture\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_table_dict(json_file: str = '0_table_dict.json') -> Dict:\n",
    "    with open(json_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_predicates(query_id: str):\n",
    "    table_dict = load_table_dict()\n",
    "    \n",
    "    predicate_file = f\"raw_predicate_tpl/query_condition_{query_id}.tpl\"\n",
    "    with open(predicate_file, 'r') as f:\n",
    "        predicate_content = f.read()\n",
    "    \n",
    "    # read train & test\n",
    "    testing_file = f\"0_mixture_test/{query_id}/{query_id}_mixture_test.json\"\n",
    "    \n",
    "    with open(testing_file, 'r') as f:\n",
    "        testing_data = json.load(f)\n",
    "        testing_params = testing_data[query_id][\"params\"]\n",
    "    \n",
    "    process_mode(query_id, predicate_content, testing_params, table_dict, \"200_testing\")\n",
    "\n",
    "\n",
    "def process_mode(query_id: str, predicate_content: str, params: List[List], table_dict: Dict, mode: str):\n",
    "    table_conditions = defaultdict(list)\n",
    "    \n",
    "    for param_set in params:\n",
    "        current_instance_conditions = defaultdict(list)\n",
    "        current_content = predicate_content\n",
    "        \n",
    "        # substitute\n",
    "        for i, param in enumerate(param_set):\n",
    "            pattern = re.compile(rf\"@param{i}\\b\")\n",
    "            current_content = pattern.sub(str(param).strip(), current_content)\n",
    "        \n",
    "        # process conditions\n",
    "        for line in current_content.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                parts = line.strip().split('_')[0]\n",
    "    \n",
    "                # check table alias\n",
    "                if '.' in parts:\n",
    "                    # e.g. d1.d_moy\n",
    "                    table_alias_number = parts.split('.')[0][-1]  # get 1\n",
    "                    table_key = parts.split('.')[1]  # get d\n",
    "                else:\n",
    "                    # original case\n",
    "                    table_key = parts\n",
    "                    table_alias_number = None\n",
    "                    \n",
    "                table_name = table_dict.get(table_key)\n",
    "                if table_name:\n",
    "                    if table_alias_number is not None:\n",
    "                        current_instance_conditions[f\"{table_name}_{table_alias_number}\"].append(line.strip())\n",
    "                    else:\n",
    "                        current_instance_conditions[table_name].append(line.strip())\n",
    "        \n",
    "        # for each table\n",
    "        for table_name, conditions in current_instance_conditions.items():\n",
    "            instance_str = '[\"' + '\", \"'.join(conditions) + '\"]'\n",
    "            table_conditions[table_name].append(instance_str)\n",
    "    \n",
    "    # output\n",
    "    output_dir = f\"0_mixture_test/{query_id}/PQO\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for table_name, instance_arrays in table_conditions.items():\n",
    "        output_file = f\"{output_dir}/{query_id}_{mode}_{table_name}.txt\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(',\\n'.join(instance_arrays))\n",
    "\n",
    "\n",
    "def main():\n",
    "    query_ids = ['025', '050', '072', '085', '100']\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        try:\n",
    "            process_predicates(query_id)\n",
    "            print(f\"Successfully processed query {query_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {query_id}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate original template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query ID: 072\n",
      "Successfully generated original_template/072.json\n",
      "\n",
      "Processing query ID: 084\n",
      "Successfully generated original_template/084.json\n",
      "\n",
      "Processing query ID: 085\n",
      "Successfully generated original_template/085.json\n",
      "\n",
      "Processing query ID: 091\n",
      "Successfully generated original_template/091.json\n",
      "\n",
      "Processing query ID: 099\n",
      "Successfully generated original_template/099.json\n",
      "\n",
      "Processing query ID: 100\n",
      "Successfully generated original_template/100.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def read_file_content(file_path):\n",
    "    \"\"\"Read and return file content.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_predicate_line(line):\n",
    "    \"\"\"Parse a single predicate line into the required format.\"\"\"\n",
    "    # Initialize the predicate structure\n",
    "    predicate = {\n",
    "        \"alias\": \"\",\n",
    "        \"column\": \"\",\n",
    "        \"operator\": \"=\",\n",
    "        \"data_type\": \"text\",\n",
    "        \"table\": \"\",\n",
    "        \"join_tables\": [\"\"],\n",
    "        \"join_tables_alias\": [\"\"],\n",
    "        \"join_tables_column\": [[\"\"]],\n",
    "        \"join_conditions\": [\"\"],\n",
    "        \"left_or_right\": [\"\"]\n",
    "    }\n",
    "    \n",
    "    return predicate\n",
    "\n",
    "def generate_json_template(query_id):\n",
    "    \"\"\"Generate JSON template for the given query ID.\"\"\"\n",
    "    # Define file paths\n",
    "    query_file_path = f\"original_tpl/query{query_id}_spj.tpl\"\n",
    "    predicate_file_path = f\"original_tpl/query_condition_{query_id}.tpl\"\n",
    "    output_path = f\"original_template/{query_id}.json\"\n",
    "    \n",
    "    # Read query content\n",
    "    query_content = read_file_content(query_file_path)\n",
    "    if query_content is None:\n",
    "        return False\n",
    "    \n",
    "    # Read and parse predicates\n",
    "    try:\n",
    "        with open(predicate_file_path, 'r', encoding='utf-8') as f:\n",
    "            predicate_lines = f.readlines()\n",
    "            predicates = [parse_predicate_line(line.strip()) \n",
    "                         for line in predicate_lines \n",
    "                         if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Predicate file {predicate_file_path} not found.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading predicate file: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(\"original_template\", exist_ok=True)\n",
    "    \n",
    "    # Create the final JSON structure\n",
    "    template = {\n",
    "        query_id: {\n",
    "                \"query\": query_content,\n",
    "                \"predicates\": predicates\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Write the JSON file\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(template, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Successfully generated {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing JSON file: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process multiple query IDs.\"\"\"\n",
    "    # You can modify this to process specific query IDs or read from a configuration\n",
    "    query_ids = ['072', '084', '085', '091', '099', '100']\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        print(f\"\\nProcessing query ID: {query_id}\")\n",
    "        generate_json_template(query_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate mixture template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['040', '027', '018', '084', '025', '091', '072', '050', '085', '013', '099', '019', '100']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def process_files(query_id):\n",
    "    # Set up base path and each component's paths\n",
    "    base_path = f\"dsb_{query_id}_original\"\n",
    "    \n",
    "    # Each component has its own directory with a testing file\n",
    "    card_path = Path(base_path) / \"cardinality\"\n",
    "    gauss_path = Path(base_path) / \"gaussian\"\n",
    "    \n",
    "    # Testing file path is relative to each component directory\n",
    "    test_path = f\"inputs/testing/{query_id}_testing_original.json\"\n",
    "    \n",
    "    # Read data from cardinality + its testing file\n",
    "    with open(card_path / test_path) as f:\n",
    "        card_test_data = json.load(f)\n",
    "    \n",
    "    # Read data from gauss + its testing file\n",
    "    with open(gauss_path / test_path) as f:\n",
    "        gauss_test_data = json.load(f)\n",
    "\n",
    "    # Verify consistency of query and predicates across all datasets\n",
    "    if not verify_consistency([card_test_data, gauss_test_data], query_id):\n",
    "        raise ValueError(f\"Inconsistent query or predicates for {query_id}\")\n",
    "\n",
    "    # Merge params from all sources\n",
    "    card_params =  card_test_data[query_id]['params']\n",
    "    gauss_params = gauss_test_data[query_id]['params']\n",
    "    \n",
    "    # Shuffle and sample 200 sets of params (600 total)\n",
    "    random.seed(2024)\n",
    "    random.shuffle(card_params)\n",
    "    random.shuffle(gauss_params)\n",
    "    sampled_params = card_params[:100] + gauss_params[:100]\n",
    "    random.shuffle(sampled_params)\n",
    "    \n",
    "    # Build result in required format\n",
    "    result = {\n",
    "        query_id: {\n",
    "            \"query\": card_test_data[query_id]['query'],  # They should all be the same at this point\n",
    "            \"predicates\": card_test_data[query_id]['predicates'],\n",
    "            \"params\": sampled_params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def verify_consistency(data_list, query_id):\n",
    "    \"\"\"\n",
    "    Verifies that query and predicates are consistent, but only checks predicates\n",
    "    between the first two datasets (cardinality and gaussian).\n",
    "    \n",
    "    Args:\n",
    "        data_list: List containing [card_test_data, csv_test_data, kepler_test_data]\n",
    "        query_id: The query ID being processed\n",
    "    Returns:\n",
    "        True if consistent according to our rules, False otherwise\n",
    "    \"\"\"\n",
    "    if len(data_list) < 2:  # We need all two datasets\n",
    "        return False\n",
    "    \n",
    "    # Unpack for clarity\n",
    "    card_data, gauss_data = data_list[0], data_list[1]\n",
    "    \n",
    "    # Check query consistency across all three datasets\n",
    "    query_consistent = (\n",
    "        card_data[query_id]['query'] == gauss_data[query_id]['query']\n",
    "    )\n",
    "\n",
    "    # Check predicates consistency with detailed field comparison\n",
    "    card_predicates = card_data[query_id]['predicates']\n",
    "    gauss_predicates = gauss_data[query_id]['predicates']\n",
    "\n",
    "    # Ensure both lists have the same length\n",
    "    if len(card_predicates) == len(gauss_predicates):\n",
    "        predicates_consistent = all(\n",
    "            card_predicate['alias'] == gauss_predicate['alias'] and\n",
    "            card_predicate['column'] == gauss_predicate['column'] and\n",
    "            card_predicate['operator'] == gauss_predicate['operator'] and\n",
    "            card_predicate['data_type'] == gauss_predicate['data_type'] and\n",
    "            card_predicate['table'] == gauss_predicate['table'] and\n",
    "            card_predicate['join_tables'] == gauss_predicate['join_tables'] and\n",
    "            card_predicate['join_tables_alias'] == gauss_predicate['join_tables_alias'] and\n",
    "            card_predicate['join_tables_column'] == gauss_predicate['join_tables_column'] and\n",
    "            card_predicate['join_conditions'] == gauss_predicate['join_conditions'] and\n",
    "            card_predicate['left_or_right'] == gauss_predicate['left_or_right']\n",
    "            for card_predicate, gauss_predicate in zip(card_predicates, gauss_predicates)\n",
    "        )\n",
    "    else:\n",
    "        predicates_consistent = False\n",
    "\n",
    "    \n",
    "    return query_consistent and predicates_consistent\n",
    "\n",
    "\n",
    "def process_all_queries():\n",
    "    # Get all query IDs from directory names\n",
    "    pattern = \"dsb_*_original\"\n",
    "    all_dirs = glob.glob(pattern)\n",
    "    query_ids = [d.split('_')[1] for d in all_dirs]\n",
    "    print(query_ids)\n",
    "    \n",
    "    # Process each query and save its result separately\n",
    "    for qid in query_ids:\n",
    "        # Get the result for this query\n",
    "        result = process_files(qid)\n",
    "        \n",
    "        # Create filename for this query's result\n",
    "        output_dir = Path(f\"0_mixture_test/{qid}\")\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_filename = output_dir / f\"{qid}_mixture_test.json\"\n",
    "        \n",
    "        # Save this query's result to its own file\n",
    "        with open(output_filename, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "# Execute the processing\n",
    "# process_all_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kepler pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing paths for Query ID 100:\n",
      "\n",
      "  Method: gaussian\n",
      "  ✗ /home/lsh/test_kepler/kepler/dsb_cardrange_workload/dsb_100_original/gaussian/outputs/results/100/training_50/metadata.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def check_paths(query_ids: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Check if specific paths exist for given query IDs for both cardinality and gaussian methods.\n",
    "    \n",
    "    Args:\n",
    "        query_ids: List of query IDs to check\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results for each query ID and method\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    results = {}\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        results[query_id] = {}\n",
    "        \n",
    "        for method in ['cardinality', 'gaussian']:\n",
    "            result = {\n",
    "                \"original_folder_exists\": False,\n",
    "                \"pqo_exists\": False,\n",
    "                \"candidate_metadata_exists\": False,\n",
    "                \"results_metadata_exists\": False,\n",
    "                \"all_paths_exist\": False,\n",
    "                \"missing_paths\": []\n",
    "            }\n",
    "            \n",
    "            # Construct base folder path\n",
    "            original_folder = f\"dsb_{query_id}_original\"\n",
    "            \n",
    "            # Construct method-specific paths\n",
    "            if method == 'cardinality':\n",
    "                pqo_path = os.path.join(\n",
    "                    current_dir,\n",
    "                    original_folder,\n",
    "                    method,\n",
    "                    \"inputs\",\n",
    "                    \"PQO\",\n",
    "                    \"join_predicates\"\n",
    "                )\n",
    "            else:  # gaussian\n",
    "                pqo_path = os.path.join(\n",
    "                    current_dir,\n",
    "                    original_folder,\n",
    "                    method,\n",
    "                    \"inputs\",\n",
    "                    \"PQO\"\n",
    "                )\n",
    "            \n",
    "            candidate_metadata_path = os.path.join(\n",
    "                current_dir,\n",
    "                original_folder,\n",
    "                method,\n",
    "                \"outputs\",\n",
    "                \"hints\",\n",
    "                str(query_id),\n",
    "                \"training_50\",\n",
    "                \"candidate_metadata.json\"\n",
    "            )\n",
    "            \n",
    "            results_metadata_path = os.path.join(\n",
    "                current_dir,\n",
    "                original_folder,\n",
    "                method,\n",
    "                \"outputs\",\n",
    "                \"results\",\n",
    "                str(query_id),\n",
    "                \"training_50\",\n",
    "                \"metadata.json\"\n",
    "            )\n",
    "            \n",
    "            # Check existence and collect missing paths\n",
    "            paths_to_check = {\n",
    "                \"original_folder\": (original_folder, \"original_folder_exists\"),\n",
    "                \"pqo\": (pqo_path, \"pqo_exists\"),\n",
    "                \"candidate_metadata\": (candidate_metadata_path, \"candidate_metadata_exists\"),\n",
    "                \"results_metadata\": (results_metadata_path, \"results_metadata_exists\")\n",
    "            }\n",
    "            \n",
    "            for path_name, (path, result_key) in paths_to_check.items():\n",
    "                exists = os.path.exists(path)\n",
    "                result[result_key] = exists\n",
    "                if not exists:\n",
    "                    result[\"missing_paths\"].append(path)\n",
    "            \n",
    "            # Check if all paths exist\n",
    "            result[\"all_paths_exist\"] = all([\n",
    "                result[\"original_folder_exists\"],\n",
    "                result[\"pqo_exists\"],\n",
    "                result[\"candidate_metadata_exists\"],\n",
    "                result[\"results_metadata_exists\"]\n",
    "            ])\n",
    "            \n",
    "            results[query_id][method] = result\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_missing_paths(results: dict):\n",
    "    \"\"\"\n",
    "    Print only the missing paths for each query ID and method.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing check results for each query ID and method\n",
    "    \"\"\"\n",
    "    has_missing_paths = False\n",
    "    \n",
    "    for query_id, methods in results.items():\n",
    "        missing_in_query = False\n",
    "        method_results = []\n",
    "        \n",
    "        for method, result in methods.items():\n",
    "            if not result[\"all_paths_exist\"]:\n",
    "                has_missing_paths = True\n",
    "                missing_in_query = True\n",
    "                method_results.append((method, result[\"missing_paths\"]))\n",
    "        \n",
    "        if missing_in_query:\n",
    "            print(f\"\\nMissing paths for Query ID {query_id}:\")\n",
    "            for method, missing_paths in method_results:\n",
    "                print(f\"\\n  Method: {method}\")\n",
    "                for path in missing_paths:\n",
    "                    print(f\"  ✗ {path}\")\n",
    "    \n",
    "    if not has_missing_paths:\n",
    "        print(\"\\nAll paths exist for all queries and methods.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the path checker.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_ids = ['013', '018', '019',\n",
    "                    '025', '027', '040',\n",
    "                    '050', '072', '084', '085',\n",
    "                    '091', '099', '100']\n",
    "        \n",
    "        # Check paths\n",
    "        results = check_paths(query_ids)\n",
    "        \n",
    "        # Print only missing paths\n",
    "        print_missing_paths(results)\n",
    "        \n",
    "    except ValueError:\n",
    "        print(\"Error: Please enter valid query IDs\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate kepler format's gaussian workload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing query_id: 013\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [6]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [6]\n",
      "\n",
      "Analyzing query_id: 018\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [6]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [6]\n",
      "\n",
      "Analyzing query_id: 019\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [5]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [5]\n",
      "\n",
      "Analyzing query_id: 025\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [3]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [3]\n",
      "\n",
      "Analyzing query_id: 027\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [4]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [4]\n",
      "\n",
      "Analyzing query_id: 040\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [4]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [4]\n",
      "\n",
      "Analyzing query_id: 050\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [3]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [3]\n",
      "\n",
      "Analyzing query_id: 072\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [5]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [5]\n",
      "\n",
      "Analyzing query_id: 084\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [2]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [2]\n",
      "\n",
      "Analyzing query_id: 085\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [7]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [7]\n",
      "\n",
      "Analyzing query_id: 091\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [3]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [3]\n",
      "\n",
      "Analyzing query_id: 099\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [5]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [5]\n",
      "\n",
      "Analyzing query_id: 100\n",
      "Train list length: 50 - Test list length: 200\n",
      "Train inner list lengths: - All equal: True\n",
      "Length(s): [6]\n",
      "Test inner list lengths: - All equal: True\n",
      "Length(s): [6]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def analyze_json_files(query_id):\n",
    "    with open('PQO_gaussian_sql.json', 'r') as f:\n",
    "        gaussian_data = json.load(f)\n",
    "    \n",
    "    query_data = gaussian_data[query_id]\n",
    "\n",
    "    # train & test param combination length\n",
    "    train_length = len(query_data['train'])\n",
    "    test_length = len(query_data['test'])\n",
    "    print(f\"\\nAnalyzing query_id: {query_id}\")\n",
    "    print(f\"Train list length: {train_length} - Test list length: {test_length}\")\n",
    "    \n",
    "    # train & test param length\n",
    "    train_inner_lengths = [len(x) for x in query_data['train']]\n",
    "    test_inner_lengths = [len(x) for x in query_data['test']]\n",
    "    print(f\"Train inner list lengths: - All equal: {len(set(train_inner_lengths)) == 1}\")\n",
    "    print(f\"Length(s): {list(set(train_inner_lengths))}\")\n",
    "    print(f\"Test inner list lengths: - All equal: {len(set(test_inner_lengths)) == 1}\")\n",
    "    print(f\"Length(s): {list(set(test_inner_lengths))}\")\n",
    "\n",
    "for query_id in ['013', '018', '019', '025', '027', '040',\n",
    "                 '050', '072', '084', '085', '091', '099', '100']:\n",
    "    analyze_json_files(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "013 9\n",
      "018 8\n",
      "019 7\n",
      "025 6\n",
      "027 6\n",
      "040 7\n",
      "050 3\n",
      "072 8\n",
      "084 2\n",
      "085 10\n",
      "091 4\n",
      "099 6\n",
      "100 10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def process_element(element, param_info):\n",
    "    idx, *split_params = param_info\n",
    "    \n",
    "    # find target sql stmt\n",
    "    target_sql = element[idx]\n",
    "    \n",
    "    if len(split_params) == 1:\n",
    "        # str1\n",
    "        str1 = split_params[0]\n",
    "        return target_sql.split(str1)[1]\n",
    "    \n",
    "    elif len(split_params) == 2:\n",
    "        # str1, str2\n",
    "        str1, str2 = split_params\n",
    "        return target_sql.split(str1)[1].split(str2)[0]\n",
    "    \n",
    "    elif len(split_params) == 3:\n",
    "        # str1, str2, str3\n",
    "        str1, str2, str3 = split_params\n",
    "        if str3 == \"TODO: pick last\":\n",
    "            return target_sql.split(str1)[1].split(str2)[1]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported str3 value: {str3}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of split parameters: {len(split_params)}\")\n",
    "\n",
    "def process_data(split_data_dict, raw_data):\n",
    "    result = {}\n",
    "    \n",
    "    # all query_id\n",
    "    for query_id, param_dict in split_data_dict.items():\n",
    "        if query_id not in raw_data:\n",
    "            continue\n",
    "            \n",
    "        result[query_id] = {\"train\": [], \"test\": []}\n",
    "        \n",
    "        # get number of params\n",
    "        num_params = len(param_dict)\n",
    "        print(query_id, num_params)\n",
    "        \n",
    "        # train\n",
    "        for element in raw_data[query_id][\"train\"]:\n",
    "            processed = []\n",
    "            for i in range(num_params):\n",
    "                param_key = f\"@param{i}\"\n",
    "                param_info = param_dict[param_key]\n",
    "                processed.append(process_element(element, param_info))\n",
    "            result[query_id][\"train\"].append(processed)\n",
    "            \n",
    "        # test\n",
    "        for element in raw_data[query_id][\"test\"]:\n",
    "            processed = []\n",
    "            for i in range(num_params):\n",
    "                param_key = f\"@param{i}\"\n",
    "                param_info = param_dict[param_key]\n",
    "                processed.append(process_element(element, param_info))\n",
    "            result[query_id][\"test\"].append(processed)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    with open('PQO_gaussian_split_dict.json', 'r') as f:\n",
    "        split_data_dict = json.load(f)\n",
    "    \n",
    "    with open('PQO_gaussian_sql.json', 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    result = process_data(split_data_dict, raw_data)\n",
    "    \n",
    "    with open('PQO_gaussian_params.json', 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "013 9\n",
      "018 8\n",
      "019 7\n",
      "025 6\n",
      "027 6\n",
      "040 7\n",
      "050 3\n",
      "072 8\n",
      "084 2\n",
      "085 10\n",
      "091 4\n",
      "099 6\n",
      "100 10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def main():\n",
    "    template_dir = Path('original_template')\n",
    "    \n",
    "    json_files = sorted([f for f in os.listdir(template_dir) if f.endswith('.json')])\n",
    "    \n",
    "    for filename in json_files:\n",
    "        query_id = filename[:-5]\n",
    "        \n",
    "        with open(template_dir / filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        predicates_length = len(data[query_id][\"predicates\"])\n",
    "        print(f\"{query_id} {predicates_length}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['D', '4 yr Degree', 'D', '4 yr Degree', 'D', '4 yr Degree', \"MN', 'NC', 'TX\", \"GA', 'NE', 'SC\", \"CA', 'CT', 'NY\"], ['D', 'Advanced Degree', 'S', 'Advanced Degree', 'D', 'College', \"IA', 'ND', 'TX\", \"MT', 'TN', 'VA\", \"AR', 'IN', 'TX\"]]\n",
      "[['M', '4 yr Degree', 'D', '4 yr Degree', 'D', '4 yr Degree', \"MI', 'SD', 'TN\", \"MI', 'ND', 'TX\", \"MI', 'MN', 'WY\"], ['W', 'Advanced Degree', 'S', 'Advanced Degree', 'W', '4 yr Degree', \"MS', 'TN', 'VA\", \"CA', 'FL', 'ND\", \"KY', 'MN', 'VT\"]]\n",
      "original (train size: 50): training 50, testing 200\n",
      "distinct (train size: 50): training 50, testing 200\n",
      "Metadata saved to dsb_013_original/gaussian/inputs/metadata/013.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "\n",
    "# meteadata\n",
    "def save_metadata_to_csv(output_dir, metadata, query_id):\n",
    "    \"\"\"\n",
    "    Saves the metadata to a CSV file in the output directory.\n",
    "    \n",
    "    Args:\n",
    "    - output_dir: The directory where the metadata.csv will be saved.\n",
    "    - metadata: A dictionary containing the metadata information to save.\n",
    "    \"\"\"\n",
    "    metadata_file = os.path.join(output_dir, f\"{query_id}.csv\")\n",
    "    \n",
    "    # Write the metadata to CSV\n",
    "    with open(metadata_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write headers\n",
    "        writer.writerow([\"Key\", \"Value\"])\n",
    "        \n",
    "        # Write metadata\n",
    "        for key, value in metadata.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "    print(f\"Metadata saved to {metadata_file}\")\n",
    "\n",
    "\n",
    "# frequency\n",
    "def get_literal_frequencies(literals):\n",
    "    \"\"\"\n",
    "    calculate param with frequency\n",
    "    \"\"\"\n",
    "    frequency_dict = collections.defaultdict(int)\n",
    "    \n",
    "    for literal in literals:\n",
    "        frequency_dict[json.dumps(literal)] += 1\n",
    "    \n",
    "    return frequency_dict\n",
    "\n",
    "\n",
    "def split_literals_and_store_with_frequency(query_id, train_literals, test_literals, output_dir, train_size=50, test_size=200):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # get distinct train & test literals\n",
    "    distinct_train_literals = list(set(map(tuple, train_literals)))\n",
    "    distinct_train_literals = [list(item) for item in distinct_train_literals]\n",
    "    \n",
    "    distinct_test_literals = list(set(map(tuple, test_literals)))\n",
    "    distinct_test_literals = [list(item) for item in distinct_test_literals]\n",
    "    \n",
    "    # calculate frequency\n",
    "    train_literal_frequencies = get_literal_frequencies(train_literals)\n",
    "    test_literal_frequencies = get_literal_frequencies(test_literals)\n",
    "    \n",
    "    # Store all train_dicts in a dictionary, keyed by train_size\n",
    "    train_dict_dict = {}\n",
    "    full_train_dict = {json.dumps(literal): train_literal_frequencies[json.dumps(literal)] for literal in distinct_train_literals}\n",
    "    train_dict_dict[len(train_literals)] = full_train_dict\n",
    "    test_dict = {json.dumps(literal): test_literal_frequencies[json.dumps(literal)] for literal in distinct_test_literals}\n",
    "    \n",
    "    # Get the subset of train_literals based on the current train_size\n",
    "    train_subset = train_literals\n",
    "    new_train_literal_freq = get_literal_frequencies(train_subset)\n",
    "\n",
    "    # Get distinct train literals for this subset\n",
    "    distinct_train_subset = list(set(map(tuple, train_subset)))\n",
    "    distinct_train_subset = [list(item) for item in distinct_train_subset]\n",
    "    \n",
    "    # Create a frequency dictionary for this subset\n",
    "    train_dict = {json.dumps(literal): new_train_literal_freq[json.dumps(literal)] for literal in distinct_train_subset}\n",
    "    \n",
    "    # Add the current train size dictionary to the list of dictionaries\n",
    "    train_dict_dict[train_size] = train_dict\n",
    "    \n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    base_dir = \"frequency\"\n",
    "    output_dir_path = os.path.join(output_dir, base_dir)\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    # Save each train literal dict with frequency based on train_size_list, including full size\n",
    "    for train_size, train_dict in train_dict_dict.items():\n",
    "        train_output_file = os.path.join(output_dir_path, f\"{query_id}_train_{train_size}_freq.json\")\n",
    "        with open(train_output_file, 'w') as train_file:\n",
    "            json.dump(train_dict, train_file, indent=4)\n",
    "\n",
    "    # Save test literals with frequency\n",
    "    test_output_file = os.path.join(output_dir_path, f\"{query_id}_test_freq.json\")\n",
    "    with open(test_output_file, 'w') as test_file:\n",
    "        json.dump(test_dict, test_file, indent=4)\n",
    "    \n",
    "    \n",
    "    return train_dict_dict, test_dict\n",
    "\n",
    "\n",
    "###################\n",
    "# Ourpur directories\n",
    "def prepare_directories(output_dir):\n",
    "    \"\"\"\n",
    "    Prepares the directory structure for storing the templates and PQO files.\n",
    "\n",
    "    Args:\n",
    "    output_dir (str): The base directory for saving files.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with the paths to various directories.\n",
    "    \"\"\"\n",
    "    dirs = {\n",
    "        'training': os.path.join(output_dir, 'training'),\n",
    "        'testing': os.path.join(output_dir, 'testing'),\n",
    "        'metadata': os.path.join(output_dir, 'metadata')\n",
    "    }\n",
    "\n",
    "    # Create the directories if they don't exist\n",
    "    for dir_path in dirs.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    return dirs\n",
    "   \n",
    "    \n",
    "def load_template(template_file, query_id):\n",
    "    \"\"\"\n",
    "    Load the query and predicates from a JSON template file.\n",
    "    \n",
    "    Args:\n",
    "    template_file (str): Path to the JSON file containing the query templates.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: query_id, query, and predicates from the first template.\n",
    "    \"\"\"\n",
    "    # Load the JSON template file\n",
    "    with open(template_file, 'r') as file:\n",
    "        templates = json.load(file)\n",
    "    \n",
    "    # Extract the corresponding query template\n",
    "    template = templates[query_id]\n",
    "    query = template['query']\n",
    "    predicates = template['predicates']\n",
    "    \n",
    "    # Check if 'params' exists in the template, if not set it to None\n",
    "    original_param_list = template.get('params', None)\n",
    "    \n",
    "    return query, predicates, original_param_list\n",
    "\n",
    "\n",
    "def save_templates_and_pqo(query_id, query, predicates, training_params, testing_params, test_literals, dirs, train_size, mode=\"original\"):\n",
    "    \"\"\"\n",
    "    Save the full, training, and testing templates, and corresponding PQO files.\n",
    "\n",
    "    Args:\n",
    "    query_id (str): The ID of the query.\n",
    "    query (str): The query string.\n",
    "    predicates (list): List of predicates.\n",
    "    training_params (list): List of training parameters.\n",
    "    testing_params (list): List of testing parameters.\n",
    "    dirs (dict): A dictionary containing the paths to the directories for saving the files.\n",
    "    \"\"\"\n",
    "    # Create training, and testing template dictionaries\n",
    "    training_template = {\n",
    "        query_id: {\n",
    "            \"query\": query,\n",
    "            \"predicates\": predicates,\n",
    "            \"params\": training_params\n",
    "        }\n",
    "    }\n",
    "\n",
    "    testing_template = {\n",
    "        query_id: {\n",
    "            \"query\": query,\n",
    "            \"predicates\": predicates,\n",
    "            \"params\": test_literals if mode == \"original\" else testing_params # from PQO - same order of the params\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save training template\n",
    "    training_output_file = os.path.join(dirs['training'], f\"{query_id}_training_{mode}_{train_size}.json\")\n",
    "    with open(training_output_file, 'w') as out_file:\n",
    "        json.dump(training_template, out_file, indent=4)\n",
    "\n",
    "    # Save testing template\n",
    "    testing_output_file = os.path.join(dirs['testing'], f\"{query_id}_testing_{mode}.json\")\n",
    "    if os.path.exists(testing_output_file):\n",
    "        print(f\"{query_id}_testing_{mode}.json already exists, skipping save.\")\n",
    "    else:\n",
    "        with open(testing_output_file, 'w') as out_file:\n",
    "            json.dump(testing_template, out_file, indent=4)\n",
    "\n",
    "    \n",
    "# generate training & testing set\n",
    "def generate_param_list_from_dict(data_dict, mode):\n",
    "    param_list = []\n",
    "    if mode == 'distinct':\n",
    "        # In 'distinct' mode, use keys (unique literals) only once\n",
    "        param_list = [eval(key) for key in data_dict.keys()]\n",
    "    elif mode == 'original':\n",
    "        # In 'original' mode, replicate keys by their frequencies\n",
    "        for key, freq in data_dict.items():\n",
    "            param_list.extend([eval(key)] * freq)\n",
    "    return param_list\n",
    "\n",
    "\n",
    "# generation based on selection\n",
    "def process_train_and_test_data(output_dir, query_id, query, predicates, train_dict_dict, test_dict, test_literals):\n",
    "    # Initialize a metadata dictionary to store relevant information\n",
    "    metadata = {}\n",
    "\n",
    "    # Generate params for training and testing\n",
    "    for mode in [\"original\", \"distinct\"]:\n",
    "        for train_size in sorted(train_dict_dict.keys()):  # train_size_list + full size\n",
    "            training_params = generate_param_list_from_dict(train_dict_dict[train_size], mode)\n",
    "            testing_params = generate_param_list_from_dict(test_dict, mode)\n",
    "            print(f\"{mode} (train size: {train_size}): training {len(training_params)}, testing {len(testing_params)}\")\n",
    "\n",
    "            # Save metadata for each mode\n",
    "            metadata[f\"{mode}_testing_params\"] = len(testing_params)\n",
    "            metadata[f\"{mode}_{train_size}_training_params\"] = len(training_params)\n",
    "\n",
    "            # Save templates and PQO for each train size\n",
    "            save_templates_and_pqo(query_id, query, predicates, training_params, testing_params, test_literals, output_dir, train_size, mode)\n",
    "\n",
    "    # Save metadata\n",
    "    save_metadata_to_csv(output_dir['metadata'], metadata, query_id)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialize\n",
    "    # for query_id in ['013', '018', '019', '025', '027', '040',\n",
    "    #              '050', '072', '084', '085', '091', '099', '100']:\n",
    "    for query_id in ['013']:\n",
    "        output_dir = f\"dsb_{query_id}_original\"\n",
    "        base_dir = \"gaussian/inputs\"\n",
    "        full_output_dir = os.path.join(output_dir, base_dir)\n",
    "        template_file = f\"original_template/{query_id}.json\"\n",
    "        \n",
    "        # preparation\n",
    "        dirs = prepare_directories(full_output_dir)\n",
    "        query, predicates, _ = load_template(template_file, query_id)\n",
    "        \n",
    "        # params\n",
    "        with open('PQO_gaussian_params.json', 'r') as f:\n",
    "            param_file = json.load(f)\n",
    "        train_literals = param_file[query_id][\"train\"]\n",
    "        print(train_literals[:2])\n",
    "        test_literals = param_file[query_id][\"test\"]\n",
    "        print(test_literals[:2])\n",
    "        \n",
    "        train_dict_dict, test_dict = split_literals_and_store_with_frequency(query_id, train_literals, test_literals, full_output_dir)\n",
    "        process_train_and_test_data(dirs, query_id, query, predicates, train_dict_dict, test_dict, test_literals)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n",
      "\n",
      "Validating testing files...\n",
      "Validation successful for testing files!\n",
      "\n",
      "Validating training files...\n",
      "Validation successful for training files!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def convert_list_to_tuple(lst):\n",
    "    return tuple(convert_list_to_tuple(x) if isinstance(x, list) else x for x in lst)\n",
    "\n",
    "def compare_lists_exactly(list1, list2):\n",
    "    list1 = sorted([tuple(ele) for ele in list1])\n",
    "    list2 = sorted([tuple(ele) for ele in list2])\n",
    "    return list1 == list2\n",
    "\n",
    "def validate_files(query_id, mode='testing'):\n",
    "    # initialize\n",
    "    base_path = f'dsb_{query_id}_original/gaussian/inputs/{mode}'\n",
    "    distinct_file = f'{base_path}/{query_id}_{mode}_distinct{\"_50\" if mode==\"training\" else \"\"}.json'\n",
    "    original_file = f'{base_path}/{query_id}_{mode}_original{\"_50\" if mode==\"training\" else \"\"}.json'\n",
    "    pqo_file = 'PQO_gaussian_params.json'\n",
    "    \n",
    "    # load files\n",
    "    distinct_data = load_json_file(distinct_file)\n",
    "    original_data = load_json_file(original_file)\n",
    "    pqo_data = load_json_file(pqo_file)\n",
    "    \n",
    "    if not all([distinct_data, original_data, pqo_data]):\n",
    "        print(\"Error: Some files could not be loaded\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        pqo_params = pqo_data[query_id][mode[:-3]]\n",
    "        original_params = original_data[query_id][\"params\"]\n",
    "            \n",
    "        # sorted, check the same\n",
    "        if not compare_lists_exactly(original_params, pqo_params):\n",
    "            print(f\"Error: {mode} original params and PQO params don't match exactly\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"Validation successful for {mode} files!\")\n",
    "        return True\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Required key not found in JSON structure: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    for query_id in ['013', '018', '019', '025', '027', '040',\n",
    "                 '050', '072', '084', '085', '091', '099', '100']:\n",
    "    \n",
    "        # validate testing files\n",
    "        print(\"\\nValidating testing files...\")\n",
    "        validate_files(query_id, 'testing')\n",
    "        \n",
    "        # validate training files\n",
    "        print(\"\\nValidating training files...\")\n",
    "        validate_files(query_id, 'training')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate original plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 combinations...\n",
      "\n",
      "Processing query_id=091, training_size=50, confidence_threshold=0\n",
      "Generated: 0_dsb_original_plans_0/091_workload_cardinality_training_size_50.csv\n",
      "Generated: 0_dsb_original_plans_0/091_workload_gaussian_training_size_50.csv\n",
      "\n",
      "Processing query_id=099, training_size=50, confidence_threshold=0\n",
      "Generated: 0_dsb_original_plans_0/099_workload_cardinality_training_size_50.csv\n",
      "Generated: 0_dsb_original_plans_0/099_workload_gaussian_training_size_50.csv\n",
      "\n",
      "Processing query_id=100, training_size=50, confidence_threshold=0\n",
      "Generated: 0_dsb_original_plans_0/100_workload_cardinality_training_size_50.csv\n",
      "Generated: 0_dsb_original_plans_0/100_workload_gaussian_training_size_50.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "def process_data(query_id, training_size, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Process data for a given query_id and training_size combination.\n",
    "    \n",
    "    Args:\n",
    "        query_id (str): The ID of the query to process\n",
    "        training_size (int): The training size to process\n",
    "    \"\"\"\n",
    "    # Define the methods to process - now using the new set of methods\n",
    "    methods = ['cardinality', 'gaussian']\n",
    "    \n",
    "    for method in methods:\n",
    "        # Form the predictions file path using the new structure\n",
    "        predictions_path = f'dsb_{query_id}_original/{method}/outputs/evaluation/{query_id}/training_{training_size}/confidence_{confidence_threshold}/predictions/'\n",
    "        \n",
    "        # Find the file that matches the pattern starting with query_id and ending with _batch_0.csv\n",
    "        predictions_file = None\n",
    "        try:\n",
    "            for filename in os.listdir(predictions_path):\n",
    "                if filename.startswith(f'{query_id}_') and filename.endswith('_batch_0.csv'):\n",
    "                    predictions_file = os.path.join(predictions_path, filename)\n",
    "                    break\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Directory not found for {method}, query_id={query_id}, training_size={training_size}\")\n",
    "            continue\n",
    "                \n",
    "        if predictions_file is None:\n",
    "            print(f\"No prediction file found for {method}, query_id={query_id}, training_size={training_size}\")\n",
    "            continue\n",
    "        \n",
    "        # Read and process the predictions file\n",
    "        predictions_df = pd.read_csv(predictions_file)\n",
    "        \n",
    "        # Read the testing data JSON\n",
    "        testing_json_path = f'dsb_{query_id}_original/{method}/inputs/testing/{query_id}_testing_original.json'\n",
    "        try:\n",
    "            with open(testing_json_path, 'r') as f:\n",
    "                query_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Testing JSON not found for query_id={query_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Process each prediction row\n",
    "        results = []\n",
    "        \n",
    "        for _, pred_row in predictions_df.iterrows():\n",
    "            params = pred_row['params']\n",
    "            plan_id = pred_row['plan_id']\n",
    "            plan_content = pred_row['plan_content']\n",
    "            \n",
    "            # Convert params from string representation to list\n",
    "            params_list = eval(params)\n",
    "            \n",
    "            # Get full query instance\n",
    "            query = query_data[query_id]['query']\n",
    "            \n",
    "            # Replace parameters in query with actual values\n",
    "            for i, param in enumerate(params_list):\n",
    "                param = str(param).strip()\n",
    "                pattern = re.compile(rf\"@param{i}\\b\")\n",
    "                query = pattern.sub(param, query)\n",
    "            \n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'plan_id': plan_id,\n",
    "                'plan_content': plan_content\n",
    "            })\n",
    "        \n",
    "        # Save results to CSV\n",
    "        output_df = pd.DataFrame(results)\n",
    "        os.makedirs(f'0_dsb_original_plans_{confidence_threshold}', exist_ok=True)\n",
    "        output_filename = f'0_dsb_original_plans_{confidence_threshold}/{query_id}_workload_{method}_training_size_{training_size}.csv'\n",
    "        output_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Generated: {output_filename}\")\n",
    "\n",
    "# Define separate lists for query_ids and training sizes\n",
    "query_ids = ['091', '099', '100']\n",
    "# '013', '018', '019', '025', '027', '040', '050', '072', '084', '085', '091', '099', '100'\n",
    "training_sizes = [50]\n",
    "confidence_thresholds = [\"0\"]\n",
    "\n",
    "# Generate all combinations using itertools.product\n",
    "combinations = list(product(query_ids, training_sizes, confidence_thresholds))\n",
    "\n",
    "# Display the total number of combinations to be processed\n",
    "print(f\"Processing {len(combinations)} combinations...\")\n",
    "\n",
    "# Process each combination\n",
    "for query_id, training_size, confidence_threshold in combinations:\n",
    "    print(f\"\\nProcessing query_id={query_id}, training_size={training_size}, confidence_threshold={confidence_threshold}\")\n",
    "    process_data(query_id, training_size, confidence_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate mixture plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3 combinations...\n",
      "\n",
      "Processing query_id=091, training_size=50\n",
      "Generated: 0_mixture_plans/091_workload_cardinality_training_size_50.csv\n",
      "Generated: 0_mixture_plans/091_workload_gaussian_training_size_50.csv\n",
      "\n",
      "Processing query_id=099, training_size=50\n",
      "Generated: 0_mixture_plans/099_workload_cardinality_training_size_50.csv\n",
      "Generated: 0_mixture_plans/099_workload_gaussian_training_size_50.csv\n",
      "\n",
      "Processing query_id=100, training_size=50\n",
      "Generated: 0_mixture_plans/100_workload_cardinality_training_size_50.csv\n",
      "Generated: 0_mixture_plans/100_workload_gaussian_training_size_50.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "def process_data(query_id, training_size):\n",
    "    \"\"\"\n",
    "    Process data for a given query_id and training_size combination.\n",
    "    \n",
    "    Args:\n",
    "        query_id (str): The ID of the query to process\n",
    "        training_size (int): The training size to process\n",
    "    \"\"\"\n",
    "    # Define the methods to process - now using the new set of methods\n",
    "    methods = ['cardinality', 'gaussian']\n",
    "    \n",
    "    for method in methods:\n",
    "        # Form the predictions file path using the new structure\n",
    "        predictions_path = f'0_mixture_test/{query_id}/{method}/training_{training_size}/confidence_0/predictions/'\n",
    "        \n",
    "        # Find the file that matches the pattern starting with query_id and ending with _batch_0.csv\n",
    "        predictions_file = None\n",
    "        try:\n",
    "            for filename in os.listdir(predictions_path):\n",
    "                if filename.startswith(f'{query_id}_') and filename.endswith('_batch_0.csv'):\n",
    "                    predictions_file = os.path.join(predictions_path, filename)\n",
    "                    break\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Directory not found for {method}, query_id={query_id}, training_size={training_size}\")\n",
    "            continue\n",
    "                \n",
    "        if predictions_file is None:\n",
    "            print(f\"No prediction file found for {method}, query_id={query_id}, training_size={training_size}\")\n",
    "            continue\n",
    "        \n",
    "        # Read and process the predictions file\n",
    "        predictions_df = pd.read_csv(predictions_file)\n",
    "        \n",
    "        # Read the testing data JSON\n",
    "        testing_json_path = f'0_mixture_test/{query_id}/{query_id}_mixture_test.json'\n",
    "        try:\n",
    "            with open(testing_json_path, 'r') as f:\n",
    "                query_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Testing JSON not found for query_id={query_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Process each prediction row\n",
    "        results = []\n",
    "        for _, pred_row in predictions_df.iterrows():\n",
    "            params = pred_row['params']\n",
    "            plan_id = pred_row['plan_id']\n",
    "            plan_content = pred_row['plan_content']\n",
    "            \n",
    "            # Convert params from string representation to list\n",
    "            params_list = eval(params)\n",
    "            \n",
    "            # Get full query instance\n",
    "            query = query_data[query_id]['query']\n",
    "            \n",
    "            # Replace parameters in query with actual values\n",
    "            for i, param in enumerate(params_list):\n",
    "                param = str(param).strip()\n",
    "                pattern = re.compile(rf\"@param{i}\\b\")\n",
    "                query = pattern.sub(param, query)\n",
    "            \n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'plan_id': plan_id,\n",
    "                'plan_content': plan_content\n",
    "            })\n",
    "        \n",
    "        # Save results to CSV\n",
    "        output_df = pd.DataFrame(results)\n",
    "        os.makedirs('0_mixture_plans', exist_ok=True)\n",
    "        output_filename = f'0_mixture_plans/{query_id}_workload_{method}_training_size_{training_size}.csv'\n",
    "        output_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Generated: {output_filename}\")\n",
    "\n",
    "# Define separate lists for query_ids and training sizes\n",
    "query_ids = ['091', '099', '100']\n",
    "# '013', '018', '019', '025', '027', '040', '050', '072', '084', '085', '091', '099', '100'\n",
    "training_sizes = [50]\n",
    "\n",
    "# Generate all combinations using itertools.product\n",
    "combinations = list(product(query_ids, training_sizes))\n",
    "\n",
    "# Display the total number of combinations to be processed\n",
    "print(f\"Processing {len(combinations)} combinations...\")\n",
    "\n",
    "# Process each combination\n",
    "for query_id, training_size in combinations:\n",
    "    print(f\"\\nProcessing query_id={query_id}, training_size={training_size}\")\n",
    "    process_data(query_id, training_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original: check the min-max confidence for each query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 0_original_analysis/confidence_range.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_confidence_ranges(query_id):\n",
    "    methods = ['cardinality', 'gaussian']\n",
    "    training_sizes = ['50']\n",
    "    results = []\n",
    "    \n",
    "    for method in methods:\n",
    "        for training_size in training_sizes:\n",
    "            base_path = f'dsb_{query_id}_original/{method}/outputs/evaluation/{query_id}/training_{training_size}/confidence_0/predictions/'\n",
    "            \n",
    "            if not os.path.exists(base_path):\n",
    "                print(f\"Warning: Directory not found: {base_path}\")\n",
    "                continue\n",
    "                \n",
    "            pattern = re.compile(f\"{query_id}_.*_batch_0\\.csv$\")\n",
    "            matching_files = []\n",
    "            \n",
    "            for file in os.listdir(base_path):\n",
    "                if pattern.match(file):\n",
    "                    matching_files.append(os.path.join(base_path, file))\n",
    "            \n",
    "            if not matching_files:\n",
    "                print(f\"Warning: No matching files found for {method} - training_{training_size}\")\n",
    "                continue\n",
    "            \n",
    "            all_confidences = []\n",
    "            for file in matching_files:\n",
    "                try:\n",
    "                    df = pd.read_csv(file)\n",
    "                    if 'confidence' in df.columns:\n",
    "                        all_confidences.extend(df['confidence'].tolist())\n",
    "                    else:\n",
    "                        print(f\"Warning: No confidence column in {file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if all_confidences:\n",
    "                min_conf = min(all_confidences)\n",
    "                max_conf = max(all_confidences)\n",
    "                \n",
    "                results.append({\n",
    "                    'query_id': query_id,\n",
    "                    'method': method,\n",
    "                    'training_size': training_size,\n",
    "                    'min_confidence': min_conf,\n",
    "                    'max_confidence': max_conf\n",
    "                })\n",
    "            \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_ids = ['013', '018', '019', '025', '027', '040', '050', '072', '084', '085', '091', '099', '100']\n",
    "    # '027', '040', '050', '072', '084', '085', '091', '099', '100'\n",
    "    all_results = []\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        all_results.extend(analyze_confidence_ranges(query_id))\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    output_dir = '0_original_analysis'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'confidence_range.csv')\n",
    "    results_df.to_csv(output_file, index=False, mode='w')\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original: training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def process_training_times():\n",
    "    base_dir = \".\"\n",
    "    output_dir = \"0_original_analysis\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = []\n",
    "    \n",
    "    for dir_name in os.listdir(base_dir):\n",
    "        if dir_name.startswith(\"dsb_\"):\n",
    "            query_id = dir_name.split(\"_\")[1]\n",
    "            base_path = Path(base_dir) / dir_name\n",
    "            \n",
    "            for method in ['cardinality', 'gaussian']:\n",
    "                method_path = base_path / method\n",
    "                \n",
    "                try:\n",
    "                    # get candidate plan generation time\n",
    "                    candidate_metadata_path = method_path / \"outputs\" / \"hints\" / query_id / \"training_50\" / \"candidate_metadata.json\"\n",
    "                    with open(candidate_metadata_path) as f:\n",
    "                        candidate_data = json.load(f)\n",
    "                        candidate_time = candidate_data[\"candidate_plan_generation_time_seconds\"] / 60\n",
    "                    \n",
    "                    # get training data generation time\n",
    "                    training_metadata_path = method_path / \"outputs\" / \"results\" / query_id / \"training_50\" / \"metadata.json\"\n",
    "                    with open(training_metadata_path) as f:\n",
    "                        training_data = json.load(f)\n",
    "                        training_time = training_data[\"training_data_all_time_seconds\"] / 60\n",
    "                    \n",
    "                    # get model prediction time\n",
    "                    eval_metadata_path = method_path / \"outputs\" / \"evaluation\" / query_id / \"training_50\" / \"confidence_0\" / \"metadata.json\"\n",
    "                    with open(eval_metadata_path) as f:\n",
    "                        eval_data = json.load(f)\n",
    "                        prediction_time = eval_data[\"model_prediction_time\"]\n",
    "                    \n",
    "                    # get training & testing time\n",
    "                    model_metadata_path = method_path / \"outputs\" / \"evaluation_single\" / query_id / \"training_50\" / \"confidence_0\" / \"metadata.json\"\n",
    "                    with open(model_metadata_path) as f:\n",
    "                        model_data = json.load(f)\n",
    "                        model_train_time = model_data[\"model_train_time\"]\n",
    "                        model_test_time = model_data[\"model_predict_time (already * 200)\"]\n",
    "                    \n",
    "                    # add result\n",
    "                    results.append({\n",
    "                        \"query_id\": query_id,\n",
    "                        \"method\": method,\n",
    "                        \"candidate_time\": round(candidate_time, 2),\n",
    "                        \"training_time\": round(training_time, 2),\n",
    "                        \"prediction_time\": round(prediction_time, 2),\n",
    "                        \"model_train_time\": round(model_train_time, 2),\n",
    "                        \"model_predict_time\": round(model_test_time, 2)\n",
    "                    })\n",
    "                    \n",
    "                except FileNotFoundError as e:\n",
    "                    print(f\"ERROR: {e}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"ERROR: {e}\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"ERROR: {e}\")\n",
    "    \n",
    "    # training_time.csv\n",
    "    output_file = Path(output_dir) / \"training_time.csv\"\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            \"query_id\",\n",
    "            \"method\",\n",
    "            \"candidate plan generation time (mins)\",\n",
    "            \"robust plan set generation (mins)\",\n",
    "            \"model prediction time (ms)\",\n",
    "            \"model train time (ms)\",\n",
    "            \"model predict time (ms)\"\n",
    "        ])\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for row in sorted(results, key=lambda x: (x[\"query_id\"])):\n",
    "            writer.writerow({\n",
    "                \"query_id\": row[\"query_id\"],\n",
    "                \"method\": row[\"method\"],\n",
    "                \"candidate plan generation time (mins)\": row[\"candidate_time\"],\n",
    "                \"robust plan set generation (mins)\": row[\"training_time\"],\n",
    "                \"model prediction time (ms)\": row[\"prediction_time\"],\n",
    "                \"model train time (ms)\": row[\"model_train_time\"],\n",
    "                \"model predict time (ms)\": row[\"model_predict_time\"]\n",
    "            })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_training_times()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original: robust plan count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 0_original_analysis/plan_cover_lengths.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def process_json_files(query_ids, methods, training_sizes):\n",
    "    # Initialize list to store results\n",
    "    results = []\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        for method in methods:\n",
    "            for training_size in training_sizes:\n",
    "                # Construct file path\n",
    "                file_path = f\"dsb_{query_id}_original/{method}/outputs/results/{query_id}/training_{training_size}/execution_output/dsb_{query_id}_metadata.json\"\n",
    "                \n",
    "                # Check if file exists\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        # Read and parse JSON file\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            data = json.load(f)\n",
    "                            \n",
    "                        # Extract plan_cover length\n",
    "                        plan_cover_length = len(data[str(query_id)][\"plan_cover\"])\n",
    "                        \n",
    "                        # Add result to list\n",
    "                        results.append({\n",
    "                            'query-id': query_id,\n",
    "                            'method': method,\n",
    "                            'training_size': training_size,\n",
    "                            'plan_cover_length': plan_cover_length\n",
    "                        })\n",
    "                    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "                        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        output_dir = \"0_original_analysis\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        output_file = f\"{output_dir}/plan_cover_lengths.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid files were found to process\")\n",
    "\n",
    "# Configuration\n",
    "query_ids = ['013', '018', '019', '025', '027', '040', '050', '072', '084', '085', '091', '099', '100']\n",
    "# '027', '040', '050', '072', '084', '085', '091', '099', '100'\n",
    "methods = ['cardinality', 'gaussian']\n",
    "training_sizes = [50]\n",
    "\n",
    "process_json_files(query_ids, methods, training_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original & mixture unique plan count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def analyze_workload_files(directory):\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('training_size_50.csv') and 'workload' in filename:\n",
    "            query_id = filename.split('_')[0]\n",
    "            method = filename.split('_')[2]\n",
    "            \n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            plan_counter = Counter(df['plan_id'])\n",
    "            results[query_id][method] = dict(plan_counter)\n",
    "    \n",
    "    return dict(results)\n",
    "\n",
    "def main():\n",
    "    # mixture plans\n",
    "    input_directory = '0_mixture_plans'\n",
    "    output_file = '0_original_analysis/mixture_plan_statistics.json'\n",
    "    result_dict = analyze_workload_files(input_directory)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(result_dict, f, indent=2)\n",
    "    \n",
    "    # original plans\n",
    "    input_directory = '0_dsb_original_plans_0'\n",
    "    output_file = '0_original_analysis/original_plan_statistics.json'\n",
    "    result_dict = analyze_workload_files(input_directory)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(result_dict, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original: single testing param file generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created: ./dsb_013_original/cardinality/inputs/testing/013_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_013_original/gaussian/inputs/testing/013_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_018_original/cardinality/inputs/testing/018_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_018_original/gaussian/inputs/testing/018_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_019_original/cardinality/inputs/testing/019_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_019_original/gaussian/inputs/testing/019_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_025_original/cardinality/inputs/testing/025_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_025_original/gaussian/inputs/testing/025_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_027_original/cardinality/inputs/testing/027_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_027_original/gaussian/inputs/testing/027_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_040_original/cardinality/inputs/testing/040_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_040_original/gaussian/inputs/testing/040_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_050_original/cardinality/inputs/testing/050_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_050_original/gaussian/inputs/testing/050_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_072_original/cardinality/inputs/testing/072_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_072_original/gaussian/inputs/testing/072_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_084_original/cardinality/inputs/testing/084_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_084_original/gaussian/inputs/testing/084_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_085_original/cardinality/inputs/testing/085_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_085_original/gaussian/inputs/testing/085_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_091_original/cardinality/inputs/testing/091_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_091_original/gaussian/inputs/testing/091_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_099_original/cardinality/inputs/testing/099_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_099_original/gaussian/inputs/testing/099_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_100_original/cardinality/inputs/testing/100_testing_original_single.json\n",
      "Processing completed successfully\n",
      "Successfully created: ./dsb_100_original/gaussian/inputs/testing/100_testing_original_single.json\n",
      "Processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_json_file(query_id, method):\n",
    "    base_path = \".\"\n",
    "    input_path = os.path.join(\n",
    "        base_path,\n",
    "        f\"dsb_{query_id}_original\",\n",
    "        method,\n",
    "        \"inputs\",\n",
    "        \"testing\",\n",
    "        f\"{query_id}_testing_original.json\"\n",
    "    )\n",
    "    \n",
    "    output_path = os.path.join(\n",
    "        base_path,\n",
    "        f\"dsb_{query_id}_original\",\n",
    "        method,\n",
    "        \"inputs\",\n",
    "        \"testing\",\n",
    "        f\"{query_id}_testing_original_single.json\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(input_path):\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "            \n",
    "        # read original file\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        if query_id not in data:\n",
    "            raise KeyError(f\"Query ID {query_id} not found in the JSON data\")\n",
    "            \n",
    "        if \"params\" not in data[query_id]:\n",
    "            raise KeyError(f\"'params' not found in data[{query_id}]\")\n",
    "            \n",
    "        # only keep the first one\n",
    "        if len(data[query_id][\"params\"]) > 0:\n",
    "            data[query_id][\"params\"] = [data[query_id][\"params\"][0]]\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"Successfully created: {output_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    for query_id in ['013', '018', '019', '025', '027', '040', '050', '072', '084', '085', '091', '099', '100']:\n",
    "        for method in ['cardinality', 'gaussian']:\n",
    "            success = process_json_file(query_id, method)\n",
    "            if success:\n",
    "                print(\"Processing completed successfully\")\n",
    "            else:\n",
    "                print(\"Processing failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian template modification: find (min, max) value from the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query ID: 013\n",
      "\n",
      "Query ID: 018\n",
      "  Train:\n",
      "    Position 2: min = 1998, max = 2002\n",
      "    Position 3: min = 1, max = 12\n",
      "    Position 5: min = 4, max = 95\n",
      "    Position 6: min = 9, max = 100\n",
      "  Test:\n",
      "    Position 2: min = 1998, max = 2002\n",
      "    Position 3: min = 1, max = 12\n",
      "    Position 5: min = 0, max = 100\n",
      "    Position 6: min = 5, max = 105\n",
      "\n",
      "Query ID: 019\n",
      "  Train:\n",
      "    Position 1: min = 1998, max = 2002\n",
      "    Position 2: min = 1, max = 12\n",
      "    Position 4: min = 1, max = 12\n",
      "    Position 5: min = 2, max = 80\n",
      "    Position 6: min = 22, max = 100\n",
      "  Test:\n",
      "    Position 1: min = 1998, max = 2002\n",
      "    Position 2: min = 1, max = 12\n",
      "    Position 4: min = 1, max = 12\n",
      "    Position 5: min = 1, max = 80\n",
      "    Position 6: min = 21, max = 100\n",
      "\n",
      "Query ID: 025\n",
      "  Train:\n",
      "    Position 0: min = 1, max = 10\n",
      "    Position 1: min = 1998, max = 2002\n",
      "    Position 2: min = 1, max = 10\n",
      "    Position 3: min = 1998, max = 2002\n",
      "    Position 4: min = 1, max = 10\n",
      "    Position 5: min = 1998, max = 2002\n",
      "  Test:\n",
      "    Position 0: min = 1, max = 10\n",
      "    Position 1: min = 1998, max = 2002\n",
      "    Position 2: min = 1, max = 10\n",
      "    Position 3: min = 1998, max = 2002\n",
      "    Position 4: min = 1, max = 10\n",
      "    Position 5: min = 1998, max = 2002\n",
      "\n",
      "Query ID: 027\n",
      "  Train:\n",
      "    Position 3: min = 1998, max = 2002\n",
      "  Test:\n",
      "    Position 3: min = 1998, max = 2002\n",
      "\n",
      "Query ID: 040\n",
      "  Train:\n",
      "    Position 2: min = 3, max = 61\n",
      "    Position 3: min = 42, max = 100\n",
      "    Position 4: min = 3, max = 81\n",
      "    Position 5: min = 22, max = 100\n",
      "    Position 6: min = 1, max = 35\n",
      "  Test:\n",
      "    Position 2: min = 2, max = 61\n",
      "    Position 3: min = 41, max = 100\n",
      "    Position 4: min = 1, max = 100\n",
      "    Position 5: min = 20, max = 119\n",
      "    Position 6: min = 1, max = 36\n",
      "\n",
      "Query ID: 050\n",
      "  Train:\n",
      "    Position 0: min = 2, max = 12\n",
      "    Position 1: min = 1, max = 6\n",
      "  Test:\n",
      "    Position 0: min = 1, max = 12\n",
      "    Position 1: min = 1, max = 6\n",
      "\n",
      "Query ID: 072\n",
      "  Train:\n",
      "    Position 1: min = 1998, max = 2002\n",
      "    Position 3: min = 0, max = 6\n",
      "    Position 4: min = 2, max = 8\n",
      "    Position 6: min = 1, max = 100\n",
      "    Position 7: min = 21, max = 120\n",
      "  Test:\n",
      "    Position 1: min = 1998, max = 2002\n",
      "    Position 3: min = 0, max = 6\n",
      "    Position 4: min = 2, max = 8\n",
      "    Position 6: min = 0, max = 100\n",
      "    Position 7: min = 20, max = 120\n",
      "\n",
      "Query ID: 084\n",
      "\n",
      "Query ID: 085\n",
      "  Train:\n",
      "    Position 0: min = 1998, max = 2002\n",
      "  Test:\n",
      "    Position 0: min = 1998, max = 2002\n",
      "\n",
      "Query ID: 091\n",
      "  Train:\n",
      "    Position 0: min = 1998, max = 2002\n",
      "    Position 1: min = 1, max = 12\n",
      "    Position 3: min = -7, max = -6\n",
      "  Test:\n",
      "    Position 0: min = 1998, max = 2002\n",
      "    Position 1: min = 1, max = 12\n",
      "    Position 3: min = -7, max = -6\n",
      "\n",
      "Query ID: 099\n",
      "  Train:\n",
      "    Position 0: min = 1176, max = 1212\n",
      "    Position 1: min = 6, max = 300\n",
      "    Position 2: min = 35, max = 329\n",
      "    Position 5: min = -5, max = -5\n",
      "  Test:\n",
      "    Position 0: min = 1176, max = 1212\n",
      "    Position 1: min = 3, max = 300\n",
      "    Position 2: min = 32, max = 329\n",
      "    Position 5: min = -5, max = -5\n",
      "\n",
      "Query ID: 100\n",
      "  Train:\n",
      "    Position 0: min = 1998, max = 2000\n",
      "    Position 2: min = 2, max = 100\n",
      "    Position 3: min = 21, max = 119\n",
      "    Position 6: min = 2, max = 267\n",
      "    Position 7: min = 16, max = 281\n",
      "    Position 8: min = 2, max = 267\n",
      "    Position 9: min = 16, max = 281\n",
      "  Test:\n",
      "    Position 0: min = 1998, max = 2000\n",
      "    Position 2: min = 1, max = 100\n",
      "    Position 3: min = 20, max = 119\n",
      "    Position 6: min = 3, max = 286\n",
      "    Position 7: min = 17, max = 300\n",
      "    Position 8: min = 3, max = 286\n",
      "    Position 9: min = 17, max = 300\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def analyze_param_combinations(PQO_gaussian_params):\n",
    "    results = {}\n",
    "    \n",
    "    # all_query_id\n",
    "    for query_id, data in PQO_gaussian_params.items():\n",
    "        results[query_id] = {\n",
    "            'train': [],\n",
    "            'test': []\n",
    "        }\n",
    "        \n",
    "        # train & test\n",
    "        for data_type in ['train', 'test']:\n",
    "            param_combinations = data[data_type]\n",
    "            if not param_combinations:  # 如果列表为空\n",
    "                continue\n",
    "                \n",
    "            # find the first combination\n",
    "            first_combination = param_combinations[0]\n",
    "            \n",
    "            # test whether the param is an integer\n",
    "            for idx, element in enumerate(first_combination):\n",
    "                try:\n",
    "                    # try for integer\n",
    "                    int(element)\n",
    "                    \n",
    "                    # get all values\n",
    "                    values = []\n",
    "                    for combination in param_combinations:\n",
    "                        try:\n",
    "                            values.append(int(combination[idx]))\n",
    "                        except (ValueError, IndexError):\n",
    "                            continue\n",
    "                    \n",
    "                    if values:  # find min & max\n",
    "                        results[query_id][data_type].append({\n",
    "                            'position': idx,\n",
    "                            'min_value': min(values),\n",
    "                            'max_value': max(values)\n",
    "                        })\n",
    "                        \n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    # print result\n",
    "    for query_id, data in results.items():\n",
    "        print(f\"\\nQuery ID: {query_id}\")\n",
    "        \n",
    "        for data_type in ['train', 'test']:\n",
    "            if data[data_type]:\n",
    "                print(f\"  {data_type.capitalize()}:\")\n",
    "                for result in data[data_type]:\n",
    "                    print(f\"    Position {result['position']}: \"\n",
    "                          f\"min = {result['min_value']}, \"\n",
    "                          f\"max = {result['max_value']}\")\n",
    "\n",
    "\n",
    "with open('PQO_gaussian_params.json', 'r') as f:\n",
    "    PQO_gaussian_params = json.load(f)\n",
    "analyze_param_combinations(PQO_gaussian_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query ID: 013\n",
      "\n",
      "Query ID: 018\n",
      "    Position 2: min = 1998, max = 2003\n",
      "    Position 3: min = 1, max = 12\n",
      "    Position 5: min = 1, max = 100\n",
      "    Position 6: min = 6, max = 100\n",
      "\n",
      "Query ID: 019\n",
      "    Position 1: min = 1998, max = 2003\n",
      "    Position 2: min = 1, max = 12\n",
      "    Position 4: min = 1, max = 12\n",
      "    Position 5: min = 1, max = 94\n",
      "    Position 6: min = 16, max = 100\n",
      "\n",
      "Query ID: 025\n",
      "    Position 0: min = 1, max = 12\n",
      "    Position 1: min = 1998, max = 2003\n",
      "    Position 2: min = 1, max = 10\n",
      "    Position 3: min = 1998, max = 2004\n",
      "    Position 4: min = 1, max = 12\n",
      "    Position 5: min = 1998, max = 2003\n",
      "\n",
      "Query ID: 027\n",
      "    Position 3: min = 1998, max = 2003\n",
      "\n",
      "Query ID: 040\n",
      "    Position 2: min = 1, max = 77\n",
      "    Position 3: min = 23, max = 100\n",
      "    Position 4: min = 1, max = 100\n",
      "    Position 5: min = 11, max = 119\n",
      "    Position 6: min = 1, max = 36\n",
      "\n",
      "Query ID: 050\n",
      "    Position 0: min = 1, max = 12\n",
      "    Position 1: min = 0, max = 6\n",
      "\n",
      "Query ID: 072\n",
      "    Position 1: min = 1998, max = 2002\n",
      "    Position 3: min = 0, max = 6\n",
      "    Position 4: min = 0, max = 8\n",
      "    Position 6: min = 1, max = 100\n",
      "    Position 7: min = 21, max = 120\n",
      "\n",
      "Query ID: 084\n",
      "    Position 1: min = 0, max = 190001\n",
      "\n",
      "Query ID: 085\n",
      "    Position 0: min = 1998, max = 2003\n",
      "\n",
      "Query ID: 091\n",
      "    Position 0: min = 1998, max = 2004\n",
      "    Position 1: min = 1, max = 12\n",
      "    Position 3: min = -7, max = -5\n",
      "\n",
      "Query ID: 099\n",
      "    Position 0: min = 1176, max = 1248\n",
      "    Position 1: min = 1, max = 300\n",
      "    Position 2: min = 15, max = 329\n",
      "    Position 5: min = -6, max = -5\n",
      "\n",
      "Query ID: 100\n",
      "    Position 0: min = 1998, max = 2003\n",
      "    Position 2: min = 1, max = 100\n",
      "    Position 3: min = 11, max = 119\n",
      "    Position 6: min = 1, max = 286\n",
      "    Position 7: min = 17, max = 300\n",
      "    Position 8: min = 1, max = 286\n",
      "    Position 9: min = 17, max = 300\n"
     ]
    }
   ],
   "source": [
    "# mixture\n",
    "import json\n",
    "import os\n",
    "\n",
    "def analyze_param_combinations(base_dir):\n",
    "    results = {}\n",
    "    \n",
    "    # Walk through all query_id directories\n",
    "    for query_id in os.listdir(base_dir):\n",
    "        file_path = os.path.join(base_dir, query_id, f\"{query_id}_mixture_test.json\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "            \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Get params directly\n",
    "        param_combinations = data[query_id].get(\"params\", [])\n",
    "        \n",
    "        results[query_id] = []\n",
    "        \n",
    "        if not param_combinations:  # if list is empty\n",
    "            continue\n",
    "            \n",
    "        # find the first combination\n",
    "        first_combination = param_combinations[0]\n",
    "        \n",
    "        # test whether the param is an integer\n",
    "        for idx, element in enumerate(first_combination):\n",
    "            try:\n",
    "                # try for integer\n",
    "                int(element)\n",
    "                \n",
    "                # get all values\n",
    "                values = []\n",
    "                for combination in param_combinations:\n",
    "                    try:\n",
    "                        values.append(int(combination[idx]))\n",
    "                    except (ValueError, IndexError):\n",
    "                        continue\n",
    "                        \n",
    "                if values:  # find min & max\n",
    "                    results[query_id].append({\n",
    "                        'position': idx,\n",
    "                        'min_value': min(values),\n",
    "                        'max_value': max(values)\n",
    "                    })\n",
    "                    \n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # print result\n",
    "    query_ids = sorted(results.keys())\n",
    "    for query_id in query_ids:\n",
    "        data = results[query_id]\n",
    "        print(f\"\\nQuery ID: {query_id}\")\n",
    "        for result in data:\n",
    "            print(f\"    Position {result['position']}: \"\n",
    "                  f\"min = {result['min_value']}, \"\n",
    "                  f\"max = {result['max_value']}\")\n",
    "\n",
    "# Usage\n",
    "analyze_param_combinations(\"0_mixture_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yang_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
